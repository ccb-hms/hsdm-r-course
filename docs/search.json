[
  {
    "objectID": "session-materials/sessions.html",
    "href": "session-materials/sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "Sessions\nThis set of pages contains the in-class materials we will be using.\nYou can browse the materials for each week on this website, but for class you need to download these materials and datasets.\n\nWeek 2: Getting started with data\nDownload Week 2 Materials Here\n\n\nWeek 3: Data visualization\nDownload Week 3 Materials Here\n\n\nWeek 4: Managing data\nDownload Week 4 Materials Here\n\n\nWeek 5: Descriptive statistics and exploratory data analysis\nDownload Week 5 Materials Here\n\n\nWeek 6: Regression analysis I\nDownload Week 6 Materials Here\n\n\nWeek 7: Regression analysis II\nDownload Week 7 Materials Here"
  },
  {
    "objectID": "session-materials/session6/session6.html",
    "href": "session-materials/session6/session6.html",
    "title": "Week 6: Regression",
    "section": "",
    "text": "Regression analysis comes in many forms. Here we will learn about linear regression. Regression is a set of tools that allow you to model and interpret the relationships between two or more variables. In regression we treat on variable as the response and a set of variables as possible predictors of the response. Regression models can be used for prediction, for explanation, to test hypotheses about relationships between variables"
  },
  {
    "objectID": "session-materials/session6/session6.html#introduction",
    "href": "session-materials/session6/session6.html#introduction",
    "title": "Week 6: Regression",
    "section": "",
    "text": "Regression analysis comes in many forms. Here we will learn about linear regression. Regression is a set of tools that allow you to model and interpret the relationships between two or more variables. In regression we treat on variable as the response and a set of variables as possible predictors of the response. Regression models can be used for prediction, for explanation, to test hypotheses about relationships between variables"
  },
  {
    "objectID": "session-materials/session6/session6.html#causal-relationships",
    "href": "session-materials/session6/session6.html#causal-relationships",
    "title": "Week 6: Regression",
    "section": "Causal Relationships",
    "text": "Causal Relationships\nLet’s look at some data from 1989 on infant death rates per 1000 of population. read.table is a more general form of read.csv which can more easily handle non-standard table formats, here the data is separated by tabs as opposed to commas.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\ninfdeath = read.table( \"infdeath.txt\", head=TRUE, sep=\"\\t\")\ndim(infdeath)\n\n[1] 20  4\n\ncolnames(infdeath)\n\n[1] \"country\" \"safe\"    \"breast\"  \"death\"  \n\n\nWe have 20 measurements on - safe=access to safe water - breast= percentage of mothers breast feeding at 6 months - death = infant death rate"
  },
  {
    "objectID": "session-materials/session6/session6.html#correlation-vs.-causation",
    "href": "session-materials/session6/session6.html#correlation-vs.-causation",
    "title": "Week 6: Regression",
    "section": "Correlation vs. Causation",
    "text": "Correlation vs. Causation\nWhen performing any time of analysis where we look at the relationship between two variables, we need to be careful to not mix up correlation and causation.\n\nplot(infdeath$breast, infdeath$death, xlab=\"% Breast Feeding\", \n     ylab=\"Infant Death Rate\", pty=\"s\", pch=19, cex.axis=1.5,\n     cex.lab=1.5)\n\n\n\n\nLooking at this data naively, it suggests that longer time breast feeding leads to greater infant mortality.\n\nplot(infdeath$safe, infdeath$breast, ylab=\"% Breast Feeding\", \n     xlab=\"% Access to safe water\", pty=\"s\", pch=19, cex.axis=1.5,\n     cex.lab=1.5)\n\n\n\n\nHowever, there is a latent variable which is actually influencing our result. Countries with access to safe water breast feed for less time."
  },
  {
    "objectID": "session-materials/session6/session6.html#causation",
    "href": "session-materials/session6/session6.html#causation",
    "title": "Week 6: Regression",
    "section": "Causation",
    "text": "Causation\nOften, as in the example above, there is a third variable (access to clean water) that affects both the response and the predictor. To truly get at causation we typically need a randomized controlled experiment However, we cannot always do experiments, e.g. most epidemiology."
  },
  {
    "objectID": "session-materials/session6/session6.html#linear-regression",
    "href": "session-materials/session6/session6.html#linear-regression",
    "title": "Week 6: Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nIn linear regression we want to relate the response, \\(y\\) to the predictor \\(x\\) using a straight line model\n$y = \\beta_0 + \\beta_1 \\cdot x + \\epsilon$\nYou may be more familiar with the notation \\(y = mx + b\\), which is the identical equation using different variable names.\nIn this model:\n\n\\(\\beta_0\\) is the intercept - the value that \\(y\\) takes when \\(x=0\\)\n\\(\\beta_1\\) is the slope of the line, relating \\(x\\) to \\(y\\) and can be interpreted as the average change in \\(y\\) for a one unit change in \\(x\\)\n\\(\\epsilon\\) represents the inherent variability in the system, it is assumed to be zero on average\n\nSome of the statistical theory is easier if you assume that \\(\\epsilon\\)’s come as independent observations with the same distribution (i.i.d), but this is not essential."
  },
  {
    "objectID": "session-materials/session6/session6.html#plot-the-data",
    "href": "session-materials/session6/session6.html#plot-the-data",
    "title": "Week 6: Regression",
    "section": "Plot the data",
    "text": "Plot the data\nWe can use the lm function in R to create a linear model. We then also use R’s formula notation to tell it what we want the model to describe. Here, we want to examine the relationship between mortality and breastfeeding, which we can specify by giving R the linear model y~x, or here infdeath$death ~ infdeath$breast. Note that we don’t need to tell R that we want the model to have an intercept, it incorporates one automatically.\n\nplot(infdeath$breast, infdeath$death, xlab=\"% Breast Feeding\", \n     ylab=\"Infant Death Rate\", pty=\"s\", pch=19, cex.axis=1.5,\n     cex.lab=1.5, xlim=c(0, 100), ylim=c(-30,150))\nlm1 = lm(infdeath$death~infdeath$breast)\nabline(lm1, col=\"blue\")\n\n\n\n\nWe can directly output the model coefficients and other information, here the learned slope and intercept of the line, by using summary.\n\nsummary(lm1)\n\n\nCall:\nlm(formula = infdeath$death ~ infdeath$breast)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.568 -21.047   1.368  19.479  33.705 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -26.978     20.148  -1.339 0.205392    \ninfdeath$breast    1.467      0.288   5.093 0.000265 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.86 on 12 degrees of freedom\n  (6 observations deleted due to missingness)\nMultiple R-squared:  0.6837,    Adjusted R-squared:  0.6573 \nF-statistic: 25.94 on 1 and 12 DF,  p-value: 0.000265\n\n\nWe can see that the intercept is about -27 and the slope is about 1.5.\n\n\n\n\n\n\nExercise\n\n\n\nTry to interpret these results. What do the slope and intercept mean?"
  },
  {
    "objectID": "session-materials/session6/session6.html#linear-regression-1",
    "href": "session-materials/session6/session6.html#linear-regression-1",
    "title": "Week 6: Regression",
    "section": "Linear Regression",
    "text": "Linear Regression\nWe denote our estimates by putting a hat on the parameter, e.g. \\(\\hat{\\beta_0}\\). This helps make it clear that the coefficients and intercept we found are just estimates based on our data. Ideally, we would want to know the true slope and intercept, but that would be impossible unless we collected data on every infant in the world.\nOur estimated line is the given by \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x\\). Now that we have these estimates, given a new \\(x\\) we can then predict \\(y\\). However, we have to be careful about whether the new \\(x\\) is sensible.\nLet’s look under the hood of the linear model. How do we choose the best straight line?"
  },
  {
    "objectID": "session-materials/session6/session6.html#least-squares",
    "href": "session-materials/session6/session6.html#least-squares",
    "title": "Week 6: Regression",
    "section": "Least Squares",
    "text": "Least Squares\nLet our predicted values be \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_i\\), and the real values in our data be denoted as \\(y_i\\).\nOne strategy to choose our predicted values is to minimize the sum of squared prediction errors:\n\\[\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\] This method is called least squares and was described by Gauss in 1822.\nAnother way to look at our model is as\n\\[\nY = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\n\\] Where we use \\(Y\\) and \\(X\\) to represent the fact that data came from some form of sample from a population we want to make inference about, and \\(\\epsilon\\) is the error between each observation and it’s real value based on our model coefficients. Usually we will assume our errors are normally distributed around 0 \\(\\epsilon \\sim N(0, \\sigma^2)\\), and so are independent (but that is not strictly necessary)."
  },
  {
    "objectID": "session-materials/session6/session6.html#simulation-experiments",
    "href": "session-materials/session6/session6.html#simulation-experiments",
    "title": "Week 6: Regression",
    "section": "Simulation Experiments",
    "text": "Simulation Experiments\nOne tool that you can use to understand how different methods work is to create a simulation experiment. In these experiments you create a model, where you know the answer, and you control the variability or stochastic component. Then, you can interpret the way in which the different quantities you are estimating behave. This can help us understand why we might be getting a particular result, or what would happen if our assumptions were not true.\n\nset.seed(123)\n \n# Generate some x values\nx = runif(20, min=20, max=70)\n\n# Set our regression\ny = 5 + .2 *x \n \n# Add in some random error\nyobs = y + rnorm(20, sd=1.5)\n\n# Create a linear model\nlm2 = lm(yobs~x)\nsummary(lm2)\n\n\nCall:\nlm(formula = yobs ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1142 -0.9480 -0.1303  1.0299  2.8389 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.71335    1.04242   6.440 4.63e-06 ***\nx            0.16055    0.02088   7.691 4.28e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.426 on 18 degrees of freedom\nMultiple R-squared:  0.7667,    Adjusted R-squared:  0.7537 \nF-statistic: 59.15 on 1 and 18 DF,  p-value: 4.275e-07"
  },
  {
    "objectID": "session-materials/session6/session6.html#repeating-simulations",
    "href": "session-materials/session6/session6.html#repeating-simulations",
    "title": "Week 6: Regression",
    "section": "Repeating simulations",
    "text": "Repeating simulations\nWe often want to run many simulations to see how likely certain results are or explore another facet of the model. Here, we run 1000 simulations like the one above. We repeatedly sample from a Normal distribution with sd=1.5 and create a new \\(y\\). Then, we estimate the parameters for that model, and save them. The code below does this using a for loop. We won’t be covering for loops, but they allow a chunk of code to repeat a certain number of times.\n\n# First, create empty vectors with the number of simulations we want to run. \nNITER=1000\nb0vec = rep(NA, NITER)\nb1vec = rep(NA, NITER)\n\n# Create NITER different simulated datasets and linear models using a for loop, storing the results into the vectors we created. \nfor( i in 1:NITER) {\n  ei = rnorm(20, sd=1.5)\n  yoi = y + ei\n  lmi = lm(yoi ~ x)\n  b0vec[i] = lmi$coefficients[1]\n  b1vec[i] = lmi$coefficients[2]\n}\n\nLet’s take a look at the distribution of coefficients produced by our simulations.\n\n\n\n\n\nWe can also see the mean and standard deviation across the simulations.\n\n# Intercept\nround(mean(b0vec), digits=3)\n\n[1] 4.965\n\nround(sqrt(var(b0vec)), digits=3)\n\n[1] 1.119\n\n# Slope\nround(mean(b1vec), digits=3)\n\n[1] 0.201\n\nround(sqrt(var(b1vec)), digits=3)\n\n[1] 0.023\n\n\nLet’s compare these values to the regression summary from our first simulation. How do the values compare? What can you interpret?\n\nsummary(lm2)\n\n\nCall:\nlm(formula = yobs ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1142 -0.9480 -0.1303  1.0299  2.8389 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.71335    1.04242   6.440 4.63e-06 ***\nx            0.16055    0.02088   7.691 4.28e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.426 on 18 degrees of freedom\nMultiple R-squared:  0.7667,    Adjusted R-squared:  0.7537 \nF-statistic: 59.15 on 1 and 18 DF,  p-value: 4.275e-07"
  },
  {
    "objectID": "session-materials/session6/session6.html#inference-about-the-model",
    "href": "session-materials/session6/session6.html#inference-about-the-model",
    "title": "Week 6: Regression",
    "section": "Inference about the model",
    "text": "Inference about the model\nWhen interpretting the summary output, we can note that: - Typically the intercept is not that interesting. - We test the hypothesis \\(H_0: \\beta_1 = 0\\) to determine whether \\(x\\) provides any explanation of \\(y\\). - \\(R^2\\) and multiple \\(R^2\\) tell us about how much of the variation in \\(y\\) has been captured by the terms in the model. - The residual standard error is an estimate of \\(\\sigma\\) from the distribution of the \\(\\epsilon_i\\).\nHowever, all of that inference is dependent on the model being correct. If the linear model does not fit the data, then the \\(\\beta\\)’s are not meaningful, the variance cannot be estimated, we cannot correctly assess whether or not the covariate(s) x(’s) help to predict \\(y\\)"
  },
  {
    "objectID": "session-materials/session6/session6.html#anscombes-quartet",
    "href": "session-materials/session6/session6.html#anscombes-quartet",
    "title": "Week 6: Regression",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet\nRecall Anscombe’s Quartet which we looked at last week. Recall that all of these models have identical summart statistics, and it turns out they all also have identical linear regression results:\n\nlibrary(datasets)\ndatasets::anscombe\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\nlong_ans &lt;- anscombe |&gt;\n  pivot_longer(\n    everything(),\n    cols_vary = \"slowest\",\n    names_to = c(\".value\", \"set\"),\n    names_pattern = \"(.)(.)\"\n  )\n\nggplot(long_ans,aes(x=x,y=y,group=set))+geom_point()+facet_wrap(~set)\n\n\n\n\nOnly the top left plot among these four datasets represents a statistical regression model: - The top right model represents a non-linear relationship, and one that seems to have no error - The bottom left plot is a straight line, with one possible recording error, but no statistical variation - The bottom right plot shows no relationship between \\(x\\) and \\(y\\) for most points, but there is one outlier, in both \\(x\\) and \\(y\\) that affects the estimated relationship.\nThe hard part is not writing the command lm(y~x). It is ensuring that the model adequately represents the data and the relationships within it Once you have convinced yourself, and others, that this is true then you can interpret and test hypotheses."
  },
  {
    "objectID": "session-materials/session6/session6.html#evaluating-models",
    "href": "session-materials/session6/session6.html#evaluating-models",
    "title": "Week 6: Regression",
    "section": "Evaluating Models",
    "text": "Evaluating Models\nThe residuals are defined as\n\\[\ne_i = y_i - \\hat{y}_i\n\\]\nWe can check the residuals for various behaviors to evaluate our model.\n1.  Constant spread: the $e_i$'s have about the same spread regardless of the value of $x$\n2.  Independence: Do the $e_i$'s show any signs of correlation.\n3.  Symmetry and Unimodality: Do the $e_i$'s have an approximately symmetric distribution with a single mode.\n4.  Normality: but that is often too strong a requirement.\n\nResidual Plots\nWe can plot the residuals (\\(y\\)-axis) against any covariates (\\(x\\)’s) and the predicted values (\\(\\hat{y}\\)’s). We want to look for trends, such as increasing or decreasing variability (fans), and outliers. When data are collected over time, we sometimes look for relationships between successive residuals. We also want to be careful to not over-interpret data if we don’t have that much of it, as we saw last week when looking at insulin.\nLet’s return to our child birth example and make a residual plot."
  },
  {
    "objectID": "session-materials/session6/session6.html#extensions---non-linear-models",
    "href": "session-materials/session6/session6.html#extensions---non-linear-models",
    "title": "Week 6: Regression",
    "section": "Extensions - non-linear models",
    "text": "Extensions - non-linear models\nSo far we’ve only look at linear models. Fitting straight lines is fine, most of the time, as we know that complex relationships can be approximated by a straight line over a restricted set of values.\nOne option would be to add polynomials in our covariates\n\\[\n    y = \\beta_0 + \\beta_1 \\cdot x + \\beta_2 \\cdot x^2 + \\epsilon\n    \\] However, these models have very strong assumptions and the effect of the polynomial terms on the fit can be large.\nAnother option is an exponential model. A straight line model is appropriate if \\(y\\) changes by a fixed amount for every unit change in \\(x\\). An exponential curve represents the case where \\(y\\) changes by a fixed percentage (or proportion) for every unit change in \\(x\\). We often see exponential models in economics.\nThe exponential model can be defined as:\n\\[\n     y = a \\cdot e^{bx} \\\\\n     ln(y) = ln(a) + b \\cdot x\n\\] Note that in the second equation, we’ve transformed the values by taking the natural log of both sides. Using this, we can fit an exponential model with linear regression if we transform \\(y\\). There are many other transformations of \\(y\\) that could be used and an extensive literature on choosing the best ones (Box-Cox transformation). However, a transformation of \\(y\\) will in general not just effect the mean relationship but it also affects the variability in your \\(y\\) values as a function of \\(x\\).\nThus, the models are not the same. If we take a look again at the exponential function and transformed function, now labeled M1 and M2, note that we would need the error to be multiplicative in M1 for them to be the same.\n\\[\nM1: y = a \\cdot e^{bx} + \\epsilon \\\\\nM2: ln(y) = ln(a) + b \\cdot x + \\epsilon \\\\\n\\epsilon \\sim N(0, \\sigma^2)\n  \\]\nWe can visualize this difference as well.\n\na = 1.5\nb = 0.3\nx = runif(100, 1,11)\neps = rnorm(100, 2)\nm1v = a*exp(b*x) + eps\nm2v = log(a) + b*x + eps\npar(mfrow=c(1,3))\nplot(x,m1v, pch=19, xlab=\"x\", ylab=\"y-hat\", main=\"M1\")\nplot(x, log(m1v), pch=19, xlab=\"x\", ylab = \"log(y-hat)\", main=\"M1 transformed\")\nplot(x,m2v, pch=19, xlab=\"x\", ylab =\"log(y)-hat\", main=\"M2\")"
  },
  {
    "objectID": "session-materials/session6/session6.html#prediction-and-prediction-intervals",
    "href": "session-materials/session6/session6.html#prediction-and-prediction-intervals",
    "title": "Week 6: Regression",
    "section": "Prediction and Prediction intervals",
    "text": "Prediction and Prediction intervals\nThere are two kinds of predictions that are interesting:\n-   *Confidence interval*: predict the mean response for a given value of $x$\n-   *Prediction interval*: predict the value of the response $y$ for an individual whose covariate is $x$\nAlmost all regression methods in R have prediction methods associated with them. Confidence intervals have less variability than prediction intervals.\n\nExample: Confidence Interval vs Prediction Interval\nReturning to our example on infant death\n\nlm1 = lm(death~breast, data=infdeath)\nsummary(lm1)\n\n\nCall:\nlm(formula = death ~ breast, data = infdeath)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-37.568 -21.047   1.368  19.479  33.705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -26.978     20.148  -1.339 0.205392    \nbreast         1.467      0.288   5.093 0.000265 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.86 on 12 degrees of freedom\n  (6 observations deleted due to missingness)\nMultiple R-squared:  0.6837,    Adjusted R-squared:  0.6573 \nF-statistic: 25.94 on 1 and 12 DF,  p-value: 0.000265\n\n\nWe can predict the mean response for country where 62% of the women breast feed at 6 months using the `predict1 function.\n\npredict(lm1, newdata=list(breast=62), interval=\"confidence\")\n\n       fit     lwr      upr\n1 63.96593 49.8048 78.12705\n\n\nOr we can predict the response for a specific country where 62% of the women breast feed at 6 months\n\npredict(lm1, newdata=list(breast=62), interval=\"prediction\")\n\n       fit      lwr      upr\n1 63.96593 10.08279 117.8491\n\n\nNote how the lower and upper bounds are much tighter for the confidence interval than for the prediction interval."
  },
  {
    "objectID": "session-materials/session6/session6.html#regressions-using-factors",
    "href": "session-materials/session6/session6.html#regressions-using-factors",
    "title": "Week 6: Regression",
    "section": "Regressions using factors",
    "text": "Regressions using factors\nAs we’ve seen, a factor is a variable that takes on a discrete set of different values. These values can be unordered (e.g. Male/Female or European, Asian, African, etc.) or they can be ordered (age: less than 18, 18-40, more than 40). In regression models, we typically implement factors using dummy variables. Essentially we create a new set of variables using indicator functions, \\(1_{Ai} = 1\\) if observation \\(i\\) has the level \\(A\\).\n\nFitting factors in regression models\nSuppose we have a factor with \\(K\\) levels We can fit factors in two ways\n-   **M1:** includes an intercept in the model and then use $K-1$ indicator variables\n-   **M2L=:** no intercept and use $K$ indicator variables.\nIn M1 the intercept is the mean value of \\(y\\), and each \\(\\beta_j\\) is the difference in mean for the \\(j^{th}\\) retained factor level from the overall mean. In M2 each of the \\(\\beta_j\\) is the mean value for \\(y\\) only within factor level \\(j\\).\nTo show an example, suppose our factor is sex, which has two levels, M and F. Our models could be\n\\[\n  M1:  Y = \\beta_0 + \\beta_M \\cdot 1_{M} + \\epsilon \\\\\n  E[Y | F]= \\beta_0 \\quad \\mbox{and} \\quad E[Y|M] = \\beta_0 + \\beta_M\n\\]\n\nheights = runif(40, min=60, max=75)\nsex = sample(c(\"M\",\"F\"), 40, replace=TRUE)\nlm1 = lm(heights ~ sex)\nsummary(lm1)\n\n\nCall:\nlm(formula = heights ~ sex)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7290 -3.4115  0.5925  3.7202  5.9065 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  68.0986     0.9463  71.966   &lt;2e-16 ***\nsexM          0.2882     1.3060   0.221    0.827    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.125 on 38 degrees of freedom\nMultiple R-squared:  0.00128,   Adjusted R-squared:  -0.025 \nF-statistic: 0.04871 on 1 and 38 DF,  p-value: 0.8265\n\n\nOr\n\\[\n  M2: y = \\beta_M \\cdot 1_{M} + \\beta_{F} \\cdot 1_{F} + \\epsilon \\\\\n  E[Y|F] = \\beta_F \\quad \\mbox{and} E[Y|M] = \\beta_M\n\\]\n\n# Adding -1 tells R that we don't want an intercept\nlm2 = lm(heights ~ sex - 1)\nsummary(lm2)\n\n\nCall:\nlm(formula = heights ~ sex - 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7290 -3.4115  0.5925  3.7202  5.9065 \n\nCoefficients:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nsexF  68.0986     0.9463   71.97   &lt;2e-16 ***\nsexM  68.3868     0.9001   75.98   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.125 on 38 degrees of freedom\nMultiple R-squared:  0.9965,    Adjusted R-squared:  0.9964 \nF-statistic:  5476 on 2 and 38 DF,  p-value: &lt; 2.2e-16\n\n\nNote that there are some issues you have to worry about when fitting a model without an intercept. With model M1 that has an intercept then for each group the test for \\(H_0: \\beta_j = 0\\) tests whether that group mean is different from the mean for the group that was used to determine the intercept. However, in M2 the test for each group, \\(H_0: \\beta_j = 0\\) is then comparing the mean for that group to zero (0). In M2, multiple-\\(R^2\\) does not have a reasonable interpretation, and we get a very high value.\n\nThis materials loosely based on Chance Encounters by Seber and Wild https://www.stat.auckland.ac.nz/~wild/ChanceEnc/index.shtml"
  },
  {
    "objectID": "session-materials/session4/session4.html",
    "href": "session-materials/session4/session4.html",
    "title": "Week 4: Preparing Data",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "session-materials/session4/session4.html#combining-the-data",
    "href": "session-materials/session4/session4.html#combining-the-data",
    "title": "Week 4: Preparing Data",
    "section": "Combining the data",
    "text": "Combining the data\nNow we can combine the data together.\n\n# Combine all years together\ntib_all &lt;- bind_rows(tib_d, tib_e, tib_f)"
  },
  {
    "objectID": "session-materials/session4/session4.html#numeric-variables",
    "href": "session-materials/session4/session4.html#numeric-variables",
    "title": "Week 4: Preparing Data",
    "section": "Numeric variables",
    "text": "Numeric variables\nLet’s start by converting numeric columns:\n\n# Set numeric columns to be numeric\ntib_all &lt;- mutate(tib_all,\n                   RIDAGEYR = as.numeric(RIDAGEYR),\n                   LBXGLU = as.numeric(LBXGLU),\n                   LBXGH = as.numeric(LBXGH),\n                   BMXBMI = as.numeric(BMXBMI))"
  },
  {
    "objectID": "session-materials/session4/session4.html#categorical-variables",
    "href": "session-materials/session4/session4.html#categorical-variables",
    "title": "Week 4: Preparing Data",
    "section": "Categorical variables",
    "text": "Categorical variables\nMany of the NHANES values are categorical data, but right now are stored as text. We can check what values exist by converting them to factors before making the decision of how to handle them in the analysis:\n\ntib_all |&gt;\n  mutate(across(c(RIAGENDR, RIDRETH1, DMDBORN), as.factor), .keep = \"none\") |&gt;\n  summary()\n\n   RIAGENDR                               RIDRETH1    \n Female:15633   Mexican American              : 7388  \n Male  :15401   Non-Hispanic Black            : 6878  \n                Non-Hispanic White            :12463  \n                Other Hispanic                : 2683  \n                Other Race - Including Multi-R: 1622  \n                                                      \n                                                      \n                           DMDBORN     \n \"Born in 50 US States or Washi:25732  \n Born Elsewhere                :  588  \n Born in Mexico                : 2511  \n Born in Other Non-Spanish Spea: 1065  \n Born in Other Spanish Speaking: 1120  \n Don't Know                    :    2  \n Refused                       :   16  \n\n\nIt looks like one of the options for DMDBORN has a lingering quote character. This can happen due to irregularities in how NHANES data is presented or small mistakes in data processing. We use the gsub function to replace all instances of double quotes with the empty string in the column.\n\ntib_all &lt;- mutate(tib_all,\n                   RIAGENDR = as.factor(RIAGENDR),\n                   RIDRETH1 = as.factor(RIDRETH1),\n                   DMDBORN = gsub(\"\\\"\", \"\", DMDBORN), # Remove quotes\n                   DMDBORN = as.factor(DMDBORN),\n                   OHXDECAY = (OHXDECAY == \"Yes\"),\n                   OHXREST = (OHXREST == \"Yes\"))"
  },
  {
    "objectID": "session-materials/session4/session4.html#new-variables",
    "href": "session-materials/session4/session4.html#new-variables",
    "title": "Week 4: Preparing Data",
    "section": "New variables",
    "text": "New variables\nWe need to create the variables mentioned in the paper are not directly present in NHANES. For some variables such as country of birth, we could store the categories in DMDBorn but this way we can also keep the original distribution.\n\n# Set columns to categories as in the paper\ntib_all &lt;- mutate(tib_all,\n\n                    age.cat = cut(\n                      RIDAGEYR,\n                      breaks = c(13, 15, 18, 100),\n                      include.lowest = TRUE,\n                      labels = c(\"13-15\", \"16-18\", \"19+\")),\n                   \n                    plasma.glucose.cat = case_when(\n                     LBXGLU &lt; 100 ~ \"&lt;100 mg/dl\",\n                     LBXGLU &lt; 126 ~ \"&gt;=100 mg/dl and &lt;126 mg/dl\", \n                     LBXGLU &gt;= 126 ~ \"&gt;=126 mg/dl\",\n                     .default = NA),\n                   \n                   hba1c.cat = case_when(\n                     LBXGH &lt; 5.7 ~ \"&lt;5.7%\",\n                     LBXGH &gt;= 5.7 ~ \"&gt;=5.7% and &lt;6.5%\",\n                     LBXGH &gt;= 6.5 ~ \"&gt;= 6.5%\",\n                     .default = NA),\n                   \n                   bmi.cat = case_when( # If we were doing this from scratch we might also want an underweight category\n                     BMXBMI &lt; 25 ~ \"Normal\", \n                     BMXBMI &lt; 30 ~ \"Overweight\",\n                     BMXBMI &gt;= 30 ~ \"Obese\",\n                     .default = NA), \n                   \n                   family.PIR.cat = case_when(\n                     INDFMPIR == \"PIR value greater than or equa\" ~ \"&gt;= 1\",\n                     INDFMPIR == \"Value greater than or equal to\" ~ \"&gt;= 1\",\n                     as.numeric(INDFMPIR) &gt;= 1 ~ \"&gt;=1\",\n                     as.numeric(INDFMPIR) &lt; 1 ~ \"&lt;1\",\n                     .default = NA),\n                   \n                   birthplace = case_when(\n                     DMDBORN == \"Born in 50 US States or Washi\" ~ \"Within the US\",\n                     is.na(DMDBORN) ~ NA,\n                     DMDBORN == \"Don't Know\" ~ NA,\n                     DMDBORN == \"Refused\" ~ NA,\n                    .default = \"Outside the US\"),\n                   dental.caries = OHXDECAY | OHXREST)\n\n\n\n\n\n\n\nExercise\n\n\n\nAdd a diabetes column to the dataset based on the guidelines from Beheshti et. al:\n\nIndividuals with an HbA1C of at least 6.5 percent or a plasma-fasting glucose value of at least 126 mg/dl were considered diabetics; prediabetics were those whose HbA1C ranged from 5.7 percent to 6.4 percent and whose fasting plasma glucose remained within 100 to 125 mg/dl; the remaining study participants, who had less than 5.7 percent HbA1C and less than 100 mg/dl fasting plasma glucose, were classified as nondiabetics.\n\nIt is recommended to use case_when, but feel free to use any method you want.\n*Hint: remember to account for missing data! When should diabetes be NA?\n\n# Add diabetes column\ntib_all &lt;- tib_all |&gt; \n          mutate(diabetes = case_when(\n            is.na(LBXGH) & is.na(LBXGLU) ~ NA,\n           LBXGH &gt;= 6.5 | LBXGLU &gt;= 126 ~ \"diabetic\",\n           LBXGH &gt;= 5.7 | LBXGLU &gt;= 100 ~ \"prediabetic\",\n            .default = \"nondiabetic\"))"
  },
  {
    "objectID": "session-materials/session4/session4.html#exploring-other-diabetes-variables",
    "href": "session-materials/session4/session4.html#exploring-other-diabetes-variables",
    "title": "Week 4: Preparing Data",
    "section": "Exploring Other Diabetes Variables",
    "text": "Exploring Other Diabetes Variables\nWe’ll start by checking the consistency of the DIQ variables. Was there anyone who said they were both diabetic and prediabetic? Here we’re using the table function, which will give us a nice counts table as output. We’ll see some other ways to make tables next week.\n\ntable(tib_all[,c(\"DIQ010\", \"DIQ160\")], exclude = NULL)\n\n            DIQ160\nDIQ010       Don't know    No   Yes  &lt;NA&gt;\n  Borderline          0     0     0   343\n  Don't know          8     7     3     5\n  No                 32 18908   690  7563\n  Refused             0     0     0     1\n  Yes                 0     0     0  2037\n  &lt;NA&gt;                0     0     0  1437\n\n\nNow let’s see how much the diabetes definition by Beheshti agrees with the DIQ variables.\nWe’ll make a variable with the same categories from the DIQ data. We’ll leave out the borderline, don’t know, and refused responses.\n\ntib_all &lt;- tib_all |&gt; \n  mutate(diabetes.DIQ = case_when(\n    DIQ010 == \"Yes\" ~ \"diabetic\",\n    DIQ160 == \"Yes\" ~ \"prediabetic\",\n    DIQ160 == \"No\" | DIQ010 == \"No\" ~ \"nondiabetic\"\n  ))\n\ntable(tib_all[,c(\"diabetes.DIQ\", \"diabetes\")])\n\n             diabetes\ndiabetes.DIQ  diabetic nondiabetic prediabetic\n  diabetic        1277         130         424\n  nondiabetic      494       11352        5238\n  prediabetic      115         233         307"
  },
  {
    "objectID": "session-materials/session4/session4.html#extracting-the-study-population",
    "href": "session-materials/session4/session4.html#extracting-the-study-population",
    "title": "Week 4: Preparing Data",
    "section": "Extracting the study population",
    "text": "Extracting the study population\nWe need to filter the summary population. Luckily, the paper provides total numbers of participants for most of the filtering steps performed. This makes it much easier for us to check our work.\n\nThe total number of individuals who participated in the NHANES from 2005 to 2010 were 31,034 study-participants. Among them, 3,660 were nonedentulous adolescents (aged 13 to 18 years old at the time of screening) who had exam survey weights assigned. From those 3,660 adolescents, data to measure the main outcome variable (dental caries experience) were available for a final sample of 3,346 adolescents, representing the population of 24,386,135 U.S. adolescents after applying the NHANES sample weights, which was described and analyzed in this study.\n\n\ntib_all &lt;- tib_all |&gt;\n  mutate(in.study = (RIDAGEYR &gt;= 13) & (RIDAGEYR &lt;= 18) & !is.na(dental.caries)) # Gets the 3346 with non-NA dental carie variable\n\nall_ado &lt;- tib_all |&gt;\n  filter(in.study)\n\nWe can now calculate values for adolescents with dental caries experience.\n\n\n\n\n\n\nExercise\n\n\n\nCalculate the percent of adolescents with dental caries experience.\n\n# Recall that you can sum boolean variables to get a count\nsum(all_ado$dental.caries)/nrow(all_ado)\n\n[1] 0.6007173"
  },
  {
    "objectID": "session-materials/session4/session4.html#exporting-data",
    "href": "session-materials/session4/session4.html#exporting-data",
    "title": "Week 4: Preparing Data",
    "section": "Exporting data",
    "text": "Exporting data\nNow that you have learned how to extract information from or summarise your raw data, you may want to export these new data sets to share them with your collaborators or for archival.\nSimilar to the read_csv() function used for reading CSV files into R, there is a write_csv() function that generates CSV files from data frames.\nLet’s use write_csv() to save the nhanes data.\n\nwrite_csv(all_ado, file = \"nhanes_processed.csv\")"
  },
  {
    "objectID": "session-materials/session2/session2.html",
    "href": "session-materials/session2/session2.html",
    "title": "Session 2: Markdown and introduction",
    "section": "",
    "text": "Welcome! Each week, in-class we will be answering questions on the reading and performing analyses based on a code notebook.\nThroughout these sessions we will be replicating the analysis from Beheshti et. al. 20211\nThis is an analysis of the association between diabetes and dental caries in U.S. adolescents. By the end of the semester we will be able to replicate and extend the analysis in this paper."
  },
  {
    "objectID": "session-materials/session2/session2.html#code-chunks",
    "href": "session-materials/session2/session2.html#code-chunks",
    "title": "Session 2: Markdown and introduction",
    "section": "Code Chunks",
    "text": "Code Chunks\nYou can start a and end code chunk using three back ticks “```”. To have a chunk run as R code, you need to assign the chunk using {r}. You can then specify options for the chunk on subsequent lines using the “hash-pipe” |#. Code chinks have a lot of options, but some of the most important are label, eval, echo, and output.\n\nx &lt;- 5\nx\n\n[1] 5\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry changing these options in the first of the two chunks below and re-rendering the document. What do each of these arguments do? Pay attention to both chunk’s output.\n\ny = 8\nx\n\n[1] 5\n\ny\n\n[1] 8\n\nx &lt;- x + y \n\n\nx # Show the value of x\n\n[1] 13"
  },
  {
    "objectID": "session-materials/session2/session2.html#markdown",
    "href": "session-materials/session2/session2.html#markdown",
    "title": "Session 2: Markdown and introduction",
    "section": "Markdown",
    "text": "Markdown\nMarkdown is a language used to quickly create formatted text. It’s great to know as it is used in R Markdown, Quarto, Jupyter, Github documents, and many other places. A pure markdown file has a .md file extension.\nYou can find a quick guide to markdown here, throughout the course we will see various things markdown can do in the readings and in-class materials.\n\n\n\n\n\n\nQuarto Vs. R Markdown\n\n\n\n\n\nFor those familiar with R Markdown, you can find a rundown of changes here.\nDue to Quarto being written as an evolution of R Markdown, it also supports most R Markdown syntax. While we could technically mix and match different types of syntax in a single document, this is bad practice. Readable code is consistent. Even if there are multiple ways to do something, it’s best to choose one way and stick with it throughout a code or document. For an example of how passionate programmers can get about consistencies in their code, check out the wikipedia article on indentation style."
  },
  {
    "objectID": "session-materials/session2/session2.html#footnotes",
    "href": "session-materials/session2/session2.html#footnotes",
    "title": "Session 2: Markdown and introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBeheshti, Mahdieh et al. “Association of Diabetes and Dental Caries Among U.S. Adolescents in the NHANES Dataset.” Pediatric dentistry vol. 43,2 (2021): 123-128.↩︎"
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html",
    "href": "session-materials/session0/bootcamp-1.html",
    "title": "Bootcamp Part 1: R Basics",
    "section": "",
    "text": "You can get output from R simply by typing math in the console:\n\n3 + 5\n\n[1] 8\n\n12 / 7\n\n[1] 1.714286\n\n\nHowever, to do useful and interesting things, we need to assign values to objects. To create an object, we need to give it a name followed by the assignment operator &lt;-, and the value we want to give it:\n\nweight_kg &lt;- 55\n\n&lt;- is the assignment operator. It assigns values on the right to objects on the left. So, after executing x &lt;- 3, the value of x is 3. The arrow can be read as 3 goes into x. For historical reasons, you can also use = for assignments, but not in every context. Because of the slight differences in syntax, it is good practice to always use &lt;- for assignments.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key) will write &lt;- in a single keystroke in a PC, while typing Option + - (push Option at the same time as the - key) does the same in a Mac.\n\n\nObjects can be given any name such as x, current_temperature, or subject_id. You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid, but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they are the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, T, mean, data, df, weights). If in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within an object name as in my.dataset. There are many functions in R with dots in their names for historical reasons, but because dots have a special meaning in R (for methods) and other programming languages, it’s best to avoid them. It is also recommended to use nouns for object names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name objects, etc.). Using a consistent coding style makes your code clearer to read for your future self and your collaborators. In R, some popular style guides are Google’s, the tidyverse’s style and the Bioconductor style guide. The tidyverse’s is very comprehensive and may seem overwhelming at first. You can install the lintr package to automatically check for issues in the styling of your code.\n\nObjects vs. variables: What are known as objects in R are known as variables in many other programming languages. Depending on the context, object and variable can have drastically different meanings. However, in this lesson, the two words are used synonymously. For more information see here.\n\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\nweight_kg &lt;- 55    # doesn't print anything\n(weight_kg &lt;- 55)  # but putting parenthesis around the call prints the value of `weight_kg`\n\n[1] 55\n\nweight_kg          # and so does typing the name of the object\n\n[1] 55\n\n\nNow that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change an object’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one object does not change the values of other objects For example, let’s store the animal’s weight in pounds in a new object, weight_lb:\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\n\n\n\n\n\n\nChallenge:\n\n\n\nWhat do you think is the current content of the object weight_lb? 126.5 or 220?"
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#creating-objects-in-r",
    "href": "session-materials/session0/bootcamp-1.html#creating-objects-in-r",
    "title": "Bootcamp Part 1: R Basics",
    "section": "",
    "text": "You can get output from R simply by typing math in the console:\n\n3 + 5\n\n[1] 8\n\n12 / 7\n\n[1] 1.714286\n\n\nHowever, to do useful and interesting things, we need to assign values to objects. To create an object, we need to give it a name followed by the assignment operator &lt;-, and the value we want to give it:\n\nweight_kg &lt;- 55\n\n&lt;- is the assignment operator. It assigns values on the right to objects on the left. So, after executing x &lt;- 3, the value of x is 3. The arrow can be read as 3 goes into x. For historical reasons, you can also use = for assignments, but not in every context. Because of the slight differences in syntax, it is good practice to always use &lt;- for assignments.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key) will write &lt;- in a single keystroke in a PC, while typing Option + - (push Option at the same time as the - key) does the same in a Mac.\n\n\nObjects can be given any name such as x, current_temperature, or subject_id. You want your object names to be explicit and not too long. They cannot start with a number (2x is not valid, but x2 is). R is case sensitive (e.g., weight_kg is different from Weight_kg). There are some names that cannot be used because they are the names of fundamental functions in R (e.g., if, else, for, see here for a complete list). In general, even if it’s allowed, it’s best to not use other function names (e.g., c, T, mean, data, df, weights). If in doubt, check the help to see if the name is already in use. It’s also best to avoid dots (.) within an object name as in my.dataset. There are many functions in R with dots in their names for historical reasons, but because dots have a special meaning in R (for methods) and other programming languages, it’s best to avoid them. It is also recommended to use nouns for object names, and verbs for function names. It’s important to be consistent in the styling of your code (where you put spaces, how you name objects, etc.). Using a consistent coding style makes your code clearer to read for your future self and your collaborators. In R, some popular style guides are Google’s, the tidyverse’s style and the Bioconductor style guide. The tidyverse’s is very comprehensive and may seem overwhelming at first. You can install the lintr package to automatically check for issues in the styling of your code.\n\nObjects vs. variables: What are known as objects in R are known as variables in many other programming languages. Depending on the context, object and variable can have drastically different meanings. However, in this lesson, the two words are used synonymously. For more information see here.\n\nWhen assigning a value to an object, R does not print anything. You can force R to print the value by using parentheses or by typing the object name:\n\nweight_kg &lt;- 55    # doesn't print anything\n(weight_kg &lt;- 55)  # but putting parenthesis around the call prints the value of `weight_kg`\n\n[1] 55\n\nweight_kg          # and so does typing the name of the object\n\n[1] 55\n\n\nNow that R has weight_kg in memory, we can do arithmetic with it. For instance, we may want to convert this weight into pounds (weight in pounds is 2.2 times the weight in kg):\n\n2.2 * weight_kg\n\n[1] 121\n\n\nWe can also change an object’s value by assigning it a new one:\n\nweight_kg &lt;- 57.5\n2.2 * weight_kg\n\n[1] 126.5\n\n\nThis means that assigning a value to one object does not change the values of other objects For example, let’s store the animal’s weight in pounds in a new object, weight_lb:\n\nweight_lb &lt;- 2.2 * weight_kg\n\nand then change weight_kg to 100.\n\nweight_kg &lt;- 100\n\n\n\n\n\n\n\nChallenge:\n\n\n\nWhat do you think is the current content of the object weight_lb? 126.5 or 220?"
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#comments",
    "href": "session-materials/session0/bootcamp-1.html#comments",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Comments",
    "text": "Comments\nThe comment character in R is #, anything to the right of a # in a script will be ignored by R. It is useful to leave notes, and explanations in your scripts.\nRStudio makes it easy to comment or uncomment a paragraph: after selecting the lines you want to comment, press at the same time on your keyboard Ctrl + Shift + C. If you only want to comment out one line, you can put the cursor at any location of that line (i.e. no need to select the whole line), then press Ctrl + Shift + C.\n\n\n\n\n\n\nChallenge\n\n\n\nWhat are the values after each statement in the following?\n\nmass &lt;- 47.5            # mass?\nage  &lt;- 122             # age?\nmass &lt;- mass * 2.0      # mass?\nage  &lt;- age - 20        # age?\nmass_index &lt;- mass/age  # mass_index?"
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#functions-and-their-arguments",
    "href": "session-materials/session0/bootcamp-1.html#functions-and-their-arguments",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Functions and their arguments",
    "text": "Functions and their arguments\nFunctions are “canned scripts” that automate more complicated sets of commands including operations assignments, etc. Many functions are predefined, or can be made available by importing R packages (more on that later). A function usually gets one or more inputs called arguments. Functions often (but not always) return a value. A typical example would be the function sqrt(). The input (the argument) must be a number, and the return value (in fact, the output) is the square root of that number. Executing a function (‘running it’) is called calling the function. An example of a function call is:\n\nb &lt;- sqrt(a)\n\nHere, the value of a is given to the sqrt() function, the sqrt() function calculates the square root, and returns the value which is then assigned to the object b. This function is very simple, because it takes just one argument.\nThe return ‘value’ of a function need not be numerical (like that of sqrt()), and it also does not need to be a single item: it can be a set of things, or even a dataset. We’ll see that when we read data files into R.\nArguments can be anything, not only numbers or filenames, but also other objects. Exactly what each argument means differs per function, and must be looked up in the documentation (see below). Some functions take arguments which may either be specified by the user, or, if left out, take on a default value: these are called options. Options are typically used to alter the way the function operates, such as whether it ignores ‘bad values’, or what symbol to use in a plot. However, if you want something specific, you can specify a value of your choice which will be used instead of the default.\nLet’s try a function that can take multiple arguments: round().\n\nround(3.14159)\n\n[1] 3\n\n\nHere, we’ve called round() with just one argument, 3.14159, and it has returned the value 3. That’s because the default is to round to the nearest whole number. If we want more digits we can see how to do that by getting information about the round function. We can use args(round) or look at the help for this function using ?round.\n\nargs(round)\n\nfunction (x, digits = 0) \nNULL\n\n\n\n?round\n\nWe see that if we want a different number of digits, we can type digits=2 or however many we want.\n\nround(3.14159, digits = 2)\n\n[1] 3.14\n\n\nIf you provide the arguments in the exact same order as they are defined you don’t have to name them:\n\nround(3.14159, 2)\n\n[1] 3.14\n\n\nAnd if you do name the arguments, you can switch their order:\n\nround(digits = 2, x = 3.14159)\n\n[1] 3.14\n\n\nIt’s good practice to put the non-optional arguments (like the number you’re rounding) first in your function call, and to specify the names of all optional arguments. If you don’t, someone reading your code might have to look up the definition of a function with unfamiliar arguments to understand what you’re doing. By specifying the name of the arguments you are also safeguarding against possible future changes in the function interface, which may potentially add new arguments in between the existing ones."
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#vectors-and-data-types",
    "href": "session-materials/session0/bootcamp-1.html#vectors-and-data-types",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Vectors and data types",
    "text": "Vectors and data types\nA vector is the most common and basic data type in R, and is pretty much the workhorse of R. A vector is composed by a series of values, such as numbers or characters. We can assign a series of values to a vector using the c() function. For example we can create a vector of animal weights and assign it to a new object weight_g:\n\nweight_g &lt;- c(50, 60, 65, 82)\nweight_g\n\n[1] 50 60 65 82\n\n\nA vector can also contain characters:\n\nstates &lt;- c(\"CT\", \"MA\", \"NY\")\nstates\n\n[1] \"CT\" \"MA\" \"NY\"\n\n\nThe quotes around “CT”, “MA”, etc. are essential here. Without the quotes R will assume there are objects called CT, MA and NY. As these objects don’t exist in R’s memory, there will be an error message.\nThere are many functions that allow you to inspect the content of a vector. length() tells you how many elements are in a particular vector:\n\nlength(weight_g)\n\n[1] 4\n\nlength(states)\n\n[1] 3\n\n\nAn important feature of a vector, is that all of the elements are the same type of data. The function class() indicates the class (the type of element) of an object:\n\nclass(weight_g)\n\n[1] \"numeric\"\n\nclass(states)\n\n[1] \"character\"\n\n\nThe function str() provides an overview of the structure of an object and its elements. It is a useful function when working with large and complex objects:\n\nstr(weight_g)\n\n num [1:4] 50 60 65 82\n\nstr(states)\n\n chr [1:3] \"CT\" \"MA\" \"NY\"\n\n\nYou can use the c() function to add other elements to your vector:\n\nweight_g &lt;- c(weight_g, 90) # add to the end of the vector\nweight_g &lt;- c(30, weight_g) # add to the beginning of the vector\nweight_g\n\n[1] 30 50 60 65 82 90\n\n\nIn the first line, we take the original vector weight_g, add the value 90 to the end of it, and save the result back into weight_g. Then we add the value 30 to the beginning, again saving the result back into weight_g.\nWe can do this over and over again to grow a vector, or assemble a dataset. As we program, this may be useful to add results that we are collecting or calculating.\nAn atomic vector is the simplest R data type and is a linear vector of a single type. Above, we saw 2 of the 6 main atomic vector types that R uses: \"character\" and \"numeric\" (or \"double\"). These are the basic building blocks that all R objects are built from. The other 4 atomic vector types are:\n\n\"logical\" for TRUE and FALSE (the boolean data type)\n\"integer\" for integer numbers (e.g., 2L, the L indicates to R that it’s an integer)\n\"complex\" to represent complex numbers with real and imaginary parts (e.g., 1 + 4i) and that’s all we’re going to say about them\n\"raw\" for bitstreams that we won’t discuss further\n\nYou can check the type of your vector using the typeof() function and inputting your vector as the argument.\nVectors are one of the many data structures that R uses. Other important ones are lists (list), matrices (matrix), data frames (data.frame), factors (factor) and arrays (array).\n\n\n\n\n\n\nChallenge:\n\n\n\nWe’ve seen that atomic vectors can be of type character, numeric (or double), integer, and logical. But what happens if we try to mix these types in a single vector?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nR implicitly converts them to all be the same type\n\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nWhat will happen in each of these examples? (hint: use class() to check the data type of your objects and type in their names to see what happens):\n\nnum_char &lt;- c(1, 2, 3, \"a\")\nnum_logical &lt;- c(1, 2, 3, TRUE, FALSE)\nchar_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\ntricky &lt;- c(1, 2, 3, \"4\")\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nclass(num_char)\n\n[1] \"character\"\n\nnum_char\n\n[1] \"1\" \"2\" \"3\" \"a\"\n\nclass(num_logical)\n\n[1] \"numeric\"\n\nnum_logical\n\n[1] 1 2 3 1 0\n\nclass(char_logical)\n\n[1] \"character\"\n\nchar_logical\n\n[1] \"a\"    \"b\"    \"c\"    \"TRUE\"\n\nclass(tricky)\n\n[1] \"character\"\n\ntricky\n\n[1] \"1\" \"2\" \"3\" \"4\"\n\n\n\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nWhy do you think it happens?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nVectors can be of only one data type. R tries to convert (coerce) the content of this vector to find a common denominator that doesn’t lose any information.\n\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nHow many values in combined_logical are \"TRUE\" (as a character) in the following example:\n\nnum_logical &lt;- c(1, 2, 3, TRUE)\nchar_logical &lt;- c(\"a\", \"b\", \"c\", TRUE)\ncombined_logical &lt;- c(num_logical, char_logical)\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOnly one. There is no memory of past data types, and the coercion happens the first time the vector is evaluated. Therefore, the TRUE in num_logical gets converted into a 1 before it gets converted into \"1\" in combined_logical.\n\ncombined_logical\n\n[1] \"1\"    \"2\"    \"3\"    \"1\"    \"a\"    \"b\"    \"c\"    \"TRUE\"\n\n\n\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nIn R, we call converting objects from one class into another class coercion. These conversions happen according to a hierarchy, whereby some types get preferentially coerced into other types. Can you draw a diagram that represents the hierarchy of how these data types are coerced?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nlogical → numeric → character ← logical"
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#subsetting-vectors",
    "href": "session-materials/session0/bootcamp-1.html#subsetting-vectors",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Subsetting vectors",
    "text": "Subsetting vectors\nIf we want to extract one or several values from a vector, we must provide one or several indices in square brackets. For instance:\n\nstates &lt;- c(\"CT\", \"MA\", \"NY\", \"PA\")\nstates[2]\n\n[1] \"MA\"\n\nstates[c(3, 2)]\n\n[1] \"NY\" \"MA\"\n\n\nWe can also repeat the indices to create an object with more elements than the original one:\n\nmore_states &lt;- states[c(1, 2, 3, 2, 1, 4)]\nmore_states\n\n[1] \"CT\" \"MA\" \"NY\" \"MA\" \"CT\" \"PA\"\n\n\nR indices start at 1. Programming languages like Fortran, MATLAB, Julia, and R start counting at 1, because that’s what human beings typically do. Languages in the C family (including C++, Java, Perl, and Python) count from 0 because that’s simpler for computers to do.\nFinally, it is also possible to get all the elements of a vector except some specified elements using negative indices:\n\nstates ## all states\n\n[1] \"CT\" \"MA\" \"NY\" \"PA\"\n\nstates[-1] ## all but the first one\n\n[1] \"MA\" \"NY\" \"PA\"\n\nstates[-c(1, 3)] ## all but 1st/3rd ones\n\n[1] \"MA\" \"PA\"\n\nstates[c(-1, -3)] ## all but 1st/3rd ones\n\n[1] \"MA\" \"PA\""
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#conditional-subsetting",
    "href": "session-materials/session0/bootcamp-1.html#conditional-subsetting",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Conditional subsetting",
    "text": "Conditional subsetting\nAnother common way of subsetting is by using a logical vector. TRUE will select the element with the same index, while FALSE will not:\n\nweight_g &lt;- c(21, 34, 39, 54, 55)\nweight_g[c(TRUE, FALSE, TRUE, TRUE, FALSE)]\n\n[1] 21 39 54\n\n\nTypically, these logical vectors are not typed by hand, but are the output of other functions or logical tests. For instance, if you wanted to select only the values above 50:\n\n## will return logicals with TRUE for the indices that meet\n## the condition\nweight_g &gt; 50\n\n[1] FALSE FALSE FALSE  TRUE  TRUE\n\n## so we can use this to select only the values above 50\nweight_g[weight_g &gt; 50]\n\n[1] 54 55\n\n\nYou can combine multiple tests using & (both conditions are true, AND) or | (at least one of the conditions is true, OR):\n\nweight_g[weight_g &lt; 30 | weight_g &gt; 50]\n\n[1] 21 54 55\n\nweight_g[weight_g &gt;= 30 & weight_g == 21]\n\nnumeric(0)\n\n\nHere, &lt; stands for “less than”, &gt; for “greater than”, &gt;= for “greater than or equal to”, and == for “equal to”. The double equal sign == is a test for numerical equality between the left and right hand sides, and should not be confused with the single = sign, which performs variable assignment (similar to &lt;-).\nA common task is to search for certain strings in a vector. One could use the “or” operator | to test for equality to multiple values, but this can quickly become tedious. The function %in% allows you to test if any of the elements of a search vector are found:\n\nstates &lt;- c(\"CT\", \"MA\", \"NY\", \"PA\")\nstates[states == \"MA\" | states == \"NY\"] # returns both MA and NY\n\n[1] \"MA\" \"NY\"\n\nstates %in% c(\"MA\", \"NY\", \"MA\", \"PA\", \"WI\")\n\n[1] FALSE  TRUE  TRUE  TRUE\n\nstates[states %in% c(\"MA\", \"NY\", \"MA\", \"PA\", \"WI\")]\n\n[1] \"MA\" \"NY\" \"PA\"\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nCan you figure out why \"four\" &gt; \"five\" returns TRUE?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\"four\" &gt; \"five\"\n\n[1] TRUE\n\n\nWhen using &gt; or &lt; on strings, R compares their alphabetical order. Here \"four\" comes after \"five\", and therefore is greater than it."
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#names",
    "href": "session-materials/session0/bootcamp-1.html#names",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Names",
    "text": "Names\nIt is possible to name each element of a vector. The code chunk below shows an initial vector without any names, how names are set, and retrieved.\n\nx &lt;- c(1, 5, 3, 5, 10)\nnames(x) ## no names\n\nNULL\n\nnames(x) &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\")\nnames(x) ## now we have names\n\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\n\nWhen a vector has names, it is possible to access elements by their name, in addition to their index.\n\nx[c(1, 3)]\n\nA C \n1 3 \n\nx[c(\"A\", \"C\")]\n\nA C \n1 3"
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#missing-data",
    "href": "session-materials/session0/bootcamp-1.html#missing-data",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Missing data",
    "text": "Missing data\nAs R was designed to analyze datasets, it includes the concept of missing data (which is uncommon in other programming languages). Missing data are represented in vectors as NA.\nWhen doing operations on numbers, most functions will return NA if the data you are working with include missing values. This feature makes it harder to overlook the cases where you are dealing with missing data. You can add the argument na.rm = TRUE to calculate the result while ignoring the missing values.\n\nheights &lt;- c(2, 4, 4, NA, 6)\nmean(heights)\n\n[1] NA\n\nmax(heights)\n\n[1] NA\n\nmean(heights, na.rm = TRUE)\n\n[1] 4\n\nmax(heights, na.rm = TRUE)\n\n[1] 6\n\n\nIf your data include missing values, you may want to become familiar with the functions is.na(), na.omit(), and complete.cases(). See below for examples.\n\n## Extract those elements which are not missing values.\nheights[!is.na(heights)]\n\n[1] 2 4 4 6\n\n## Returns the object with incomplete cases removed.\n## The returned object is an atomic vector of type `\"numeric\"`\n## (or `\"double\"`).\nna.omit(heights)\n\n[1] 2 4 4 6\nattr(,\"na.action\")\n[1] 4\nattr(,\"class\")\n[1] \"omit\"\n\n## Extract those elements which are complete cases.\n## The returned object is an atomic vector of type `\"numeric\"`\n## (or `\"double\"`).\nheights[complete.cases(heights)]\n\n[1] 2 4 4 6\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\n\nUsing this vector of heights in inches, create a new vector with the NAs removed.\n\n\nheights &lt;- c(63, 69, 60, 65, NA, 68, 61, 70, 61, 59, 64, 69, 63, 63, NA, 72, 65, 64, 70, 63, 65)\n\n\nUse the function median() to calculate the median of the heights vector.\nUse R to figure out how many people in the set are taller than 67 inches.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nheights_no_na &lt;- heights[!is.na(heights)]\n## or\nheights_no_na &lt;- na.omit(heights)\n\n\nmedian(heights, na.rm = TRUE)\n\n[1] 64\n\n\n\nheights_above_67 &lt;- heights_no_na[heights_no_na &gt; 67]\nlength(heights_above_67)\n\n[1] 6"
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#sec:genvec",
    "href": "session-materials/session0/bootcamp-1.html#sec:genvec",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Generating vectors",
    "text": "Generating vectors\nThere exists some functions to generate vectors of different type. To generate a vector of numerics, one can use the numeric() constructor, providing the length of the output vector as parameter. The values will be initialised with 0.\n\nnumeric(3)\n\n[1] 0 0 0\n\nnumeric(10)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\nNote that if we ask for a vector of numerics of length 0, we obtain exactly that:\n\nnumeric(0)\n\nnumeric(0)\n\n\nThere are similar constructors for characters and logicals, named character() and logical() respectively.\n\n\n\n\n\n\nChallenge:\n\n\n\nWhat are the defaults for character and logical vectors?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncharacter(2) ## the empty character\n\n[1] \"\" \"\"\n\nlogical(2)   ## FALSE\n\n[1] FALSE FALSE"
  },
  {
    "objectID": "session-materials/session0/bootcamp-1.html#categorical-data",
    "href": "session-materials/session0/bootcamp-1.html#categorical-data",
    "title": "Bootcamp Part 1: R Basics",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nFactors\nSince factors are special vectors, the same rules for selecting values using indices apply.\n\nexpression &lt;- factor(c(\"high\",\"low\",\"low\",\"medium\",\"high\",\"medium\",\"medium\",\"low\",\"low\",\"low\"))\n\nIn this vector we can imagine gene expression data has been stored as 3 categories or levels: low, medium, and high.\nLet’s extract the values of the factor with high expression:\n\nexpression[expression == \"high\"]    ## This will only return those elements in the factor equal to \"high\"\n\n[1] high high\nLevels: high low medium\n\n\nUnder the hood, factors are stored as integer values in R. To view the integer assignments under the hood you can use str():\n\nstr(expression)\n\n Factor w/ 3 levels \"high\",\"low\",\"medium\": 1 2 2 3 1 3 3 2 2 2\n\n\nThe categories are referred to as “factor levels”. As we learned earlier, the levels in the expression factor were assigned integers alphabetically, with high=1, low=2, medium=3. However, it makes more sense for us if low=1, medium=2 and high=3. We can change the order of the categories by releveling the factor.\nTo relevel the categories, you can add the levels argument to the factor() function, and give it a vector with the categories listed in the required order:\n\nexpression &lt;- factor(expression, levels=c(\"low\", \"medium\", \"high\"))     # you can re-factor a factor \n\nNow we have a releveled factor with low as the lowest or first category, medium as the second and high as the third. This is reflected in the way they are listed in the output of str(), as well as in the numbering of which category is where in the factor.\n\nstr(expression)\n\n Factor w/ 3 levels \"low\",\"medium\",..: 3 1 1 2 3 2 2 1 1 1\n\n\n\nNote: Releveling often becomes necessary when you need a specific category in a factor to be the “base” category, i.e. category that is equal to 1. One example would be if you need the “control” to be the “base” in a given RNA-seq experiment.\n\n\n\nData Frame\nA data.frame is the de facto data structure for most tabular data and what we use for statistics and plotting. A data.frame is similar to a matrix in that it’s a collection of vectors of the same length and each vector represents a column. However, in a dataframe each vector can be of a different data type (e.g., characters, integers, factors).\nWe can create a dataframe by bringing vectors together to form the columns. We do this using the data.frame() function, and giving the function the different vectors we would like to bind together. This function will only work for vectors of the same length.\n\n# Create a data frame and store it as a variable called 'df'\nages &lt;- c(12, 14, 14, 16, 12, 15)\nis_diabetic &lt;- c(TRUE, FALSE, FALSE, FALSE, TRUE, FALSE)\nsex &lt;- factor(c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\", \"Female\"))\ndf &lt;- data.frame(ages, is_diabetic, sex)\n\nWe can see that a new variable called df has been created in our Environment within a new section called Data. In the Environment, it specifies that df has 6 observations of 3 variables. What does that mean? In R, rows always come first, so it means that df has 6 rows and 3 columns. We can get additional information if we click on the blue circle with the white triangle in the middle next to df. It will display information about each of the columns in the data frame, giving information about what the data type is of each of the columns and the first few values of those columns.\nAnother handy feature in RStudio is that if we hover the cursor over the variable name in the Environment, df, it will turn into a pointing finger. If you click on df, it will open the data frame as it’s own tab next to the script editor. We can explore the table interactively within this window. To close, just click on the X on the tab.\nAs with any variable, we can print the values stored inside to the console if we type the variable’s name and run.\n\ndf\n\n  ages is_diabetic    sex\n1   12        TRUE   Male\n2   14       FALSE Female\n3   14       FALSE Female\n4   16       FALSE   Male\n5   12        TRUE   Male\n6   15       FALSE Female\n\n\n\n\nLists\nLists are a data structure in R that can be perhaps a bit daunting at first, but soon become amazingly useful. A list is a data structure that can hold any number of any types of other data structures.\nIf you have variables of different data structures you wish to combine, you can put all of those into one list object by using the list() function and placing all the items you wish to combine within parentheses:\n\nlist1 &lt;- list(ages, df, age)\n\nWe see list1 appear within the Data section of our environment as a list of 3 components or variables. If we click on the blue circle with a triangle in the middle, it’s not quite as interpretable as it was for data frames.\nEssentially, each component is preceded by a colon. The first colon give the expression vector, the second colon precedes the df data frame, with the dollar signs indicating the different columns, the last colon gives the single value, age.\nLet’s type list1 and print to the console by running it.\n\nlist1\n\n[[1]]\n[1] 12 14 14 16 12 15\n\n[[2]]\n  ages is_diabetic    sex\n1   12        TRUE   Male\n2   14       FALSE Female\n3   14       FALSE Female\n4   16       FALSE   Male\n5   12        TRUE   Male\n6   15       FALSE Female\n\n[[3]]\n[1] 102\n\n\nThere are three components corresponding to the three different variables we passed in, and what you see is that structure of each is retained. Each component of a list is referenced based on the number position.\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "resources/install.html",
    "href": "resources/install.html",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Open an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder.\n\n\n\n\n\n\n\n\nOpen an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the executable file. Run the .exe file and follow the installation instructions.\n\nPermissions\nNote that you should install R and RStudio to a drive where you have read and write permissions. Otherwise, your ability to install R packages (a frequent occurrence) will be impacted. If you encounter problems, try opening RStudio by right-clicking the icon and selecting “Run as administrator”. Other tips can be found in the page [R on network drives].\nHow to update R and RStudio\nYour version of R is printed to the R Console at start-up. You can also run sessionInfo().\nTo update R, go to the website mentioned above and re-install R. Be aware that the old R version will still exist in your computer. You can temporarily run an older version (older “installation”) of R by clicking “Tools” -&gt; “Global Options” in RStudio and choosing an R version. This can be useful if you want to use a package that has not been updated to work on the newest version of R.\nTo update RStudio, you can go to the website above and re-download RStudio. Another option is to click “Help” -&gt; “Check for Updates” within RStudio, but this may not show the very latest updates.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstructions adapted from guide developed by HMS Research computing and Chapter 3 of the The Epidemiologist R Handbook."
  },
  {
    "objectID": "resources/install.html#mac-users",
    "href": "resources/install.html#mac-users",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Open an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for (Mac) OS X” link at the top of the page.\nClick on the file containing the latest version of R under “Files.”\nSave the .pkg file, double-click it to open, and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder."
  },
  {
    "objectID": "resources/install.html#windows-users",
    "href": "resources/install.html#windows-users",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Open an internet browser and go to www.r-project.org.\nClick the “download R” link in the middle of the page under “Getting Started.”\nSelect a CRAN location (a mirror site) and click the corresponding link.\nClick on the “Download R for Windows” link at the top of the page.\nClick on the “install R for the first time” link at the top of the page.\nClick “Download R for Windows” and save the executable file somewhere on your computer. Run the .exe file and follow the installation instructions.\nNow that R is installed, you need to download and install RStudio.\n\n\n\n\n\nGo to www.rstudio.com and click on the “Download RStudio” button.\nClick on “DOWNLOAD” in the upper right corner.\nDownload the Free version of RStudio Desktop.\nSave the executable file. Run the .exe file and follow the installation instructions.\n\nPermissions\nNote that you should install R and RStudio to a drive where you have read and write permissions. Otherwise, your ability to install R packages (a frequent occurrence) will be impacted. If you encounter problems, try opening RStudio by right-clicking the icon and selecting “Run as administrator”. Other tips can be found in the page [R on network drives].\nHow to update R and RStudio\nYour version of R is printed to the R Console at start-up. You can also run sessionInfo().\nTo update R, go to the website mentioned above and re-install R. Be aware that the old R version will still exist in your computer. You can temporarily run an older version (older “installation”) of R by clicking “Tools” -&gt; “Global Options” in RStudio and choosing an R version. This can be useful if you want to use a package that has not been updated to work on the newest version of R.\nTo update RStudio, you can go to the website above and re-download RStudio. Another option is to click “Help” -&gt; “Check for Updates” within RStudio, but this may not show the very latest updates."
  },
  {
    "objectID": "resources/install.html#reference",
    "href": "resources/install.html#reference",
    "title": "Installing R and RStudio",
    "section": "",
    "text": "Instructions adapted from guide developed by HMS Research computing and Chapter 3 of the The Epidemiologist R Handbook."
  },
  {
    "objectID": "readings/using-tables.html",
    "href": "readings/using-tables.html",
    "title": "Basic Data Operations",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\n\n\nWe will continue to use the same linelist data we saw during Session 0. This is a fictional Ebola outbreak, expanded from the ebola_sim practice dataset in the outbreaks package.\n\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of linelist:\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse select() from dplyr to select the columns you want to retain, and to specify their order in the data frame.\nHere are ALL the column names in the linelist at this point in the cleaning pipe chain:\n\nnames(linelist)\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"age\"                 \n[10] \"age_unit\"             \"age_years\"            \"age_cat\"             \n[13] \"age_cat5\"             \"hospital\"             \"lon\"                 \n[16] \"lat\"                  \"infector\"             \"source\"              \n[19] \"wt_kg\"                \"ht_cm\"                \"ct_blood\"            \n[22] \"fever\"                \"chills\"               \"cough\"               \n[25] \"aches\"                \"vomit\"                \"temp\"                \n[28] \"time_admission\"       \"bmi\"                  \"days_onset_hosp\"     \n\n\n\n\nSelect only the columns you want to remain\nPut their names in the select() command, with no quotation marks. They will appear in the data frame in the order you provide. Note that if you include a column that does not exist, R will return an error (see use of any_of() below if you want no error in this situation).\n\n# linelist dataset is piped through select() command, and names() prints just the column names\nlinelist %&gt;% \n  select(case_id, date_onset, date_hospitalisation, fever) %&gt;% \n  names()  # display the column names\n\n[1] \"case_id\"              \"date_onset\"           \"date_hospitalisation\"\n[4] \"fever\"               \n\n\n\n\n\nIndicate which columns to remove by placing a minus symbol “-” in front of the column name (e.g. select(-outcome)), or a vector of column names (as below). All other columns will be retained.\n\nlinelist %&gt;% \n  select(-c(date_onset, fever:vomit)) %&gt;% # remove date_onset and all columns from fever to vomit\n  names()\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_hospitalisation\" \"date_outcome\"         \"outcome\"             \n [7] \"gender\"               \"age\"                  \"age_unit\"            \n[10] \"age_years\"            \"age_cat\"              \"age_cat5\"            \n[13] \"hospital\"             \"lon\"                  \"lat\"                 \n[16] \"infector\"             \"source\"               \"wt_kg\"               \n[19] \"ht_cm\"                \"ct_blood\"             \"temp\"                \n[22] \"time_admission\"       \"bmi\"                  \"days_onset_hosp\"     \n\n\nYou can also remove a column using base R syntax, by defining it as NULL. For example:\n\nlinelist$date_onset &lt;- NULL   # deletes column with base R syntax \n\n\n\n\nselect() can also be used as an independent command (not in a pipe chain). In this case, the first argument is the original dataframe to be operated upon.\n\n# Create a new linelist with id and age-related columns\nlinelist_age &lt;- select(linelist, case_id, contains(\"age\"))\n\n# display the column names\nnames(linelist_age)\n\n[1] \"case_id\"   \"age\"       \"age_unit\"  \"age_years\" \"age_cat\"   \"age_cat5\" \n\n\n\n\n\n\nIn addition to selecting columns, we can create new columns with mutate(). The syntax is: mutate(new_column_name = value or transformation). mutate() can also be used to modify an existing column.\n\n\nThe most basic mutate() command to create a new column might look like this. It creates a new column new_col where the value in every row is 10.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(new_col = 10)\n\nYou can also reference values in other columns, to perform calculations. Below, a new column bmi is created to hold the Body Mass Index (BMI) for each case - as calculated using the formula BMI = kg/m^2, using column ht_cm and column wt_kg.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(bmi = wt_kg / (ht_cm/100)^2)\n\nIf creating multiple new columns, separate each with a comma and new line. Below are examples of new columns, including ones that consist of values from other columns combined using str_glue() from the stringr package.\n\nnew_col_demo &lt;- linelist %&gt;%                       \n  mutate(\n    new_var_dup    = case_id,             # new column = duplicate/copy another existing column\n    new_var_static = 7,                   # new column = all values the same\n    new_var_static = new_var_static + 5,  # you can overwrite a column, and it can be a calculation using other variables\n    new_var_paste  = stringr::str_glue(\"{hospital} on ({date_hospitalisation})\") # new column = pasting together values from other columns\n    ) %&gt;% \n  select(case_id, hospital, date_hospitalisation, contains(\"new\"))        # show only new columns, for demonstration purposes\n\nReview the new columns. For demonstration purposes, only the new columns and the columns used to create them are shown:\n\n\n   case_id                             hospital date_hospitalisation\n1   5fe599                                Other           2014-05-15\n2   8689b7                              Missing           2014-05-14\n3   11f8ea St. Mark's Maternity Hospital (SMMH)           2014-05-18\n4   b8812a                        Port Hospital           2014-05-20\n5   893f25                    Military Hospital           2014-05-22\n6   be99c8                        Port Hospital           2014-05-23\n7   07e3e8                              Missing           2014-05-29\n8   369449                              Missing           2014-06-03\n9   f393b4                              Missing           2014-06-06\n10  1389ca                              Missing           2014-06-07\n11  2978ac                        Port Hospital           2014-06-08\n12  57a565                    Military Hospital           2014-06-15\n13  fc15ef                              Missing           2014-06-17\n14  2eaa9a                              Missing           2014-06-17\n15  bbfa93                                Other           2014-06-20\n16  c97dd9                        Port Hospital           2014-06-19\n17  f50e8a                        Port Hospital           2014-06-23\n18  3a7673                        Port Hospital           2014-06-24\n19  7f5a01                              Missing           2014-06-27\n20  ddddee                                Other           2014-06-28\n21  99e8fa                        Port Hospital           2014-06-29\n22  567136                        Port Hospital           2014-07-03\n23  9371a9 St. Mark's Maternity Hospital (SMMH)           2014-07-09\n24  bc2adf                              Missing           2014-07-09\n25  403057                                Other           2014-07-11\n26  8bd1e8                              Missing           2014-07-11\n27  f327be St. Mark's Maternity Hospital (SMMH)           2014-07-13\n28  42e1a9                    Military Hospital           2014-07-14\n29  90e5fe                        Port Hospital           2014-07-14\n30  959170                     Central Hospital           2014-07-13\n31  8ebf6e                    Military Hospital           2014-07-14\n32  e56412                     Central Hospital           2014-07-17\n33  6d788e                              Missing           2014-07-17\n34  a47529                    Military Hospital           2014-07-18\n35  67be4e                                Other           2014-07-19\n36  da8ecb                              Missing           2014-07-20\n37  148f18                              Missing           2014-07-20\n38  2cb9a5                        Port Hospital           2014-07-22\n39  f5c142                        Port Hospital           2014-07-24\n40  70a9fe                        Port Hospital           2014-07-26\n41  3ad520                              Missing           2014-07-24\n42  062638                     Central Hospital           2014-07-27\n43  c76676                    Military Hospital           2014-07-25\n44  baacc1                                Other           2014-07-27\n45  497372                                Other           2014-07-31\n46  23e499                                Other           2014-08-01\n47  38cc4a                              Missing           2014-08-03\n48  3789ee St. Mark's Maternity Hospital (SMMH)           2014-08-02\n49  c71dcd St. Mark's Maternity Hospital (SMMH)           2014-08-02\n50  6b70f0                              Missing           2014-08-04\n   new_var_dup new_var_static\n1       5fe599             12\n2       8689b7             12\n3       11f8ea             12\n4       b8812a             12\n5       893f25             12\n6       be99c8             12\n7       07e3e8             12\n8       369449             12\n9       f393b4             12\n10      1389ca             12\n11      2978ac             12\n12      57a565             12\n13      fc15ef             12\n14      2eaa9a             12\n15      bbfa93             12\n16      c97dd9             12\n17      f50e8a             12\n18      3a7673             12\n19      7f5a01             12\n20      ddddee             12\n21      99e8fa             12\n22      567136             12\n23      9371a9             12\n24      bc2adf             12\n25      403057             12\n26      8bd1e8             12\n27      f327be             12\n28      42e1a9             12\n29      90e5fe             12\n30      959170             12\n31      8ebf6e             12\n32      e56412             12\n33      6d788e             12\n34      a47529             12\n35      67be4e             12\n36      da8ecb             12\n37      148f18             12\n38      2cb9a5             12\n39      f5c142             12\n40      70a9fe             12\n41      3ad520             12\n42      062638             12\n43      c76676             12\n44      baacc1             12\n45      497372             12\n46      23e499             12\n47      38cc4a             12\n48      3789ee             12\n49      c71dcd             12\n50      6b70f0             12\n                                          new_var_paste\n1                                 Other on (2014-05-15)\n2                               Missing on (2014-05-14)\n3  St. Mark's Maternity Hospital (SMMH) on (2014-05-18)\n4                         Port Hospital on (2014-05-20)\n5                     Military Hospital on (2014-05-22)\n6                         Port Hospital on (2014-05-23)\n7                               Missing on (2014-05-29)\n8                               Missing on (2014-06-03)\n9                               Missing on (2014-06-06)\n10                              Missing on (2014-06-07)\n11                        Port Hospital on (2014-06-08)\n12                    Military Hospital on (2014-06-15)\n13                              Missing on (2014-06-17)\n14                              Missing on (2014-06-17)\n15                                Other on (2014-06-20)\n16                        Port Hospital on (2014-06-19)\n17                        Port Hospital on (2014-06-23)\n18                        Port Hospital on (2014-06-24)\n19                              Missing on (2014-06-27)\n20                                Other on (2014-06-28)\n21                        Port Hospital on (2014-06-29)\n22                        Port Hospital on (2014-07-03)\n23 St. Mark's Maternity Hospital (SMMH) on (2014-07-09)\n24                              Missing on (2014-07-09)\n25                                Other on (2014-07-11)\n26                              Missing on (2014-07-11)\n27 St. Mark's Maternity Hospital (SMMH) on (2014-07-13)\n28                    Military Hospital on (2014-07-14)\n29                        Port Hospital on (2014-07-14)\n30                     Central Hospital on (2014-07-13)\n31                    Military Hospital on (2014-07-14)\n32                     Central Hospital on (2014-07-17)\n33                              Missing on (2014-07-17)\n34                    Military Hospital on (2014-07-18)\n35                                Other on (2014-07-19)\n36                              Missing on (2014-07-20)\n37                              Missing on (2014-07-20)\n38                        Port Hospital on (2014-07-22)\n39                        Port Hospital on (2014-07-24)\n40                        Port Hospital on (2014-07-26)\n41                              Missing on (2014-07-24)\n42                     Central Hospital on (2014-07-27)\n43                    Military Hospital on (2014-07-25)\n44                                Other on (2014-07-27)\n45                                Other on (2014-07-31)\n46                                Other on (2014-08-01)\n47                              Missing on (2014-08-03)\n48 St. Mark's Maternity Hospital (SMMH) on (2014-08-02)\n49 St. Mark's Maternity Hospital (SMMH) on (2014-08-02)\n50                              Missing on (2014-08-04)\n\n\n\n\n\n\n\n\nTransmute\n\n\n\nA variation on mutate() is the function transmute(). This function adds a new column just like mutate(), but also drops/removes all other columns that you do not mention within its parentheses.\n\n\n\n\n\nColumns containing values that are dates, numbers, or logical values (TRUE/FALSE) will only behave as expected if they are correctly classified. There is a difference between “2” of class character and 2 of class numeric! There are ways to set column class during the import commands, but this is often cumbersome.\nFirst, let’s run some checks on important columns to see if they are the correct class. Currently, the class of the age column is character. To perform quantitative analyses, we need these numbers to be recognized as numeric!\n\nclass(linelist$age)\n\n[1] \"numeric\"\n\n\nTo resolve this, use the ability of mutate() to re-define a column with a transformation. We define the column as itself, but converted to a different class. Here is a basic example, converting or ensuring that the column age is class Numeric:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(age = as.numeric(age))\n\nIn a similar way, you can use as.character() and as.logical(). To convert to class Factor, you can use factor().\n\n\n\n\nA typical cleaning step after you have cleaned the columns and re-coded values is to filter the data frame for specific rows using the dplyr verb filter().\nWithin filter(), specify the logic that must be TRUE for a row in the dataset to be kept. Below we show how to filter rows based on simple and complex logical conditions.\n\n\n\nThis simple example re-defines the dataframe linelist as itself, having filtered the rows to meet a logical condition. Only the rows where the logical statement within the parentheses evaluates to TRUE are kept.\nIn this example, the logical statement is gender == \"f\", which is asking whether the value in the column gender is equal to “f” (case sensitive).\nBefore the filter is applied, the number of rows in linelist is nrow(linelist).\n\nlinelist &lt;- linelist %&gt;% \n  filter(gender == \"f\")   # keep only rows where gender is equal to \"f\"\n\nAfter the filter is applied, the number of rows in linelist is linelist %&gt;% filter(gender == \"f\") %&gt;% nrow().\n\n\n\nMore complex logical statements can be constructed using parentheses ( ), OR |, negate !, %in%, and AND & operators. An example is below:\nNote: You can use the ! operator in front of a logical criteria to negate it. For example, !is.na(column) evaluates to true if the column value is not missing. Likewise !column %in% c(\"a\", \"b\", \"c\") evaluates to true if the column value is not in the vector.\n\n\nBelow is a simple one-line command to create a histogram of onset dates. See that a second smaller outbreak from 2012-2013 is also included in this raw dataset. For our analyses, we want to remove entries from this earlier outbreak.\n\nhist(linelist$date_onset, breaks = 50)\n\n\n\n\n\n\n\nCan we just filter by date_onset to rows after June 2013? Caution! Applying the code filter(date_onset &gt; as.Date(\"2013-06-01\"))) would remove any rows in the later epidemic with a missing date of onset!\n\n\n\n\n\n\nConditions with NA\n\n\n\nFiltering to greater than (&gt;) or less than (&lt;) a date or number can remove any rows with missing values (NA)! This is because NA is treated as infinitely large and small.\n\n\n\n\n\n\nFiltering can also be done as a stand-alone command (not part of a pipe chain). Like other dplyr verbs, in this case the first argument must be the dataset itself.\n\n# dataframe &lt;- filter(dataframe, condition(s) for rows to keep)\n\nlinelist &lt;- filter(linelist, !is.na(case_id))\n\nYou can also use base R to subset using square brackets which reflect the [rows, columns] that you want to retain.\n\n# dataframe &lt;- dataframe[row conditions, column conditions] (blank means keep all)\n\nlinelist &lt;- linelist[!is.na(case_id), ]\n\n\n\n\n\n\nUse the dplyr function arrange() to sort or order the rows by column values.\nSimple list the columns in the order they should be sorted on. Specify .by_group = TRUE if you want the sorting to to first occur by any groupings applied to the data.\nBy default, column will be sorted in “ascending” order (which applies to numeric and also to character columns). You can sort a variable in “descending” order by wrapping it with desc().\nSorting data with arrange() is particularly useful when making tables for publication, using slice() to take the “top” rows per group, or setting factor level order by order of appearance.\nFor example, to sort the our linelist rows by hospital, then by date_onset in descending order, we would use:\n\nlinelist %&gt;% \n   arrange(hospital, desc(date_onset))"
  },
  {
    "objectID": "readings/using-tables.html#select-or-re-order-columns",
    "href": "readings/using-tables.html#select-or-re-order-columns",
    "title": "Basic Data Operations",
    "section": "",
    "text": "Use select() from dplyr to select the columns you want to retain, and to specify their order in the data frame.\nHere are ALL the column names in the linelist at this point in the cleaning pipe chain:\n\nnames(linelist)\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"age\"                 \n[10] \"age_unit\"             \"age_years\"            \"age_cat\"             \n[13] \"age_cat5\"             \"hospital\"             \"lon\"                 \n[16] \"lat\"                  \"infector\"             \"source\"              \n[19] \"wt_kg\"                \"ht_cm\"                \"ct_blood\"            \n[22] \"fever\"                \"chills\"               \"cough\"               \n[25] \"aches\"                \"vomit\"                \"temp\"                \n[28] \"time_admission\"       \"bmi\"                  \"days_onset_hosp\"     \n\n\n\n\nSelect only the columns you want to remain\nPut their names in the select() command, with no quotation marks. They will appear in the data frame in the order you provide. Note that if you include a column that does not exist, R will return an error (see use of any_of() below if you want no error in this situation).\n\n# linelist dataset is piped through select() command, and names() prints just the column names\nlinelist %&gt;% \n  select(case_id, date_onset, date_hospitalisation, fever) %&gt;% \n  names()  # display the column names\n\n[1] \"case_id\"              \"date_onset\"           \"date_hospitalisation\"\n[4] \"fever\"               \n\n\n\n\n\nIndicate which columns to remove by placing a minus symbol “-” in front of the column name (e.g. select(-outcome)), or a vector of column names (as below). All other columns will be retained.\n\nlinelist %&gt;% \n  select(-c(date_onset, fever:vomit)) %&gt;% # remove date_onset and all columns from fever to vomit\n  names()\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_hospitalisation\" \"date_outcome\"         \"outcome\"             \n [7] \"gender\"               \"age\"                  \"age_unit\"            \n[10] \"age_years\"            \"age_cat\"              \"age_cat5\"            \n[13] \"hospital\"             \"lon\"                  \"lat\"                 \n[16] \"infector\"             \"source\"               \"wt_kg\"               \n[19] \"ht_cm\"                \"ct_blood\"             \"temp\"                \n[22] \"time_admission\"       \"bmi\"                  \"days_onset_hosp\"     \n\n\nYou can also remove a column using base R syntax, by defining it as NULL. For example:\n\nlinelist$date_onset &lt;- NULL   # deletes column with base R syntax \n\n\n\n\nselect() can also be used as an independent command (not in a pipe chain). In this case, the first argument is the original dataframe to be operated upon.\n\n# Create a new linelist with id and age-related columns\nlinelist_age &lt;- select(linelist, case_id, contains(\"age\"))\n\n# display the column names\nnames(linelist_age)\n\n[1] \"case_id\"   \"age\"       \"age_unit\"  \"age_years\" \"age_cat\"   \"age_cat5\""
  },
  {
    "objectID": "readings/using-tables.html#column-creation-and-transformation",
    "href": "readings/using-tables.html#column-creation-and-transformation",
    "title": "Basic Data Operations",
    "section": "",
    "text": "In addition to selecting columns, we can create new columns with mutate(). The syntax is: mutate(new_column_name = value or transformation). mutate() can also be used to modify an existing column.\n\n\nThe most basic mutate() command to create a new column might look like this. It creates a new column new_col where the value in every row is 10.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(new_col = 10)\n\nYou can also reference values in other columns, to perform calculations. Below, a new column bmi is created to hold the Body Mass Index (BMI) for each case - as calculated using the formula BMI = kg/m^2, using column ht_cm and column wt_kg.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(bmi = wt_kg / (ht_cm/100)^2)\n\nIf creating multiple new columns, separate each with a comma and new line. Below are examples of new columns, including ones that consist of values from other columns combined using str_glue() from the stringr package.\n\nnew_col_demo &lt;- linelist %&gt;%                       \n  mutate(\n    new_var_dup    = case_id,             # new column = duplicate/copy another existing column\n    new_var_static = 7,                   # new column = all values the same\n    new_var_static = new_var_static + 5,  # you can overwrite a column, and it can be a calculation using other variables\n    new_var_paste  = stringr::str_glue(\"{hospital} on ({date_hospitalisation})\") # new column = pasting together values from other columns\n    ) %&gt;% \n  select(case_id, hospital, date_hospitalisation, contains(\"new\"))        # show only new columns, for demonstration purposes\n\nReview the new columns. For demonstration purposes, only the new columns and the columns used to create them are shown:\n\n\n   case_id                             hospital date_hospitalisation\n1   5fe599                                Other           2014-05-15\n2   8689b7                              Missing           2014-05-14\n3   11f8ea St. Mark's Maternity Hospital (SMMH)           2014-05-18\n4   b8812a                        Port Hospital           2014-05-20\n5   893f25                    Military Hospital           2014-05-22\n6   be99c8                        Port Hospital           2014-05-23\n7   07e3e8                              Missing           2014-05-29\n8   369449                              Missing           2014-06-03\n9   f393b4                              Missing           2014-06-06\n10  1389ca                              Missing           2014-06-07\n11  2978ac                        Port Hospital           2014-06-08\n12  57a565                    Military Hospital           2014-06-15\n13  fc15ef                              Missing           2014-06-17\n14  2eaa9a                              Missing           2014-06-17\n15  bbfa93                                Other           2014-06-20\n16  c97dd9                        Port Hospital           2014-06-19\n17  f50e8a                        Port Hospital           2014-06-23\n18  3a7673                        Port Hospital           2014-06-24\n19  7f5a01                              Missing           2014-06-27\n20  ddddee                                Other           2014-06-28\n21  99e8fa                        Port Hospital           2014-06-29\n22  567136                        Port Hospital           2014-07-03\n23  9371a9 St. Mark's Maternity Hospital (SMMH)           2014-07-09\n24  bc2adf                              Missing           2014-07-09\n25  403057                                Other           2014-07-11\n26  8bd1e8                              Missing           2014-07-11\n27  f327be St. Mark's Maternity Hospital (SMMH)           2014-07-13\n28  42e1a9                    Military Hospital           2014-07-14\n29  90e5fe                        Port Hospital           2014-07-14\n30  959170                     Central Hospital           2014-07-13\n31  8ebf6e                    Military Hospital           2014-07-14\n32  e56412                     Central Hospital           2014-07-17\n33  6d788e                              Missing           2014-07-17\n34  a47529                    Military Hospital           2014-07-18\n35  67be4e                                Other           2014-07-19\n36  da8ecb                              Missing           2014-07-20\n37  148f18                              Missing           2014-07-20\n38  2cb9a5                        Port Hospital           2014-07-22\n39  f5c142                        Port Hospital           2014-07-24\n40  70a9fe                        Port Hospital           2014-07-26\n41  3ad520                              Missing           2014-07-24\n42  062638                     Central Hospital           2014-07-27\n43  c76676                    Military Hospital           2014-07-25\n44  baacc1                                Other           2014-07-27\n45  497372                                Other           2014-07-31\n46  23e499                                Other           2014-08-01\n47  38cc4a                              Missing           2014-08-03\n48  3789ee St. Mark's Maternity Hospital (SMMH)           2014-08-02\n49  c71dcd St. Mark's Maternity Hospital (SMMH)           2014-08-02\n50  6b70f0                              Missing           2014-08-04\n   new_var_dup new_var_static\n1       5fe599             12\n2       8689b7             12\n3       11f8ea             12\n4       b8812a             12\n5       893f25             12\n6       be99c8             12\n7       07e3e8             12\n8       369449             12\n9       f393b4             12\n10      1389ca             12\n11      2978ac             12\n12      57a565             12\n13      fc15ef             12\n14      2eaa9a             12\n15      bbfa93             12\n16      c97dd9             12\n17      f50e8a             12\n18      3a7673             12\n19      7f5a01             12\n20      ddddee             12\n21      99e8fa             12\n22      567136             12\n23      9371a9             12\n24      bc2adf             12\n25      403057             12\n26      8bd1e8             12\n27      f327be             12\n28      42e1a9             12\n29      90e5fe             12\n30      959170             12\n31      8ebf6e             12\n32      e56412             12\n33      6d788e             12\n34      a47529             12\n35      67be4e             12\n36      da8ecb             12\n37      148f18             12\n38      2cb9a5             12\n39      f5c142             12\n40      70a9fe             12\n41      3ad520             12\n42      062638             12\n43      c76676             12\n44      baacc1             12\n45      497372             12\n46      23e499             12\n47      38cc4a             12\n48      3789ee             12\n49      c71dcd             12\n50      6b70f0             12\n                                          new_var_paste\n1                                 Other on (2014-05-15)\n2                               Missing on (2014-05-14)\n3  St. Mark's Maternity Hospital (SMMH) on (2014-05-18)\n4                         Port Hospital on (2014-05-20)\n5                     Military Hospital on (2014-05-22)\n6                         Port Hospital on (2014-05-23)\n7                               Missing on (2014-05-29)\n8                               Missing on (2014-06-03)\n9                               Missing on (2014-06-06)\n10                              Missing on (2014-06-07)\n11                        Port Hospital on (2014-06-08)\n12                    Military Hospital on (2014-06-15)\n13                              Missing on (2014-06-17)\n14                              Missing on (2014-06-17)\n15                                Other on (2014-06-20)\n16                        Port Hospital on (2014-06-19)\n17                        Port Hospital on (2014-06-23)\n18                        Port Hospital on (2014-06-24)\n19                              Missing on (2014-06-27)\n20                                Other on (2014-06-28)\n21                        Port Hospital on (2014-06-29)\n22                        Port Hospital on (2014-07-03)\n23 St. Mark's Maternity Hospital (SMMH) on (2014-07-09)\n24                              Missing on (2014-07-09)\n25                                Other on (2014-07-11)\n26                              Missing on (2014-07-11)\n27 St. Mark's Maternity Hospital (SMMH) on (2014-07-13)\n28                    Military Hospital on (2014-07-14)\n29                        Port Hospital on (2014-07-14)\n30                     Central Hospital on (2014-07-13)\n31                    Military Hospital on (2014-07-14)\n32                     Central Hospital on (2014-07-17)\n33                              Missing on (2014-07-17)\n34                    Military Hospital on (2014-07-18)\n35                                Other on (2014-07-19)\n36                              Missing on (2014-07-20)\n37                              Missing on (2014-07-20)\n38                        Port Hospital on (2014-07-22)\n39                        Port Hospital on (2014-07-24)\n40                        Port Hospital on (2014-07-26)\n41                              Missing on (2014-07-24)\n42                     Central Hospital on (2014-07-27)\n43                    Military Hospital on (2014-07-25)\n44                                Other on (2014-07-27)\n45                                Other on (2014-07-31)\n46                                Other on (2014-08-01)\n47                              Missing on (2014-08-03)\n48 St. Mark's Maternity Hospital (SMMH) on (2014-08-02)\n49 St. Mark's Maternity Hospital (SMMH) on (2014-08-02)\n50                              Missing on (2014-08-04)\n\n\n\n\n\n\n\n\nTransmute\n\n\n\nA variation on mutate() is the function transmute(). This function adds a new column just like mutate(), but also drops/removes all other columns that you do not mention within its parentheses.\n\n\n\n\n\nColumns containing values that are dates, numbers, or logical values (TRUE/FALSE) will only behave as expected if they are correctly classified. There is a difference between “2” of class character and 2 of class numeric! There are ways to set column class during the import commands, but this is often cumbersome.\nFirst, let’s run some checks on important columns to see if they are the correct class. Currently, the class of the age column is character. To perform quantitative analyses, we need these numbers to be recognized as numeric!\n\nclass(linelist$age)\n\n[1] \"numeric\"\n\n\nTo resolve this, use the ability of mutate() to re-define a column with a transformation. We define the column as itself, but converted to a different class. Here is a basic example, converting or ensuring that the column age is class Numeric:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(age = as.numeric(age))\n\nIn a similar way, you can use as.character() and as.logical(). To convert to class Factor, you can use factor()."
  },
  {
    "objectID": "readings/using-tables.html#filter-rows",
    "href": "readings/using-tables.html#filter-rows",
    "title": "Basic Data Operations",
    "section": "",
    "text": "A typical cleaning step after you have cleaned the columns and re-coded values is to filter the data frame for specific rows using the dplyr verb filter().\nWithin filter(), specify the logic that must be TRUE for a row in the dataset to be kept. Below we show how to filter rows based on simple and complex logical conditions.\n\n\n\nThis simple example re-defines the dataframe linelist as itself, having filtered the rows to meet a logical condition. Only the rows where the logical statement within the parentheses evaluates to TRUE are kept.\nIn this example, the logical statement is gender == \"f\", which is asking whether the value in the column gender is equal to “f” (case sensitive).\nBefore the filter is applied, the number of rows in linelist is nrow(linelist).\n\nlinelist &lt;- linelist %&gt;% \n  filter(gender == \"f\")   # keep only rows where gender is equal to \"f\"\n\nAfter the filter is applied, the number of rows in linelist is linelist %&gt;% filter(gender == \"f\") %&gt;% nrow().\n\n\n\nMore complex logical statements can be constructed using parentheses ( ), OR |, negate !, %in%, and AND & operators. An example is below:\nNote: You can use the ! operator in front of a logical criteria to negate it. For example, !is.na(column) evaluates to true if the column value is not missing. Likewise !column %in% c(\"a\", \"b\", \"c\") evaluates to true if the column value is not in the vector.\n\n\nBelow is a simple one-line command to create a histogram of onset dates. See that a second smaller outbreak from 2012-2013 is also included in this raw dataset. For our analyses, we want to remove entries from this earlier outbreak.\n\nhist(linelist$date_onset, breaks = 50)\n\n\n\n\n\n\n\nCan we just filter by date_onset to rows after June 2013? Caution! Applying the code filter(date_onset &gt; as.Date(\"2013-06-01\"))) would remove any rows in the later epidemic with a missing date of onset!\n\n\n\n\n\n\nConditions with NA\n\n\n\nFiltering to greater than (&gt;) or less than (&lt;) a date or number can remove any rows with missing values (NA)! This is because NA is treated as infinitely large and small.\n\n\n\n\n\n\nFiltering can also be done as a stand-alone command (not part of a pipe chain). Like other dplyr verbs, in this case the first argument must be the dataset itself.\n\n# dataframe &lt;- filter(dataframe, condition(s) for rows to keep)\n\nlinelist &lt;- filter(linelist, !is.na(case_id))\n\nYou can also use base R to subset using square brackets which reflect the [rows, columns] that you want to retain.\n\n# dataframe &lt;- dataframe[row conditions, column conditions] (blank means keep all)\n\nlinelist &lt;- linelist[!is.na(case_id), ]"
  },
  {
    "objectID": "readings/using-tables.html#arrange-and-sort",
    "href": "readings/using-tables.html#arrange-and-sort",
    "title": "Basic Data Operations",
    "section": "",
    "text": "Use the dplyr function arrange() to sort or order the rows by column values.\nSimple list the columns in the order they should be sorted on. Specify .by_group = TRUE if you want the sorting to to first occur by any groupings applied to the data.\nBy default, column will be sorted in “ascending” order (which applies to numeric and also to character columns). You can sort a variable in “descending” order by wrapping it with desc().\nSorting data with arrange() is particularly useful when making tables for publication, using slice() to take the “top” rows per group, or setting factor level order by order of appearance.\nFor example, to sort the our linelist rows by hospital, then by date_onset in descending order, we would use:\n\nlinelist %&gt;% \n   arrange(hospital, desc(date_onset))"
  },
  {
    "objectID": "readings/simple-tests.html",
    "href": "readings/simple-tests.html",
    "title": "Simple statistical tests",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\nThis page demonstrates how to conduct simple statistical tests using base R, rstatix, and gtsummary.\n\nT-test\n\nShapiro-Wilk test\n\nWilcoxon rank sum test\n\nKruskal-Wallis test\n\nChi-squared test\n\nCorrelations between numeric variables\n\n…many other tests can be performed, but we showcase just these common ones and link to further documentation.\nEach of the above packages bring certain advantages and disadvantages:\n\nUse base R functions to print a statistical outputs to the R Console\n\nUse rstatix functions to return results in a data frame, or if you want tests to run by group\n\nUse gtsummary if you want to quickly print publication-ready tables\n\n\n\n\n\n\n\nLoad packages\n\n\n\n\n\n\npacman::p_load(\n  rio,          # File import\n  here,         # File locator\n  skimr,        # get overview of data\n  tidyverse,    # data management + ggplot2 graphics, \n  gtsummary,    # summary statistics and tests\n  rstatix,      # statistics\n  corrr,        # correlation analayis for numeric variables\n  janitor,      # adding totals and percents to tables\n  flextable     # converting tables to HTML\n  )\n\n\n\n\n\n\nLet’s look a classic example in statistics: flipping a coin to see if it is fair. We flip the coin 100 times and each time record whether it came up heads or tails. So, we have a record that could look something like HHTTHTHTT...\nLet’s simulate the experiment in R, using a biased coin:\n\nset.seed(0xdada)\nnumFlips = 100\nprobHead = 0.6\n# Sample is a function in base R which let's us take a random sample from a vector, with or without replacement. \n# This line is sampling numFlips times from the vector ['H','T'] with replacement, with the probabilities for \n# each item in the vector being defined in the prob argument as [probHead, 1-probHead]\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\n# Thus, coinFlips is a character vector of a random sequence of 'T' and 'H'. \nhead(coinFlips)\n\n[1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n\n\nNow, if the coin were fair, we would expect half of the time to get heads. Let’s see.\n\ntable(coinFlips)\n\ncoinFlips\n H  T \n59 41 \n\n\nThat is different from 50/50. However, does the data deviates strong enough to conclude that this coin isn’t fair? We know that the total number of heads seen in 100 coin tosses for a fair coin follows \\(B(100, 0.5)\\), making it a suitable test statistic.\nTo decide, let’s look at the sampling distribution of our test statistic – the total number of heads seen in 100 coin tosses – for a fair coin. As we learned, we can do this with the binomial distribution. Let’s plot a fair coin and mark our observation with a blue line:\n\n# This line sets k as the vector [0, 1, 2,...,numFlips]\nk &lt;- 0:numFlips\n# Recall that binary variables (TRUE and FALSE) are interpreted as 1 and 0, so we can use this operation\n# to count the number of heads in coinFlips. We practice these kinds of operations in session 3. \nnumHeads &lt;- sum(coinFlips == \"H\")\n# We use dbinom here to get the probability mass at every integer from 1-numFlips so that we can plot the distribution. \np &lt;- dbinom(k, size = numFlips, prob = 0.5)\n# We then convert it into a dataframe for easier plotting. \nbinomDensity &lt;- data.frame(k = k, p = p)\nhead(binomDensity)\n\n  k            p\n1 0 7.888609e-31\n2 1 7.888609e-29\n3 2 3.904861e-27\n4 3 1.275588e-25\n5 4 3.093301e-24\n6 5 5.939138e-23\n\n# Here, we are plotting the binomial distribution, with a vertical line representing\n# the number of heads we actually observed. We will learn how to create plots in session 4. \n# Thus, to complete our test we simply need to identify whether or not the blue line\n# is in our rejection region. \nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p), stat = \"identity\") +\n  geom_vline(xintercept = numHeads, col = \"blue\")\n\n\n\n\nHow do we quantify whether the observed value is among those values that we are likely to see from a fair coin, or whether its deviation from the expected value is already large enough for us to conclude with enough confidence that the coin is biased?\nWe divide the set of all possible \\(k(0-100)\\) in two complementary subsets, the rejection region and the region of no rejection. We want to make the rejection region as large as possible while keeping their total probability, assuming the null hypothesis, below some threshold \\(\\alpha\\)(say, 0.05).\n\nalpha &lt;- 0.05\n# We get the density of our plot in sorted order, meaning that we'll see binomDensity\n# jump back and forth between the distribution's tails as p increases. \nbinomDensity &lt;- binomDensity[order(p),]\n# We then manually calculate our rejection region by finding where the cumulative sum in the distribution\n# is less than or equal to our chosen alpha level. \nbinomDensity$reject &lt;- cumsum(binomDensity$p) &lt;= alpha\nhead(binomDensity)\n\n      k            p reject\n1     0 7.888609e-31   TRUE\n101 100 7.888609e-31   TRUE\n2     1 7.888609e-29   TRUE\n100  99 7.888609e-29   TRUE\n3     2 3.904861e-27   TRUE\n99   98 3.904861e-27   TRUE\n\n# Now we recreate the same plot as before, but adding red borders around the parts of our distribution\n# in the rejection region. \nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n  scale_colour_manual(\n    values = c(`TRUE` = \"red\", `FALSE` = \"darkgrey\")) +\n  geom_vline(xintercept = numHeads, col = \"blue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nWe sorted the \\(p\\)-values from lowest to highest (order), and added a column reject by computing the cumulative sum (cumsum) of the \\(p\\)-values and thresholding it against alpha.\nThe logical column reject therefore marks with TRUE a set of \\(k\\)s whose total probability is less than \\(\\alpha\\).\nThe rejection region is marked in red, containing both very large and very small values of \\(k\\), which can be considered unlikely under the null hypothesis.\nR provides not only functions for the densities (e.g., dbinom) but also for the cumulative distribution functions (pbinom). Those are more precise and faster than cumsum over the probabilities.\nThe (cumulative) distribution function is defined as the probability that a random variable \\(X\\) will take a value less than or equal to \\(x\\).\n\\[F(x) = P(X \\le x)\\]\nWe have just gone through the steps of a binomial test. This is a frequently used test and therefore available in R as a single function.\nWe have just gone through the steps of a binomial test. In fact, this is such a frequent activity in R that it has been wrapped into a single function, and we can compare its output to our results.\n\nbinom.test(x = numHeads, n = numFlips, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  numHeads and numFlips\nnumber of successes = 59, number of trials = 100, p-value = 0.08863\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4871442 0.6873800\nsample estimates:\nprobability of success \n                  0.59 \n\n\n\n\n\nWe can summarize what we just did with a series of steps:\n\nDecide on the effect that you are interested in, design a suitable experiment or study, pick a data summary function and test statistic.\nSet up a null hypothesis, which is a simple, computationally tractable model of reality that lets you compute the null distribution, i.e., the possible outcomes of the test statistic and their probabilities under the assumption that the null hypothesis is true.\nDecide on the rejection region, i.e., a subset of possible outcomes whose total probability is small.\nDo the experiment and collect the data; compute the test statistic.\nMake a decision: reject the null hypothesis if the test statistic is in the rejection region.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHaving set out the mechanics of testing, we can assess how well we are doing. The following table, called a confusion matrix, compares reality (whether or not the null hypothesis is in fact true) with our decision whether or not to reject the null hypothesis after we have seen the data.\n\n\n\n\n\n\n\n\nTest vs reality\nNull is true\nNull is false\n\n\n\n\nReject null\nType I error (false positive)\nTrue postitive\n\n\nDo not reject null\nTrue negative\nType II error (false negative)\n\n\n\nIt is always possible to reduce one of the two error types at the cost of increasing the other one. The real challenge is to find an acceptable trade-off between both of them. We can always decrease the false positive rate (FPR) by shifting the threshold to the right. We can become more “conservative”. But this happens at the price of higher false negative rate (FNR). Analogously, we can decrease the FNR by shifting the threshold to the left. But then again, this happens at the price of higher FPR. T he FPR is the same as the probability \\(\\alpha\\) that we mentioned above. \\(1-\\alpha\\) is also called the specificity of a test. The FNR is sometimes also called \\(\\beta\\), and \\(1-\\beta\\) the power, sensitivity or true positive rate of a test. The power of a test can be understood as the likelihood of it “catching” a true positive, or correctly rejecting the null hypothesis.\nGenerally, there are three factors that can affect statistical power:\n\nSample size: Larger samples provide greater statistical power\nEffect size: A given design will always have greater power to find a large effect than a small effect (because finding large effects is easier)\nType I error rate: There is a relationship between Type I error and power such that (all else being equal) decreasing Type I error will also decrease power.\n\n\n\n\n\n\n\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\n\n\n\n\nYou can use base R functions to conduct statistical tests. The commands are relatively simple and results will print to the R Console for simple viewing. However, the outputs are usually lists and so are harder to manipulate if you want to use the results in subsequent operations.\n\n\nA t-test, also called “Student’s t-Test”, is typically used to determine if there is a significant difference between the means of some numeric variable between two groups. Here we’ll show the syntax to do this test depending on whether the columns are in the same data frame.\nSyntax 1: This is the syntax when your numeric and categorical columns are in the same data frame. Provide the numeric column on the left side of the equation and the categorical column on the right side. Specify the dataset to data =. Optionally, set paired = TRUE, and conf.level = (0.95 default), and alternative = (either “two.sided”, “less”, or “greater”). Enter ?t.test for more details.\n\n## compare mean age by outcome group with a t-test\nt.test(age_years ~ gender, data = linelist)\n\n\n    Welch Two Sample t-test\n\ndata:  age_years by gender\nt = -21.344, df = 4902.3, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group f and group m is not equal to 0\n95 percent confidence interval:\n -7.571920 -6.297975\nsample estimates:\nmean in group f mean in group m \n       12.60207        19.53701 \n\n\nSyntax 2: You can compare two separate numeric vectors using this alternative syntax. For example, if the two columns are in different data sets.\n\nt.test(df1$age_years, df2$age_years)\n\nYou can also use a t-test to determine whether a sample mean is significantly different from some specific value. Here we conduct a one-sample t-test with the known/hypothesized population mean as mu =:\n\nt.test(linelist$age_years, mu = 45)\n\n\n\n\nThe Shapiro-Wilk test can be used to determine whether a sample came from a normally-distributed population (an assumption of many other tests and analysis, such as the t-test). However, this can only be used on a sample between 3 and 5000 observations. For larger samples a quantile-quantile plot may be helpful.\n\nshapiro.test(linelist$age_years)\n\n\n\n\nThe Wilcoxon rank sum test, also called the Mann–Whitney U test, is often used to help determine if two numeric samples are from the same distribution when their populations are not normally distributed or have unequal variance.\n\n## compare age distribution by outcome group with a wilcox test\nwilcox.test(age_years ~ outcome, data = linelist)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  age_years by outcome\nW = 2501868, p-value = 0.8308\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\nThe Kruskal-Wallis test is an extension of the Wilcoxon rank sum test that can be used to test for differences in the distribution of more than two samples. When only two samples are used it gives identical results to the Wilcoxon rank sum test.\n\n## compare age distribution by outcome group with a kruskal-wallis test\nkruskal.test(age_years ~ outcome, linelist)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  age_years by outcome\nKruskal-Wallis chi-squared = 0.045675, df = 1, p-value = 0.8308\n\n\n\n\n\nPearson’s Chi-squared test is used in testing for significant differences between categorical croups.\n\n## compare the proportions in each group with a chi-squared test\nchisq.test(linelist$gender, linelist$outcome)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  linelist$gender and linelist$outcome\nX-squared = 0.0011841, df = 1, p-value = 0.9725\n\n\n\n\n\n\nCorrelation between numeric variables can be investigated using the tidyverse\ncorrr package. It allows you to compute correlations using Pearson, Kendall tau or Spearman rho. The package creates a table and also has a function to automatically plot the values.\n\ncorrelation_tab &lt;- linelist %&gt;% \n  select(generation, age, ct_blood, days_onset_hosp, wt_kg, ht_cm) %&gt;%   # keep numeric variables of interest\n  correlate()      # create correlation table (using default pearson)\n\ncorrelation_tab    # print\n\n# A tibble: 6 × 7\n  term            generation      age ct_blood days_onset_hosp    wt_kg    ht_cm\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 generation        NA       -2.22e-2  0.179         -0.288    -0.0302  -0.00942\n2 age               -0.0222  NA        0.00849       -0.000635  0.833    0.877  \n3 ct_blood           0.179    8.49e-3 NA             -0.600    -0.00636  0.0181 \n4 days_onset_hosp   -0.288   -6.35e-4 -0.600         NA         0.0153  -0.00953\n5 wt_kg             -0.0302   8.33e-1 -0.00636        0.0153   NA        0.884  \n6 ht_cm             -0.00942  8.77e-1  0.0181        -0.00953   0.884   NA      \n\n## remove duplicate entries (the table above is mirrored) \ncorrelation_tab &lt;- correlation_tab %&gt;% \n  shave()\n\n## view correlation table \ncorrelation_tab\n\n# A tibble: 6 × 7\n  term            generation       age ct_blood days_onset_hosp  wt_kg ht_cm\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 generation        NA       NA        NA              NA       NA        NA\n2 age               -0.0222  NA        NA              NA       NA        NA\n3 ct_blood           0.179    0.00849  NA              NA       NA        NA\n4 days_onset_hosp   -0.288   -0.000635 -0.600          NA       NA        NA\n5 wt_kg             -0.0302   0.833    -0.00636         0.0153  NA        NA\n6 ht_cm             -0.00942  0.877     0.0181         -0.00953  0.884    NA\n\n## plot correlations \nrplot(correlation_tab)\n\n\n\n\n\n\n\n\nMuch of the information in this page is adapted from these resources and vignettes online:\n\ngtsummary\ndplyr\ncorrr\nsthda correlation -Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes.\nModern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "readings/simple-tests.html#an-example-statistical-test",
    "href": "readings/simple-tests.html#an-example-statistical-test",
    "title": "Simple statistical tests",
    "section": "",
    "text": "Let’s look a classic example in statistics: flipping a coin to see if it is fair. We flip the coin 100 times and each time record whether it came up heads or tails. So, we have a record that could look something like HHTTHTHTT...\nLet’s simulate the experiment in R, using a biased coin:\n\nset.seed(0xdada)\nnumFlips = 100\nprobHead = 0.6\n# Sample is a function in base R which let's us take a random sample from a vector, with or without replacement. \n# This line is sampling numFlips times from the vector ['H','T'] with replacement, with the probabilities for \n# each item in the vector being defined in the prob argument as [probHead, 1-probHead]\ncoinFlips = sample(c(\"H\", \"T\"), size = numFlips,\n  replace = TRUE, prob = c(probHead, 1 - probHead))\n# Thus, coinFlips is a character vector of a random sequence of 'T' and 'H'. \nhead(coinFlips)\n\n[1] \"T\" \"T\" \"H\" \"T\" \"H\" \"H\"\n\n\nNow, if the coin were fair, we would expect half of the time to get heads. Let’s see.\n\ntable(coinFlips)\n\ncoinFlips\n H  T \n59 41 \n\n\nThat is different from 50/50. However, does the data deviates strong enough to conclude that this coin isn’t fair? We know that the total number of heads seen in 100 coin tosses for a fair coin follows \\(B(100, 0.5)\\), making it a suitable test statistic.\nTo decide, let’s look at the sampling distribution of our test statistic – the total number of heads seen in 100 coin tosses – for a fair coin. As we learned, we can do this with the binomial distribution. Let’s plot a fair coin and mark our observation with a blue line:\n\n# This line sets k as the vector [0, 1, 2,...,numFlips]\nk &lt;- 0:numFlips\n# Recall that binary variables (TRUE and FALSE) are interpreted as 1 and 0, so we can use this operation\n# to count the number of heads in coinFlips. We practice these kinds of operations in session 3. \nnumHeads &lt;- sum(coinFlips == \"H\")\n# We use dbinom here to get the probability mass at every integer from 1-numFlips so that we can plot the distribution. \np &lt;- dbinom(k, size = numFlips, prob = 0.5)\n# We then convert it into a dataframe for easier plotting. \nbinomDensity &lt;- data.frame(k = k, p = p)\nhead(binomDensity)\n\n  k            p\n1 0 7.888609e-31\n2 1 7.888609e-29\n3 2 3.904861e-27\n4 3 1.275588e-25\n5 4 3.093301e-24\n6 5 5.939138e-23\n\n# Here, we are plotting the binomial distribution, with a vertical line representing\n# the number of heads we actually observed. We will learn how to create plots in session 4. \n# Thus, to complete our test we simply need to identify whether or not the blue line\n# is in our rejection region. \nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p), stat = \"identity\") +\n  geom_vline(xintercept = numHeads, col = \"blue\")\n\n\n\n\nHow do we quantify whether the observed value is among those values that we are likely to see from a fair coin, or whether its deviation from the expected value is already large enough for us to conclude with enough confidence that the coin is biased?\nWe divide the set of all possible \\(k(0-100)\\) in two complementary subsets, the rejection region and the region of no rejection. We want to make the rejection region as large as possible while keeping their total probability, assuming the null hypothesis, below some threshold \\(\\alpha\\)(say, 0.05).\n\nalpha &lt;- 0.05\n# We get the density of our plot in sorted order, meaning that we'll see binomDensity\n# jump back and forth between the distribution's tails as p increases. \nbinomDensity &lt;- binomDensity[order(p),]\n# We then manually calculate our rejection region by finding where the cumulative sum in the distribution\n# is less than or equal to our chosen alpha level. \nbinomDensity$reject &lt;- cumsum(binomDensity$p) &lt;= alpha\nhead(binomDensity)\n\n      k            p reject\n1     0 7.888609e-31   TRUE\n101 100 7.888609e-31   TRUE\n2     1 7.888609e-29   TRUE\n100  99 7.888609e-29   TRUE\n3     2 3.904861e-27   TRUE\n99   98 3.904861e-27   TRUE\n\n# Now we recreate the same plot as before, but adding red borders around the parts of our distribution\n# in the rejection region. \nggplot(binomDensity) +\n  geom_bar(aes(x = k, y = p, col = reject), stat = \"identity\") +\n  scale_colour_manual(\n    values = c(`TRUE` = \"red\", `FALSE` = \"darkgrey\")) +\n  geom_vline(xintercept = numHeads, col = \"blue\") +\n  theme(legend.position = \"none\")\n\n\n\n\nWe sorted the \\(p\\)-values from lowest to highest (order), and added a column reject by computing the cumulative sum (cumsum) of the \\(p\\)-values and thresholding it against alpha.\nThe logical column reject therefore marks with TRUE a set of \\(k\\)s whose total probability is less than \\(\\alpha\\).\nThe rejection region is marked in red, containing both very large and very small values of \\(k\\), which can be considered unlikely under the null hypothesis.\nR provides not only functions for the densities (e.g., dbinom) but also for the cumulative distribution functions (pbinom). Those are more precise and faster than cumsum over the probabilities.\nThe (cumulative) distribution function is defined as the probability that a random variable \\(X\\) will take a value less than or equal to \\(x\\).\n\\[F(x) = P(X \\le x)\\]\nWe have just gone through the steps of a binomial test. This is a frequently used test and therefore available in R as a single function.\nWe have just gone through the steps of a binomial test. In fact, this is such a frequent activity in R that it has been wrapped into a single function, and we can compare its output to our results.\n\nbinom.test(x = numHeads, n = numFlips, p = 0.5)\n\n\n    Exact binomial test\n\ndata:  numHeads and numFlips\nnumber of successes = 59, number of trials = 100, p-value = 0.08863\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.4871442 0.6873800\nsample estimates:\nprobability of success \n                  0.59"
  },
  {
    "objectID": "readings/simple-tests.html#hypothesis-tests",
    "href": "readings/simple-tests.html#hypothesis-tests",
    "title": "Simple statistical tests",
    "section": "",
    "text": "We can summarize what we just did with a series of steps:\n\nDecide on the effect that you are interested in, design a suitable experiment or study, pick a data summary function and test statistic.\nSet up a null hypothesis, which is a simple, computationally tractable model of reality that lets you compute the null distribution, i.e., the possible outcomes of the test statistic and their probabilities under the assumption that the null hypothesis is true.\nDecide on the rejection region, i.e., a subset of possible outcomes whose total probability is small.\nDo the experiment and collect the data; compute the test statistic.\nMake a decision: reject the null hypothesis if the test statistic is in the rejection region."
  },
  {
    "objectID": "readings/simple-tests.html#types-of-error",
    "href": "readings/simple-tests.html#types-of-error",
    "title": "Simple statistical tests",
    "section": "",
    "text": "Having set out the mechanics of testing, we can assess how well we are doing. The following table, called a confusion matrix, compares reality (whether or not the null hypothesis is in fact true) with our decision whether or not to reject the null hypothesis after we have seen the data.\n\n\n\n\n\n\n\n\nTest vs reality\nNull is true\nNull is false\n\n\n\n\nReject null\nType I error (false positive)\nTrue postitive\n\n\nDo not reject null\nTrue negative\nType II error (false negative)\n\n\n\nIt is always possible to reduce one of the two error types at the cost of increasing the other one. The real challenge is to find an acceptable trade-off between both of them. We can always decrease the false positive rate (FPR) by shifting the threshold to the right. We can become more “conservative”. But this happens at the price of higher false negative rate (FNR). Analogously, we can decrease the FNR by shifting the threshold to the left. But then again, this happens at the price of higher FPR. T he FPR is the same as the probability \\(\\alpha\\) that we mentioned above. \\(1-\\alpha\\) is also called the specificity of a test. The FNR is sometimes also called \\(\\beta\\), and \\(1-\\beta\\) the power, sensitivity or true positive rate of a test. The power of a test can be understood as the likelihood of it “catching” a true positive, or correctly rejecting the null hypothesis.\nGenerally, there are three factors that can affect statistical power:\n\nSample size: Larger samples provide greater statistical power\nEffect size: A given design will always have greater power to find a large effect than a small effect (because finding large effects is easier)\nType I error rate: There is a relationship between Type I error and power such that (all else being equal) decreasing Type I error will also decrease power."
  },
  {
    "objectID": "readings/simple-tests.html#preparation",
    "href": "readings/simple-tests.html#preparation",
    "title": "Simple statistical tests",
    "section": "",
    "text": "We import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")"
  },
  {
    "objectID": "readings/simple-tests.html#base-r",
    "href": "readings/simple-tests.html#base-r",
    "title": "Simple statistical tests",
    "section": "",
    "text": "You can use base R functions to conduct statistical tests. The commands are relatively simple and results will print to the R Console for simple viewing. However, the outputs are usually lists and so are harder to manipulate if you want to use the results in subsequent operations.\n\n\nA t-test, also called “Student’s t-Test”, is typically used to determine if there is a significant difference between the means of some numeric variable between two groups. Here we’ll show the syntax to do this test depending on whether the columns are in the same data frame.\nSyntax 1: This is the syntax when your numeric and categorical columns are in the same data frame. Provide the numeric column on the left side of the equation and the categorical column on the right side. Specify the dataset to data =. Optionally, set paired = TRUE, and conf.level = (0.95 default), and alternative = (either “two.sided”, “less”, or “greater”). Enter ?t.test for more details.\n\n## compare mean age by outcome group with a t-test\nt.test(age_years ~ gender, data = linelist)\n\n\n    Welch Two Sample t-test\n\ndata:  age_years by gender\nt = -21.344, df = 4902.3, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group f and group m is not equal to 0\n95 percent confidence interval:\n -7.571920 -6.297975\nsample estimates:\nmean in group f mean in group m \n       12.60207        19.53701 \n\n\nSyntax 2: You can compare two separate numeric vectors using this alternative syntax. For example, if the two columns are in different data sets.\n\nt.test(df1$age_years, df2$age_years)\n\nYou can also use a t-test to determine whether a sample mean is significantly different from some specific value. Here we conduct a one-sample t-test with the known/hypothesized population mean as mu =:\n\nt.test(linelist$age_years, mu = 45)\n\n\n\n\nThe Shapiro-Wilk test can be used to determine whether a sample came from a normally-distributed population (an assumption of many other tests and analysis, such as the t-test). However, this can only be used on a sample between 3 and 5000 observations. For larger samples a quantile-quantile plot may be helpful.\n\nshapiro.test(linelist$age_years)\n\n\n\n\nThe Wilcoxon rank sum test, also called the Mann–Whitney U test, is often used to help determine if two numeric samples are from the same distribution when their populations are not normally distributed or have unequal variance.\n\n## compare age distribution by outcome group with a wilcox test\nwilcox.test(age_years ~ outcome, data = linelist)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  age_years by outcome\nW = 2501868, p-value = 0.8308\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\nThe Kruskal-Wallis test is an extension of the Wilcoxon rank sum test that can be used to test for differences in the distribution of more than two samples. When only two samples are used it gives identical results to the Wilcoxon rank sum test.\n\n## compare age distribution by outcome group with a kruskal-wallis test\nkruskal.test(age_years ~ outcome, linelist)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  age_years by outcome\nKruskal-Wallis chi-squared = 0.045675, df = 1, p-value = 0.8308\n\n\n\n\n\nPearson’s Chi-squared test is used in testing for significant differences between categorical croups.\n\n## compare the proportions in each group with a chi-squared test\nchisq.test(linelist$gender, linelist$outcome)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  linelist$gender and linelist$outcome\nX-squared = 0.0011841, df = 1, p-value = 0.9725"
  },
  {
    "objectID": "readings/simple-tests.html#correlations",
    "href": "readings/simple-tests.html#correlations",
    "title": "Simple statistical tests",
    "section": "",
    "text": "Correlation between numeric variables can be investigated using the tidyverse\ncorrr package. It allows you to compute correlations using Pearson, Kendall tau or Spearman rho. The package creates a table and also has a function to automatically plot the values.\n\ncorrelation_tab &lt;- linelist %&gt;% \n  select(generation, age, ct_blood, days_onset_hosp, wt_kg, ht_cm) %&gt;%   # keep numeric variables of interest\n  correlate()      # create correlation table (using default pearson)\n\ncorrelation_tab    # print\n\n# A tibble: 6 × 7\n  term            generation      age ct_blood days_onset_hosp    wt_kg    ht_cm\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 generation        NA       -2.22e-2  0.179         -0.288    -0.0302  -0.00942\n2 age               -0.0222  NA        0.00849       -0.000635  0.833    0.877  \n3 ct_blood           0.179    8.49e-3 NA             -0.600    -0.00636  0.0181 \n4 days_onset_hosp   -0.288   -6.35e-4 -0.600         NA         0.0153  -0.00953\n5 wt_kg             -0.0302   8.33e-1 -0.00636        0.0153   NA        0.884  \n6 ht_cm             -0.00942  8.77e-1  0.0181        -0.00953   0.884   NA      \n\n## remove duplicate entries (the table above is mirrored) \ncorrelation_tab &lt;- correlation_tab %&gt;% \n  shave()\n\n## view correlation table \ncorrelation_tab\n\n# A tibble: 6 × 7\n  term            generation       age ct_blood days_onset_hosp  wt_kg ht_cm\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 generation        NA       NA        NA              NA       NA        NA\n2 age               -0.0222  NA        NA              NA       NA        NA\n3 ct_blood           0.179    0.00849  NA              NA       NA        NA\n4 days_onset_hosp   -0.288   -0.000635 -0.600          NA       NA        NA\n5 wt_kg             -0.0302   0.833    -0.00636         0.0153  NA        NA\n6 ht_cm             -0.00942  0.877     0.0181         -0.00953  0.884    NA\n\n## plot correlations \nrplot(correlation_tab)"
  },
  {
    "objectID": "readings/simple-tests.html#resources",
    "href": "readings/simple-tests.html#resources",
    "title": "Simple statistical tests",
    "section": "",
    "text": "Much of the information in this page is adapted from these resources and vignettes online:\n\ngtsummary\ndplyr\ncorrr\nsthda correlation -Statistical Thinking for the 21st Century by Russell A. Poldrack. This work is distributed under the terms of the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited and the material is used for noncommercial purposes.\nModern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber. This work is distributed under the terms of the Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited, the material is used for noncommercial purposes, and the same license is used for any derivative material."
  },
  {
    "objectID": "readings/regression.html",
    "href": "readings/regression.html",
    "title": "Univariate and multivariable regression",
    "section": "",
    "text": "::: callout-tip ## Extended Materials\nYou can find the original, extended version of this chapter here. ::\n\nThis page demonstrates the use of base R regression functions such as glm() and the gtsummary package to look at associations between variables (e.g. odds ratios, risk ratios and hazard ratios). It also uses functions like tidy() from the broom package to clean-up regression outputs.\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\npacman::p_load(\n  rio,          # File import\n  here,         # File locator\n  tidyverse,    # data management + ggplot2 graphics, \n  stringr,      # manipulate text strings \n  purrr,        # loop over objects in a tidy way\n  gtsummary,    # summary statistics and tests \n  broom,        # tidy up results from regressions\n  lmtest,       # likelihood-ratio tests\n  parameters,   # alternative to tidy up results from regressions\n  see          # alternative to visualise forest plots\n  )\n\n\n\n\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\n\n\n\n\n\nWe store the names of the explanatory columns as a character vector. This will be referenced later.\n\n## define variables of interest \nexplanatory_vars &lt;- c(\"gender\", \"fever\", \"chills\", \"cough\", \"aches\", \"vomit\")\n\n\n\n\nBelow we convert the explanatory columns from “yes”/“no”, “m”/“f”, and “dead”/“alive” to 1 / 0, to cooperate with the expectations of logistic regression models. To do this efficiently, used across() from dplyr to transform multiple columns at one time. The function we apply to each column is case_when() (also dplyr) which applies logic to convert specified values to 1’s and 0’s.\nNote: the “.” below represents the column that is being processed by across() at that moment.\n\n## convert dichotomous variables to 0/1 \nlinelist &lt;- linelist %&gt;%  \n  mutate(across(                                      \n    .cols = all_of(c(explanatory_vars, \"outcome\")),  ## for each column listed and \"outcome\"\n    .fns = ~case_when(                              \n      . %in% c(\"m\", \"yes\", \"Death\")   ~ 1,           ## recode male, yes and death to 1\n      . %in% c(\"f\", \"no\",  \"Recover\") ~ 0,           ## female, no and recover to 0\n      TRUE                            ~ NA_real_)    ## otherwise set to missing\n    )\n  )\n\n\n\n\nTo drop rows with missing values, can use the tidyr function drop_na(). However, we only want to do this for rows that are missing values in the columns of interest.\nThe first thing we must to is make sure our explanatory_vars vector includes the column age (age would have produced an error in the previous case_when() operation, which was only for dichotomous variables). Then we pipe the linelist to drop_na() to remove any rows with missing values in the outcome column or any of the explanatory_vars columns.\nBefore running the code, the number of rows in the linelist is nrow(linelist).\n\n## add in age_category to the explanatory vars \nexplanatory_vars &lt;- c(explanatory_vars, \"age_cat\")\n\n## drop rows with missing information for variables of interest \nlinelist &lt;- linelist %&gt;% \n  drop_na(any_of(c(\"outcome\", explanatory_vars)))\n\nThe number of rows remaining in linelist is nrow(linelist).\n\n\n\n\n\n\n\n\nYour use case will determine which R package you use. We present two options for doing univariate analysis:\n\nUse functions available in base R to quickly print results to the console. Use the broom package to tidy up the outputs.\n\nUse the gtsummary package to model and get publication-ready outputs\n\n\n\n\n\n\nThe base R function lm() perform linear regression, assessing the relationship between numeric response and explanatory variables that are assumed to have a linear relationship.\nProvide the equation as a formula, with the response and explanatory column names separated by a tilde ~. Also, specify the dataset to data =. Define the model results as an R object, to use later.\n\nlm_results &lt;- lm(ht_cm ~ age, data = linelist)\n\nYou can then run summary() on the model results to see the coefficients (Estimates), P-value, residuals, and other measures.\n\nsummary(lm_results)\n\n\nCall:\nlm(formula = ht_cm ~ age, data = linelist)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-128.579  -15.854    1.177   15.887  175.483 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  69.9051     0.5979   116.9   &lt;2e-16 ***\nage           3.4354     0.0293   117.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.75 on 4165 degrees of freedom\nMultiple R-squared:  0.7675,    Adjusted R-squared:  0.7674 \nF-statistic: 1.375e+04 on 1 and 4165 DF,  p-value: &lt; 2.2e-16\n\n\nAlternatively you can use the tidy() function from the broom package to pull the results in to a table. What the results tell us is that for each year increase in age the height increases by 3.5 cm and this is statistically significant.\n\ntidy(lm_results)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    69.9     0.598       117.       0\n2 age             3.44    0.0293      117.       0\n\n\nYou can then also use this regression to add it to a ggplot, to do this we first pull the points for the observed data and the fitted line in to one data frame using the augment() function from broom.\n\n## pull the regression points and observed data in to one dataset\npoints &lt;- augment(lm_results)\n\n## plot the data using age as the x-axis \nggplot(points, aes(x = age)) + \n  ## add points for height \n  geom_point(aes(y = ht_cm)) + \n  ## add your regression line \n  geom_line(aes(y = .fitted), colour = \"red\")\n\n\n\n\nIt is also possible to add a simple linear regression straight straight in ggplot using the geom_smooth() function.\n\n## add your data to a plot \n ggplot(linelist, aes(x = age, y = ht_cm)) + \n  ## show points\n  geom_point() + \n  ## add a linear regression \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSee the Resource section at the end of this chapter for more detailed tutorials.\n\n\n\nThe function glm() from the stats package (part of base R) is used to fit Generalized Linear Models (GLM).\nglm() can be used for univariate and multivariable logistic regression (e.g. to get Odds Ratios). Here are the core parts:\n\n# arguments for glm()\nglm(formula, family, data, weights, subset, ...)\n\n\nformula = The model is provided to glm() as an equation, with the outcome on the left and explanatory variables on the right of a tilde ~.\n\nfamily = This determines the type of model to run. For logistic regression, use family = \"binomial\", for poisson use family = \"poisson\". Other examples are in the table below.\n\ndata = Specify your data frame\n\nIf necessary, you can also specify the link function via the syntax family = familytype(link = \"linkfunction\")). You can read more in the documentation about other families and optional arguments such as weights = and subset = (?glm).\n\n\n\nFamily\nDefault link function\n\n\n\n\n\"binomial\"\n(link = \"logit\")\n\n\n\"gaussian\"\n(link = \"identity\")\n\n\n\"Gamma\"\n(link = \"inverse\")\n\n\n\"inverse.gaussian\"\n(link = \"1/mu^2\")\n\n\n\"poisson\"\n(link = \"log\")\n\n\n\"quasi\"\n(link = \"identity\", variance = \"constant\")\n\n\n\"quasibinomial\"\n(link = \"logit\")\n\n\n\"quasipoisson\"\n(link = \"log\")\n\n\n\nWhen running glm() it is most common to save the results as a named R object. Then you can print the results to your console using summary() as shown below, or perform other operations on the results (e.g. exponentiate).\n\n\n\nIn this example we are assessing the association between different age categories and the outcome of death (coded as 1 in the Preparation section). Below is a univariate model of outcome by age_cat. We save the model output as model and then print it with summary() to the console. Note the estimates provided are the log odds and that the baseline level is the first factor level of age_cat (“0-4”).\n\nmodel &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist)\nsummary(model)\n\n\nCall:\nglm(formula = outcome ~ age_cat, family = \"binomial\", data = linelist)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   0.233738   0.072805   3.210  0.00133 **\nage_cat5-9   -0.062898   0.101733  -0.618  0.53640   \nage_cat10-14  0.138204   0.107186   1.289  0.19726   \nage_cat15-19 -0.005565   0.113343  -0.049  0.96084   \nage_cat20-29  0.027511   0.102133   0.269  0.78765   \nage_cat30-49  0.063764   0.113771   0.560  0.57517   \nage_cat50-69 -0.387889   0.259240  -1.496  0.13459   \nage_cat70+   -0.639203   0.915770  -0.698  0.48518   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5705.1  on 4159  degrees of freedom\nAIC: 5721.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nTo alter the baseline level of a given variable, ensure the column is class Factor and move the desired level to the first position with fct_relevel(). For example, below we take column age_cat and set “20-29” as the baseline before piping the modified data frame into glm().\n\nlinelist %&gt;% \n  mutate(age_cat = fct_relevel(age_cat, \"20-29\", after = 0)) %&gt;% \n  glm(formula = outcome ~ age_cat, family = \"binomial\") %&gt;% \n  summary()\n\n\nCall:\nglm(formula = outcome ~ age_cat, family = \"binomial\", data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.26125    0.07163   3.647 0.000265 ***\nage_cat0-4   -0.02751    0.10213  -0.269 0.787652    \nage_cat5-9   -0.09041    0.10090  -0.896 0.370220    \nage_cat10-14  0.11069    0.10639   1.040 0.298133    \nage_cat15-19 -0.03308    0.11259  -0.294 0.768934    \nage_cat30-49  0.03625    0.11302   0.321 0.748390    \nage_cat50-69 -0.41540    0.25891  -1.604 0.108625    \nage_cat70+   -0.66671    0.91568  -0.728 0.466546    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5705.1  on 4159  degrees of freedom\nAIC: 5721.1\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nFor most uses, several modifications must be made to the above outputs. The function tidy() from the package broom is convenient for making the model results presentable.\nHere we demonstrate how to combine model outputs with a table of counts.\n\nGet the exponentiated log odds ratio estimates and confidence intervals by passing the model to tidy() and setting exponentiate = TRUE and conf.int = TRUE.\n\n\nmodel &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist) %&gt;% \n  tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;%        # exponentiate and produce CIs\n  mutate(across(where(is.numeric), round, digits = 2))  # round all numeric columns\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nBelow is the outputted tibble model:\n\n\n\n\n\n\n\n\nCombine these model results with a table of counts. Below, we create the a counts cross-table with the tabyl() function from janitor.\n\n\ncounts_table &lt;- linelist %&gt;% \n  janitor::tabyl(age_cat, outcome)\n\nHere is what this counts_table data frame looks like:\n\n\n\n\n\n\n\nNow we can bind the counts_table and the model results together horizontally with bind_cols() (dplyr). Remember that with bind_cols() the rows in the two data frames must be aligned perfectly. In this code, because we are binding within a pipe chain, we use . to represent the piped object counts_table as we bind it to model. To finish the process, we use select() to pick the desired columns and their order, and finally apply the base R round() function across all numeric columns to specify 2 decimal places.\n\ncombined &lt;- counts_table %&gt;%           # begin with table of counts\n  bind_cols(., model) %&gt;%              # combine with the outputs of the regression \n  select(term, 2:3, estimate,          # select and re-order cols\n         conf.low, conf.high, p.value) %&gt;% \n  mutate(across(where(is.numeric), round, digits = 2)) ## round to 2 decimal places\n\nHere is what the combined data frame looks like, printed nicely as an image with a function from flextable. The [Tables for presentation] explains how to customize such tables with flextable, or or you can use numerous other packages such as knitr or GT.\n\ncombined &lt;- combined %&gt;% \n  flextable::qflextable()\n\n\n\n\nBelow we present a method using glm() and tidy() for a more simple approach, see the section on gtsummary.\nTo run the models on several exposure variables to produce univariate odds ratios (i.e. not controlling for each other), you can use the approach below. It uses str_c() from stringr to create univariate formulas (see [Characters and strings]), runs the glm() regression on each formula, passes each glm() output to tidy() and finally collapses all the model outputs together with bind_rows() from tidyr. This approach uses map() from the package purrr to iterate.\n\nCreate a vector of column names of the explanatory variables. We already have this as explanatory_vars from the Preparation section of this page.\nUse str_c() to create multiple string formulas, with outcome on the left, and a column name from explanatory_vars on the right. The period . substitutes for the column name in explanatory_vars.\n\n\nexplanatory_vars %&gt;% str_c(\"outcome ~ \", .)\n\n[1] \"outcome ~ gender\"  \"outcome ~ fever\"   \"outcome ~ chills\" \n[4] \"outcome ~ cough\"   \"outcome ~ aches\"   \"outcome ~ vomit\"  \n[7] \"outcome ~ age_cat\"\n\n\n\nPass these string formulas to map() and set ~glm() as the function to apply to each input. Within glm(), set the regression formula as as.formula(.x) where .x will be replaced by the string formula defined in the step above. map() will loop over each of the string formulas, running regressions for each one.\nThe outputs of this first map() are passed to a second map() command, which applies tidy() to the regression outputs.\nFinally the output of the second map() (a list of tidied data frames) is condensed with bind_rows(), resulting in one data frame with all the univariate results.\n\n\nmodels &lt;- explanatory_vars %&gt;%       # begin with variables of interest\n  str_c(\"outcome ~ \", .) %&gt;%         # combine each variable into formula (\"outcome ~ variable of interest\")\n  \n  # iterate through each univariate formula\n  map(                               \n    .f = ~glm(                       # pass the formulas one-by-one to glm()\n      formula = as.formula(.x),      # within glm(), the string formula is .x\n      family = \"binomial\",           # specify type of glm (logistic)\n      data = linelist)) %&gt;%          # dataset\n  \n  # tidy up each of the glm regression outputs from above\n  map(\n    .f = ~tidy(\n      .x, \n      exponentiate = TRUE,           # exponentiate \n      conf.int = TRUE)) %&gt;%          # return confidence intervals\n  \n  # collapse the list of regression outputs in to one data frame\n  bind_rows() %&gt;% \n  \n  # round all numeric columns\n  mutate(across(where(is.numeric), round, digits = 2))\n\nThis time, the end object models is longer because it now represents the combined results of several univariate regressions. Click through to see all the rows of model.\n\n\n\n\n\n\n\nAs before, we can create a counts table from the linelist for each explanatory variable, bind it to models, and make a nice table. We begin with the variables, and iterate through them with map(). We iterate through a user-defined function which involves creating a counts table with dplyr functions. Then the results are combined and bound with the models model results.\n\n## for each explanatory variable\nuniv_tab_base &lt;- explanatory_vars %&gt;% \n  map(.f = \n    ~{linelist %&gt;%                ## begin with linelist\n        group_by(outcome) %&gt;%     ## group data set by outcome\n        count(.data[[.x]]) %&gt;%    ## produce counts for variable of interest\n        pivot_wider(              ## spread to wide format (as in cross-tabulation)\n          names_from = outcome,\n          values_from = n) %&gt;% \n        drop_na(.data[[.x]]) %&gt;%         ## drop rows with missings\n        rename(\"variable\" = .x) %&gt;%      ## change variable of interest column to \"variable\"\n        mutate(variable = as.character(variable))} ## convert to character, else non-dichotomous (categorical) variables come out as factor and cant be merged\n      ) %&gt;% \n  \n  ## collapse the list of count outputs in to one data frame\n  bind_rows() %&gt;% \n  \n  ## merge with the outputs of the regression \n  bind_cols(., models) %&gt;% \n  \n  ## only keep columns interested in \n  select(term, 2:3, estimate, conf.low, conf.high, p.value) %&gt;% \n  \n  ## round decimal places\n  mutate(across(where(is.numeric), round, digits = 2))\n\nBelow is what the data frame looks like.\n\n\n\n\n\n\n\n\n\n\n\n\nBelow we present the use of tbl_uvregression() from the gtsummary package. gtsummary functions do a good job of running statistics and producing professional-looking outputs. This function produces a table of univariate regression results.\nWe select only the necessary columns from the linelist (explanatory variables and the outcome variable) and pipe them into tbl_uvregression(). We are going to run univariate regression on each of the columns we defined as explanatory_vars in the data Preparation section (gender, fever, chills, cough, aches, vomit, and age_cat).\nWithin the function itself, we provide the method = as glm (no quotes), the y = outcome column (outcome), specify to method.args = that we want to run logistic regression via family = binomial, and we tell it to exponentiate the results.\nThe output is HTML and contains the counts\n\nuniv_tab &lt;- linelist %&gt;% \n  dplyr::select(explanatory_vars, outcome) %&gt;% ## select variables of interest\n\n  tbl_uvregression(                         ## produce univariate table\n    method = glm,                           ## define regression want to run (generalised linear model)\n    y = outcome,                            ## define outcome variable\n    method.args = list(family = binomial),  ## define what type of glm want to run (logistic)\n    exponentiate = TRUE                     ## exponentiate to produce odds ratios (rather than log odds)\n  )\n\n## view univariate results table \nuniv_tab\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    gender\n4,167\n1.00\n0.88, 1.13\n&gt;0.9\n    fever\n4,167\n1.00\n0.85, 1.17\n&gt;0.9\n    chills\n4,167\n1.03\n0.89, 1.21\n0.7\n    cough\n4,167\n1.15\n0.97, 1.37\n0.11\n    aches\n4,167\n0.93\n0.76, 1.14\n0.5\n    vomit\n4,167\n1.09\n0.96, 1.23\n0.2\n    age_cat\n4,167\n\n\n\n        0-4\n\n—\n—\n\n        5-9\n\n0.94\n0.77, 1.15\n0.5\n        10-14\n\n1.15\n0.93, 1.42\n0.2\n        15-19\n\n0.99\n0.80, 1.24\n&gt;0.9\n        20-29\n\n1.03\n0.84, 1.26\n0.8\n        30-49\n\n1.07\n0.85, 1.33\n0.6\n        50-69\n\n0.68\n0.41, 1.13\n0.13\n        70+\n\n0.53\n0.07, 3.20\n0.5\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThere are many modifications you can make to this table output, such as adjusting the text labels, bolding rows by their p-value, etc. See tutorials here and elsewhere online.\n\n\n\n\n\nFor multivariable analysis, we again present two approaches:\n\nglm() and tidy()\n\ngtsummary package\n\nThe workflow is similar for each and only the last step of pulling together a final table is different.\n\n\nHere we use glm() but add more variables to the right side of the equation, separated by plus symbols (+).\nTo run the model with all of our explanatory variables we would run:\n\nmv_reg &lt;- glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = \"binomial\", data = linelist)\n\nsummary(mv_reg)\n\n\nCall:\nglm(formula = outcome ~ gender + fever + chills + cough + aches + \n    vomit + age_cat, family = \"binomial\", data = linelist)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   0.069054   0.131726   0.524    0.600\ngender        0.002448   0.065133   0.038    0.970\nfever         0.004309   0.080522   0.054    0.957\nchills        0.034112   0.078924   0.432    0.666\ncough         0.138584   0.089909   1.541    0.123\naches        -0.070705   0.104078  -0.679    0.497\nvomit         0.086098   0.062618   1.375    0.169\nage_cat5-9   -0.063562   0.101851  -0.624    0.533\nage_cat10-14  0.136372   0.107275   1.271    0.204\nage_cat15-19 -0.011074   0.113640  -0.097    0.922\nage_cat20-29  0.026552   0.102780   0.258    0.796\nage_cat30-49  0.059569   0.116402   0.512    0.609\nage_cat50-69 -0.388964   0.262384  -1.482    0.138\nage_cat70+   -0.647443   0.917375  -0.706    0.480\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5700.2  on 4153  degrees of freedom\nAIC: 5728.2\n\nNumber of Fisher Scoring iterations: 4\n\n\nIf you want to include two variables and an interaction between them you can separate them with an asterisk * instead of a +. Separate them with a colon : if you are only specifying the interaction. For example:\n\nglm(outcome ~ gender + age_cat * fever, family = \"binomial\", data = linelist)\n\nOptionally, you can use this code to leverage the pre-defined vector of column names and re-create the above command using str_c(). This might be useful if your explanatory variable names are changing, or you don’t want to type them all out again.\n\n## run a regression with all variables of interest \nmv_reg &lt;- explanatory_vars %&gt;%  ## begin with vector of explanatory column names\n  str_c(collapse = \"+\") %&gt;%     ## combine all names of the variables of interest separated by a plus\n  str_c(\"outcome ~ \", .) %&gt;%    ## combine the names of variables of interest with outcome in formula style\n  glm(family = \"binomial\",      ## define type of glm as logistic,\n      data = linelist)          ## define your dataset\n\n\n\nYou can build your model step-by-step, saving various models that include certain explanatory variables. You can compare these models with likelihood-ratio tests using lrtest() from the package lmtest, as below:\nNOTE: Using base anova(model1, model2, test = \"Chisq) produces the same results \n\nmodel1 &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist)\nmodel2 &lt;- glm(outcome ~ age_cat + gender, family = \"binomial\", data = linelist)\n\nlmtest::lrtest(model1, model2)\n\nLikelihood ratio test\n\nModel 1: outcome ~ age_cat\nModel 2: outcome ~ age_cat + gender\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)\n1   8 -2852.6                    \n2   9 -2852.6  1 2e-04     0.9883\n\n\nAnother option is to take the model object and apply the step() function from the stats package. Specify which variable selection direction you want use when building the model.\n\n## choose a model using forward selection based on AIC\n## you can also do \"backward\" or \"both\" by adjusting the direction\nfinal_mv_reg &lt;- mv_reg %&gt;%\n  step(direction = \"forward\", trace = FALSE)\n\nYou can also turn off scientific notation in your R session, for clarity:\n\noptions(scipen=999)\n\nAs described in the section on univariate analysis, pass the model output to tidy() to exponentiate the log odds and CIs. Finally we round all numeric columns to two decimal places. Scroll through to see all the rows.\n\nmv_tab_base &lt;- final_mv_reg %&gt;% \n  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;%  ## get a tidy dataframe of estimates \n  mutate(across(where(is.numeric), round, digits = 2))          ## round \n\nHere is what the resulting data frame looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe gtsummary package provides the tbl_regression() function, which will take the outputs from a regression (glm() in this case) and produce an nice summary table.\n\n## show results table of final regression \nmv_tab &lt;- tbl_regression(final_mv_reg, exponentiate = TRUE)\n\nLet’s see the table:\n\nmv_tab\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    gender\n1.00\n0.88, 1.14\n&gt;0.9\n    fever\n1.00\n0.86, 1.18\n&gt;0.9\n    chills\n1.03\n0.89, 1.21\n0.7\n    cough\n1.15\n0.96, 1.37\n0.12\n    aches\n0.93\n0.76, 1.14\n0.5\n    vomit\n1.09\n0.96, 1.23\n0.2\n    age_cat\n\n\n\n        0-4\n—\n—\n\n        5-9\n0.94\n0.77, 1.15\n0.5\n        10-14\n1.15\n0.93, 1.41\n0.2\n        15-19\n0.99\n0.79, 1.24\n&gt;0.9\n        20-29\n1.03\n0.84, 1.26\n0.8\n        30-49\n1.06\n0.85, 1.33\n0.6\n        50-69\n0.68\n0.40, 1.13\n0.14\n        70+\n0.52\n0.07, 3.19\n0.5\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nYou can also combine several different output tables produced by gtsummary with the tbl_merge() function. We now combine the multivariable results with the gtsummary univariate results that we created above:\n\n## combine with univariate results \ntbl_merge(\n  tbls = list(univ_tab, mv_tab),                          # combine\n  tab_spanner = c(\"**Univariate**\", \"**Multivariable**\")) # set header names\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Univariate\n      \n      \n        Multivariable\n      \n    \n    \n      N\n      OR1\n      95% CI1\n      p-value\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    gender\n4,167\n1.00\n0.88, 1.13\n&gt;0.9\n1.00\n0.88, 1.14\n&gt;0.9\n    fever\n4,167\n1.00\n0.85, 1.17\n&gt;0.9\n1.00\n0.86, 1.18\n&gt;0.9\n    chills\n4,167\n1.03\n0.89, 1.21\n0.7\n1.03\n0.89, 1.21\n0.7\n    cough\n4,167\n1.15\n0.97, 1.37\n0.11\n1.15\n0.96, 1.37\n0.12\n    aches\n4,167\n0.93\n0.76, 1.14\n0.5\n0.93\n0.76, 1.14\n0.5\n    vomit\n4,167\n1.09\n0.96, 1.23\n0.2\n1.09\n0.96, 1.23\n0.2\n    age_cat\n4,167\n\n\n\n\n\n\n        0-4\n\n—\n—\n\n—\n—\n\n        5-9\n\n0.94\n0.77, 1.15\n0.5\n0.94\n0.77, 1.15\n0.5\n        10-14\n\n1.15\n0.93, 1.42\n0.2\n1.15\n0.93, 1.41\n0.2\n        15-19\n\n0.99\n0.80, 1.24\n&gt;0.9\n0.99\n0.79, 1.24\n&gt;0.9\n        20-29\n\n1.03\n0.84, 1.26\n0.8\n1.03\n0.84, 1.26\n0.8\n        30-49\n\n1.07\n0.85, 1.33\n0.6\n1.06\n0.85, 1.33\n0.6\n        50-69\n\n0.68\n0.41, 1.13\n0.13\n0.68\n0.40, 1.13\n0.14\n        70+\n\n0.53\n0.07, 3.20\n0.5\n0.52\n0.07, 3.19\n0.5\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\nAn alternative way of combining the glm()/tidy() univariate and multivariable outputs is with the dplyr join functions.\n\nJoin the univariate results from earlier (univ_tab_base, which contains counts) with the tidied multivariable results mv_tab_base\n\nUse select() to keep only the columns we want, specify their order, and re-name them\n\nUse round() with two decimal places on all the column that are class Double\n\n\n## combine univariate and multivariable tables \nleft_join(univ_tab_base, mv_tab_base, by = \"term\") %&gt;% \n  ## choose columns and rename them\n  select( # new name =  old name\n    \"characteristic\" = term, \n    \"recovered\"      = \"0\", \n    \"dead\"           = \"1\", \n    \"univ_or\"        = estimate.x, \n    \"univ_ci_low\"    = conf.low.x, \n    \"univ_ci_high\"   = conf.high.x,\n    \"univ_pval\"      = p.value.x, \n    \"mv_or\"          = estimate.y, \n    \"mvv_ci_low\"     = conf.low.y, \n    \"mv_ci_high\"     = conf.high.y,\n    \"mv_pval\"        = p.value.y \n  ) %&gt;% \n  mutate(across(where(is.double), round, 2))   \n\n# A tibble: 20 × 11\n   characteristic recovered  dead univ_or univ_ci_low univ_ci_high univ_pval\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)          909  1168    1.28        1.18         1.4       0   \n 2 gender               916  1174    1           0.88         1.13      0.97\n 3 (Intercept)          340   436    1.28        1.11         1.48      0   \n 4 fever               1485  1906    1           0.85         1.17      0.99\n 5 (Intercept)         1472  1877    1.28        1.19         1.37      0   \n 6 chills               353   465    1.03        0.89         1.21      0.68\n 7 (Intercept)          272   309    1.14        0.97         1.34      0.13\n 8 cough               1553  2033    1.15        0.97         1.37      0.11\n 9 (Intercept)         1636  2114    1.29        1.21         1.38      0   \n10 aches                189   228    0.93        0.76         1.14      0.51\n11 (Intercept)          931  1144    1.23        1.13         1.34      0   \n12 vomit                894  1198    1.09        0.96         1.23      0.17\n13 (Intercept)          338   427    1.26        1.1          1.46      0   \n14 age_cat5-9           365   433    0.94        0.77         1.15      0.54\n15 age_cat10-14         273   396    1.15        0.93         1.42      0.2 \n16 age_cat15-19         238   299    0.99        0.8          1.24      0.96\n17 age_cat20-29         345   448    1.03        0.84         1.26      0.79\n18 age_cat30-49         228   307    1.07        0.85         1.33      0.58\n19 age_cat50-69          35    30    0.68        0.41         1.13      0.13\n20 age_cat70+             3     2    0.53        0.07         3.2       0.49\n# ℹ 4 more variables: mv_or &lt;dbl&gt;, mvv_ci_low &lt;dbl&gt;, mv_ci_high &lt;dbl&gt;,\n#   mv_pval &lt;dbl&gt;"
  },
  {
    "objectID": "readings/regression.html#univariate",
    "href": "readings/regression.html#univariate",
    "title": "Univariate and multivariable regression",
    "section": "",
    "text": "Your use case will determine which R package you use. We present two options for doing univariate analysis:\n\nUse functions available in base R to quickly print results to the console. Use the broom package to tidy up the outputs.\n\nUse the gtsummary package to model and get publication-ready outputs\n\n\n\n\n\n\nThe base R function lm() perform linear regression, assessing the relationship between numeric response and explanatory variables that are assumed to have a linear relationship.\nProvide the equation as a formula, with the response and explanatory column names separated by a tilde ~. Also, specify the dataset to data =. Define the model results as an R object, to use later.\n\nlm_results &lt;- lm(ht_cm ~ age, data = linelist)\n\nYou can then run summary() on the model results to see the coefficients (Estimates), P-value, residuals, and other measures.\n\nsummary(lm_results)\n\n\nCall:\nlm(formula = ht_cm ~ age, data = linelist)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-128.579  -15.854    1.177   15.887  175.483 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  69.9051     0.5979   116.9   &lt;2e-16 ***\nage           3.4354     0.0293   117.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.75 on 4165 degrees of freedom\nMultiple R-squared:  0.7675,    Adjusted R-squared:  0.7674 \nF-statistic: 1.375e+04 on 1 and 4165 DF,  p-value: &lt; 2.2e-16\n\n\nAlternatively you can use the tidy() function from the broom package to pull the results in to a table. What the results tell us is that for each year increase in age the height increases by 3.5 cm and this is statistically significant.\n\ntidy(lm_results)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    69.9     0.598       117.       0\n2 age             3.44    0.0293      117.       0\n\n\nYou can then also use this regression to add it to a ggplot, to do this we first pull the points for the observed data and the fitted line in to one data frame using the augment() function from broom.\n\n## pull the regression points and observed data in to one dataset\npoints &lt;- augment(lm_results)\n\n## plot the data using age as the x-axis \nggplot(points, aes(x = age)) + \n  ## add points for height \n  geom_point(aes(y = ht_cm)) + \n  ## add your regression line \n  geom_line(aes(y = .fitted), colour = \"red\")\n\n\n\n\nIt is also possible to add a simple linear regression straight straight in ggplot using the geom_smooth() function.\n\n## add your data to a plot \n ggplot(linelist, aes(x = age, y = ht_cm)) + \n  ## show points\n  geom_point() + \n  ## add a linear regression \n  geom_smooth(method = \"lm\", se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSee the Resource section at the end of this chapter for more detailed tutorials.\n\n\n\nThe function glm() from the stats package (part of base R) is used to fit Generalized Linear Models (GLM).\nglm() can be used for univariate and multivariable logistic regression (e.g. to get Odds Ratios). Here are the core parts:\n\n# arguments for glm()\nglm(formula, family, data, weights, subset, ...)\n\n\nformula = The model is provided to glm() as an equation, with the outcome on the left and explanatory variables on the right of a tilde ~.\n\nfamily = This determines the type of model to run. For logistic regression, use family = \"binomial\", for poisson use family = \"poisson\". Other examples are in the table below.\n\ndata = Specify your data frame\n\nIf necessary, you can also specify the link function via the syntax family = familytype(link = \"linkfunction\")). You can read more in the documentation about other families and optional arguments such as weights = and subset = (?glm).\n\n\n\nFamily\nDefault link function\n\n\n\n\n\"binomial\"\n(link = \"logit\")\n\n\n\"gaussian\"\n(link = \"identity\")\n\n\n\"Gamma\"\n(link = \"inverse\")\n\n\n\"inverse.gaussian\"\n(link = \"1/mu^2\")\n\n\n\"poisson\"\n(link = \"log\")\n\n\n\"quasi\"\n(link = \"identity\", variance = \"constant\")\n\n\n\"quasibinomial\"\n(link = \"logit\")\n\n\n\"quasipoisson\"\n(link = \"log\")\n\n\n\nWhen running glm() it is most common to save the results as a named R object. Then you can print the results to your console using summary() as shown below, or perform other operations on the results (e.g. exponentiate).\n\n\n\nIn this example we are assessing the association between different age categories and the outcome of death (coded as 1 in the Preparation section). Below is a univariate model of outcome by age_cat. We save the model output as model and then print it with summary() to the console. Note the estimates provided are the log odds and that the baseline level is the first factor level of age_cat (“0-4”).\n\nmodel &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist)\nsummary(model)\n\n\nCall:\nglm(formula = outcome ~ age_cat, family = \"binomial\", data = linelist)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   0.233738   0.072805   3.210  0.00133 **\nage_cat5-9   -0.062898   0.101733  -0.618  0.53640   \nage_cat10-14  0.138204   0.107186   1.289  0.19726   \nage_cat15-19 -0.005565   0.113343  -0.049  0.96084   \nage_cat20-29  0.027511   0.102133   0.269  0.78765   \nage_cat30-49  0.063764   0.113771   0.560  0.57517   \nage_cat50-69 -0.387889   0.259240  -1.496  0.13459   \nage_cat70+   -0.639203   0.915770  -0.698  0.48518   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5705.1  on 4159  degrees of freedom\nAIC: 5721.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nTo alter the baseline level of a given variable, ensure the column is class Factor and move the desired level to the first position with fct_relevel(). For example, below we take column age_cat and set “20-29” as the baseline before piping the modified data frame into glm().\n\nlinelist %&gt;% \n  mutate(age_cat = fct_relevel(age_cat, \"20-29\", after = 0)) %&gt;% \n  glm(formula = outcome ~ age_cat, family = \"binomial\") %&gt;% \n  summary()\n\n\nCall:\nglm(formula = outcome ~ age_cat, family = \"binomial\", data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.26125    0.07163   3.647 0.000265 ***\nage_cat0-4   -0.02751    0.10213  -0.269 0.787652    \nage_cat5-9   -0.09041    0.10090  -0.896 0.370220    \nage_cat10-14  0.11069    0.10639   1.040 0.298133    \nage_cat15-19 -0.03308    0.11259  -0.294 0.768934    \nage_cat30-49  0.03625    0.11302   0.321 0.748390    \nage_cat50-69 -0.41540    0.25891  -1.604 0.108625    \nage_cat70+   -0.66671    0.91568  -0.728 0.466546    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5705.1  on 4159  degrees of freedom\nAIC: 5721.1\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\nFor most uses, several modifications must be made to the above outputs. The function tidy() from the package broom is convenient for making the model results presentable.\nHere we demonstrate how to combine model outputs with a table of counts.\n\nGet the exponentiated log odds ratio estimates and confidence intervals by passing the model to tidy() and setting exponentiate = TRUE and conf.int = TRUE.\n\n\nmodel &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist) %&gt;% \n  tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;%        # exponentiate and produce CIs\n  mutate(across(where(is.numeric), round, digits = 2))  # round all numeric columns\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, digits = 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\nBelow is the outputted tibble model:\n\n\n\n\n\n\n\n\nCombine these model results with a table of counts. Below, we create the a counts cross-table with the tabyl() function from janitor.\n\n\ncounts_table &lt;- linelist %&gt;% \n  janitor::tabyl(age_cat, outcome)\n\nHere is what this counts_table data frame looks like:\n\n\n\n\n\n\n\nNow we can bind the counts_table and the model results together horizontally with bind_cols() (dplyr). Remember that with bind_cols() the rows in the two data frames must be aligned perfectly. In this code, because we are binding within a pipe chain, we use . to represent the piped object counts_table as we bind it to model. To finish the process, we use select() to pick the desired columns and their order, and finally apply the base R round() function across all numeric columns to specify 2 decimal places.\n\ncombined &lt;- counts_table %&gt;%           # begin with table of counts\n  bind_cols(., model) %&gt;%              # combine with the outputs of the regression \n  select(term, 2:3, estimate,          # select and re-order cols\n         conf.low, conf.high, p.value) %&gt;% \n  mutate(across(where(is.numeric), round, digits = 2)) ## round to 2 decimal places\n\nHere is what the combined data frame looks like, printed nicely as an image with a function from flextable. The [Tables for presentation] explains how to customize such tables with flextable, or or you can use numerous other packages such as knitr or GT.\n\ncombined &lt;- combined %&gt;% \n  flextable::qflextable()\n\n\n\n\nBelow we present a method using glm() and tidy() for a more simple approach, see the section on gtsummary.\nTo run the models on several exposure variables to produce univariate odds ratios (i.e. not controlling for each other), you can use the approach below. It uses str_c() from stringr to create univariate formulas (see [Characters and strings]), runs the glm() regression on each formula, passes each glm() output to tidy() and finally collapses all the model outputs together with bind_rows() from tidyr. This approach uses map() from the package purrr to iterate.\n\nCreate a vector of column names of the explanatory variables. We already have this as explanatory_vars from the Preparation section of this page.\nUse str_c() to create multiple string formulas, with outcome on the left, and a column name from explanatory_vars on the right. The period . substitutes for the column name in explanatory_vars.\n\n\nexplanatory_vars %&gt;% str_c(\"outcome ~ \", .)\n\n[1] \"outcome ~ gender\"  \"outcome ~ fever\"   \"outcome ~ chills\" \n[4] \"outcome ~ cough\"   \"outcome ~ aches\"   \"outcome ~ vomit\"  \n[7] \"outcome ~ age_cat\"\n\n\n\nPass these string formulas to map() and set ~glm() as the function to apply to each input. Within glm(), set the regression formula as as.formula(.x) where .x will be replaced by the string formula defined in the step above. map() will loop over each of the string formulas, running regressions for each one.\nThe outputs of this first map() are passed to a second map() command, which applies tidy() to the regression outputs.\nFinally the output of the second map() (a list of tidied data frames) is condensed with bind_rows(), resulting in one data frame with all the univariate results.\n\n\nmodels &lt;- explanatory_vars %&gt;%       # begin with variables of interest\n  str_c(\"outcome ~ \", .) %&gt;%         # combine each variable into formula (\"outcome ~ variable of interest\")\n  \n  # iterate through each univariate formula\n  map(                               \n    .f = ~glm(                       # pass the formulas one-by-one to glm()\n      formula = as.formula(.x),      # within glm(), the string formula is .x\n      family = \"binomial\",           # specify type of glm (logistic)\n      data = linelist)) %&gt;%          # dataset\n  \n  # tidy up each of the glm regression outputs from above\n  map(\n    .f = ~tidy(\n      .x, \n      exponentiate = TRUE,           # exponentiate \n      conf.int = TRUE)) %&gt;%          # return confidence intervals\n  \n  # collapse the list of regression outputs in to one data frame\n  bind_rows() %&gt;% \n  \n  # round all numeric columns\n  mutate(across(where(is.numeric), round, digits = 2))\n\nThis time, the end object models is longer because it now represents the combined results of several univariate regressions. Click through to see all the rows of model.\n\n\n\n\n\n\n\nAs before, we can create a counts table from the linelist for each explanatory variable, bind it to models, and make a nice table. We begin with the variables, and iterate through them with map(). We iterate through a user-defined function which involves creating a counts table with dplyr functions. Then the results are combined and bound with the models model results.\n\n## for each explanatory variable\nuniv_tab_base &lt;- explanatory_vars %&gt;% \n  map(.f = \n    ~{linelist %&gt;%                ## begin with linelist\n        group_by(outcome) %&gt;%     ## group data set by outcome\n        count(.data[[.x]]) %&gt;%    ## produce counts for variable of interest\n        pivot_wider(              ## spread to wide format (as in cross-tabulation)\n          names_from = outcome,\n          values_from = n) %&gt;% \n        drop_na(.data[[.x]]) %&gt;%         ## drop rows with missings\n        rename(\"variable\" = .x) %&gt;%      ## change variable of interest column to \"variable\"\n        mutate(variable = as.character(variable))} ## convert to character, else non-dichotomous (categorical) variables come out as factor and cant be merged\n      ) %&gt;% \n  \n  ## collapse the list of count outputs in to one data frame\n  bind_rows() %&gt;% \n  \n  ## merge with the outputs of the regression \n  bind_cols(., models) %&gt;% \n  \n  ## only keep columns interested in \n  select(term, 2:3, estimate, conf.low, conf.high, p.value) %&gt;% \n  \n  ## round decimal places\n  mutate(across(where(is.numeric), round, digits = 2))\n\nBelow is what the data frame looks like.\n\n\n\n\n\n\n\n\n\n\n\n\nBelow we present the use of tbl_uvregression() from the gtsummary package. gtsummary functions do a good job of running statistics and producing professional-looking outputs. This function produces a table of univariate regression results.\nWe select only the necessary columns from the linelist (explanatory variables and the outcome variable) and pipe them into tbl_uvregression(). We are going to run univariate regression on each of the columns we defined as explanatory_vars in the data Preparation section (gender, fever, chills, cough, aches, vomit, and age_cat).\nWithin the function itself, we provide the method = as glm (no quotes), the y = outcome column (outcome), specify to method.args = that we want to run logistic regression via family = binomial, and we tell it to exponentiate the results.\nThe output is HTML and contains the counts\n\nuniv_tab &lt;- linelist %&gt;% \n  dplyr::select(explanatory_vars, outcome) %&gt;% ## select variables of interest\n\n  tbl_uvregression(                         ## produce univariate table\n    method = glm,                           ## define regression want to run (generalised linear model)\n    y = outcome,                            ## define outcome variable\n    method.args = list(family = binomial),  ## define what type of glm want to run (logistic)\n    exponentiate = TRUE                     ## exponentiate to produce odds ratios (rather than log odds)\n  )\n\n## view univariate results table \nuniv_tab\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    gender\n4,167\n1.00\n0.88, 1.13\n&gt;0.9\n    fever\n4,167\n1.00\n0.85, 1.17\n&gt;0.9\n    chills\n4,167\n1.03\n0.89, 1.21\n0.7\n    cough\n4,167\n1.15\n0.97, 1.37\n0.11\n    aches\n4,167\n0.93\n0.76, 1.14\n0.5\n    vomit\n4,167\n1.09\n0.96, 1.23\n0.2\n    age_cat\n4,167\n\n\n\n        0-4\n\n—\n—\n\n        5-9\n\n0.94\n0.77, 1.15\n0.5\n        10-14\n\n1.15\n0.93, 1.42\n0.2\n        15-19\n\n0.99\n0.80, 1.24\n&gt;0.9\n        20-29\n\n1.03\n0.84, 1.26\n0.8\n        30-49\n\n1.07\n0.85, 1.33\n0.6\n        50-69\n\n0.68\n0.41, 1.13\n0.13\n        70+\n\n0.53\n0.07, 3.20\n0.5\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nThere are many modifications you can make to this table output, such as adjusting the text labels, bolding rows by their p-value, etc. See tutorials here and elsewhere online."
  },
  {
    "objectID": "readings/regression.html#multivariable",
    "href": "readings/regression.html#multivariable",
    "title": "Univariate and multivariable regression",
    "section": "",
    "text": "For multivariable analysis, we again present two approaches:\n\nglm() and tidy()\n\ngtsummary package\n\nThe workflow is similar for each and only the last step of pulling together a final table is different.\n\n\nHere we use glm() but add more variables to the right side of the equation, separated by plus symbols (+).\nTo run the model with all of our explanatory variables we would run:\n\nmv_reg &lt;- glm(outcome ~ gender + fever + chills + cough + aches + vomit + age_cat, family = \"binomial\", data = linelist)\n\nsummary(mv_reg)\n\n\nCall:\nglm(formula = outcome ~ gender + fever + chills + cough + aches + \n    vomit + age_cat, family = \"binomial\", data = linelist)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   0.069054   0.131726   0.524    0.600\ngender        0.002448   0.065133   0.038    0.970\nfever         0.004309   0.080522   0.054    0.957\nchills        0.034112   0.078924   0.432    0.666\ncough         0.138584   0.089909   1.541    0.123\naches        -0.070705   0.104078  -0.679    0.497\nvomit         0.086098   0.062618   1.375    0.169\nage_cat5-9   -0.063562   0.101851  -0.624    0.533\nage_cat10-14  0.136372   0.107275   1.271    0.204\nage_cat15-19 -0.011074   0.113640  -0.097    0.922\nage_cat20-29  0.026552   0.102780   0.258    0.796\nage_cat30-49  0.059569   0.116402   0.512    0.609\nage_cat50-69 -0.388964   0.262384  -1.482    0.138\nage_cat70+   -0.647443   0.917375  -0.706    0.480\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5712.4  on 4166  degrees of freedom\nResidual deviance: 5700.2  on 4153  degrees of freedom\nAIC: 5728.2\n\nNumber of Fisher Scoring iterations: 4\n\n\nIf you want to include two variables and an interaction between them you can separate them with an asterisk * instead of a +. Separate them with a colon : if you are only specifying the interaction. For example:\n\nglm(outcome ~ gender + age_cat * fever, family = \"binomial\", data = linelist)\n\nOptionally, you can use this code to leverage the pre-defined vector of column names and re-create the above command using str_c(). This might be useful if your explanatory variable names are changing, or you don’t want to type them all out again.\n\n## run a regression with all variables of interest \nmv_reg &lt;- explanatory_vars %&gt;%  ## begin with vector of explanatory column names\n  str_c(collapse = \"+\") %&gt;%     ## combine all names of the variables of interest separated by a plus\n  str_c(\"outcome ~ \", .) %&gt;%    ## combine the names of variables of interest with outcome in formula style\n  glm(family = \"binomial\",      ## define type of glm as logistic,\n      data = linelist)          ## define your dataset\n\n\n\nYou can build your model step-by-step, saving various models that include certain explanatory variables. You can compare these models with likelihood-ratio tests using lrtest() from the package lmtest, as below:\nNOTE: Using base anova(model1, model2, test = \"Chisq) produces the same results \n\nmodel1 &lt;- glm(outcome ~ age_cat, family = \"binomial\", data = linelist)\nmodel2 &lt;- glm(outcome ~ age_cat + gender, family = \"binomial\", data = linelist)\n\nlmtest::lrtest(model1, model2)\n\nLikelihood ratio test\n\nModel 1: outcome ~ age_cat\nModel 2: outcome ~ age_cat + gender\n  #Df  LogLik Df Chisq Pr(&gt;Chisq)\n1   8 -2852.6                    \n2   9 -2852.6  1 2e-04     0.9883\n\n\nAnother option is to take the model object and apply the step() function from the stats package. Specify which variable selection direction you want use when building the model.\n\n## choose a model using forward selection based on AIC\n## you can also do \"backward\" or \"both\" by adjusting the direction\nfinal_mv_reg &lt;- mv_reg %&gt;%\n  step(direction = \"forward\", trace = FALSE)\n\nYou can also turn off scientific notation in your R session, for clarity:\n\noptions(scipen=999)\n\nAs described in the section on univariate analysis, pass the model output to tidy() to exponentiate the log odds and CIs. Finally we round all numeric columns to two decimal places. Scroll through to see all the rows.\n\nmv_tab_base &lt;- final_mv_reg %&gt;% \n  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %&gt;%  ## get a tidy dataframe of estimates \n  mutate(across(where(is.numeric), round, digits = 2))          ## round \n\nHere is what the resulting data frame looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe gtsummary package provides the tbl_regression() function, which will take the outputs from a regression (glm() in this case) and produce an nice summary table.\n\n## show results table of final regression \nmv_tab &lt;- tbl_regression(final_mv_reg, exponentiate = TRUE)\n\nLet’s see the table:\n\nmv_tab\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    gender\n1.00\n0.88, 1.14\n&gt;0.9\n    fever\n1.00\n0.86, 1.18\n&gt;0.9\n    chills\n1.03\n0.89, 1.21\n0.7\n    cough\n1.15\n0.96, 1.37\n0.12\n    aches\n0.93\n0.76, 1.14\n0.5\n    vomit\n1.09\n0.96, 1.23\n0.2\n    age_cat\n\n\n\n        0-4\n—\n—\n\n        5-9\n0.94\n0.77, 1.15\n0.5\n        10-14\n1.15\n0.93, 1.41\n0.2\n        15-19\n0.99\n0.79, 1.24\n&gt;0.9\n        20-29\n1.03\n0.84, 1.26\n0.8\n        30-49\n1.06\n0.85, 1.33\n0.6\n        50-69\n0.68\n0.40, 1.13\n0.14\n        70+\n0.52\n0.07, 3.19\n0.5\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nYou can also combine several different output tables produced by gtsummary with the tbl_merge() function. We now combine the multivariable results with the gtsummary univariate results that we created above:\n\n## combine with univariate results \ntbl_merge(\n  tbls = list(univ_tab, mv_tab),                          # combine\n  tab_spanner = c(\"**Univariate**\", \"**Multivariable**\")) # set header names\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Univariate\n      \n      \n        Multivariable\n      \n    \n    \n      N\n      OR1\n      95% CI1\n      p-value\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    gender\n4,167\n1.00\n0.88, 1.13\n&gt;0.9\n1.00\n0.88, 1.14\n&gt;0.9\n    fever\n4,167\n1.00\n0.85, 1.17\n&gt;0.9\n1.00\n0.86, 1.18\n&gt;0.9\n    chills\n4,167\n1.03\n0.89, 1.21\n0.7\n1.03\n0.89, 1.21\n0.7\n    cough\n4,167\n1.15\n0.97, 1.37\n0.11\n1.15\n0.96, 1.37\n0.12\n    aches\n4,167\n0.93\n0.76, 1.14\n0.5\n0.93\n0.76, 1.14\n0.5\n    vomit\n4,167\n1.09\n0.96, 1.23\n0.2\n1.09\n0.96, 1.23\n0.2\n    age_cat\n4,167\n\n\n\n\n\n\n        0-4\n\n—\n—\n\n—\n—\n\n        5-9\n\n0.94\n0.77, 1.15\n0.5\n0.94\n0.77, 1.15\n0.5\n        10-14\n\n1.15\n0.93, 1.42\n0.2\n1.15\n0.93, 1.41\n0.2\n        15-19\n\n0.99\n0.80, 1.24\n&gt;0.9\n0.99\n0.79, 1.24\n&gt;0.9\n        20-29\n\n1.03\n0.84, 1.26\n0.8\n1.03\n0.84, 1.26\n0.8\n        30-49\n\n1.07\n0.85, 1.33\n0.6\n1.06\n0.85, 1.33\n0.6\n        50-69\n\n0.68\n0.41, 1.13\n0.13\n0.68\n0.40, 1.13\n0.14\n        70+\n\n0.53\n0.07, 3.20\n0.5\n0.52\n0.07, 3.19\n0.5\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\nAn alternative way of combining the glm()/tidy() univariate and multivariable outputs is with the dplyr join functions.\n\nJoin the univariate results from earlier (univ_tab_base, which contains counts) with the tidied multivariable results mv_tab_base\n\nUse select() to keep only the columns we want, specify their order, and re-name them\n\nUse round() with two decimal places on all the column that are class Double\n\n\n## combine univariate and multivariable tables \nleft_join(univ_tab_base, mv_tab_base, by = \"term\") %&gt;% \n  ## choose columns and rename them\n  select( # new name =  old name\n    \"characteristic\" = term, \n    \"recovered\"      = \"0\", \n    \"dead\"           = \"1\", \n    \"univ_or\"        = estimate.x, \n    \"univ_ci_low\"    = conf.low.x, \n    \"univ_ci_high\"   = conf.high.x,\n    \"univ_pval\"      = p.value.x, \n    \"mv_or\"          = estimate.y, \n    \"mvv_ci_low\"     = conf.low.y, \n    \"mv_ci_high\"     = conf.high.y,\n    \"mv_pval\"        = p.value.y \n  ) %&gt;% \n  mutate(across(where(is.double), round, 2))   \n\n# A tibble: 20 × 11\n   characteristic recovered  dead univ_or univ_ci_low univ_ci_high univ_pval\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)          909  1168    1.28        1.18         1.4       0   \n 2 gender               916  1174    1           0.88         1.13      0.97\n 3 (Intercept)          340   436    1.28        1.11         1.48      0   \n 4 fever               1485  1906    1           0.85         1.17      0.99\n 5 (Intercept)         1472  1877    1.28        1.19         1.37      0   \n 6 chills               353   465    1.03        0.89         1.21      0.68\n 7 (Intercept)          272   309    1.14        0.97         1.34      0.13\n 8 cough               1553  2033    1.15        0.97         1.37      0.11\n 9 (Intercept)         1636  2114    1.29        1.21         1.38      0   \n10 aches                189   228    0.93        0.76         1.14      0.51\n11 (Intercept)          931  1144    1.23        1.13         1.34      0   \n12 vomit                894  1198    1.09        0.96         1.23      0.17\n13 (Intercept)          338   427    1.26        1.1          1.46      0   \n14 age_cat5-9           365   433    0.94        0.77         1.15      0.54\n15 age_cat10-14         273   396    1.15        0.93         1.42      0.2 \n16 age_cat15-19         238   299    0.99        0.8          1.24      0.96\n17 age_cat20-29         345   448    1.03        0.84         1.26      0.79\n18 age_cat30-49         228   307    1.07        0.85         1.33      0.58\n19 age_cat50-69          35    30    0.68        0.41         1.13      0.13\n20 age_cat70+             3     2    0.53        0.07         3.2       0.49\n# ℹ 4 more variables: mv_or &lt;dbl&gt;, mvv_ci_low &lt;dbl&gt;, mv_ci_high &lt;dbl&gt;,\n#   mv_pval &lt;dbl&gt;"
  },
  {
    "objectID": "readings/readings.html",
    "href": "readings/readings.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome!\nIn this module of the course, we will be learning how to use R to analyze data. This workbook will contain pre-class readings, in-class materials, and links to additional resources.\nMost of this workbook has been adapted from The Epidemiologist R Handbook. You can find more details at the bottom of this page.\n\n\nAs stated on the R project website, R is a programming language and environment for statistical computing and graphics. It is highly versatile, extendable, and community-driven.\n\n\nThe learning curve might be steeper than with other software, but with R, the results of your analysis do not rely on remembering a succession of pointing and clicking, but instead on a series of written commands, and that’s a good thing! So, if you want to redo your analysis because you collected more data, you don’t have to remember which button you clicked in which order to obtain your results; you just have to run your script again.\nWorking with scripts makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes.\nWorking with scripts forces you to have a deeper understanding of what you are doing, and facilitates your learning and comprehension of the methods you use.\n\n\n\nReproducibility means that someone else (including your future self) can obtain the same results from the same dataset when using the same analysis code.\nR integrates with other tools to generate manuscripts or reports from your code. If you collect more data, or fix a mistake in your dataset, the figures and the statistical tests in your manuscript or report are updated automatically.\nAn increasing number of journals and funding agencies expect analyses to be reproducible, so knowing R will give you an edge with these requirements.\n\n\n\nWith 10000+ packages1 that can be installed to extend its capabilities, R provides a framework that allows you to combine statistical approaches from many scientific disciplines to best suit the analytical framework you need to analyse your data. For instance, R has packages for image analysis, GIS, time series, population genetics, and a lot more.\n\n\n\n\n\nExponential increase of the number of packages available on CRAN, the Comprehensive R Archive Network. From the R Journal, Volume 10/2, December 2018.\n\n\n\n\n\n\n\nThe skills you learn with R scale easily with the size of your dataset. Whether your dataset has hundreds or millions of lines, it won’t make much difference to you.\nR is designed for data analysis. It comes with special data structures and data types that make handling of missing data and statistical factors convenient.\nR can connect to spreadsheets, databases, and many other data formats, on your computer or on the web.\n\n\n\nThe plotting functionalities in R are extensive, and allow you to adjust any aspect of your graph to convey most effectively the message from your data.\n\n\n\nThousands of people use R daily. Many of them are willing to help you through mailing lists and websites such as Stack Overflow, or on the RStudio community. These broad user communities extend to specialised areas such as bioinformatics. One such subset of the R community is Bioconductor, a scientific project for analysis and comprehension “of data from current and emerging biological assays.” Another example is R-Ladies, a worldwide organization whose mission is to promote gender diversity in the R community. It is one of the largest organizations of R users and likely has a chapter near you!\n\n\n\nAnyone can inspect the source code to see how R works. Because of this transparency, there is less chance for mistakes, and if you (or someone else) find some, you can report and fix bugs.\n\n\n\n\nMost of the materials in this workbook have been adapted from The Epidemiologist R Handbook with some changes made and materials incorporated from other sources. These additional sources are attributed in the chapters they are a part of. The Epidemiologist R Handbook is licensed by Applied Epi Incorporated under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)."
  },
  {
    "objectID": "readings/readings.html#why-learn-r",
    "href": "readings/readings.html#why-learn-r",
    "title": "Introduction",
    "section": "",
    "text": "As stated on the R project website, R is a programming language and environment for statistical computing and graphics. It is highly versatile, extendable, and community-driven.\n\n\nThe learning curve might be steeper than with other software, but with R, the results of your analysis do not rely on remembering a succession of pointing and clicking, but instead on a series of written commands, and that’s a good thing! So, if you want to redo your analysis because you collected more data, you don’t have to remember which button you clicked in which order to obtain your results; you just have to run your script again.\nWorking with scripts makes the steps you used in your analysis clear, and the code you write can be inspected by someone else who can give you feedback and spot mistakes.\nWorking with scripts forces you to have a deeper understanding of what you are doing, and facilitates your learning and comprehension of the methods you use.\n\n\n\nReproducibility means that someone else (including your future self) can obtain the same results from the same dataset when using the same analysis code.\nR integrates with other tools to generate manuscripts or reports from your code. If you collect more data, or fix a mistake in your dataset, the figures and the statistical tests in your manuscript or report are updated automatically.\nAn increasing number of journals and funding agencies expect analyses to be reproducible, so knowing R will give you an edge with these requirements.\n\n\n\nWith 10000+ packages1 that can be installed to extend its capabilities, R provides a framework that allows you to combine statistical approaches from many scientific disciplines to best suit the analytical framework you need to analyse your data. For instance, R has packages for image analysis, GIS, time series, population genetics, and a lot more.\n\n\n\n\n\nExponential increase of the number of packages available on CRAN, the Comprehensive R Archive Network. From the R Journal, Volume 10/2, December 2018.\n\n\n\n\n\n\n\nThe skills you learn with R scale easily with the size of your dataset. Whether your dataset has hundreds or millions of lines, it won’t make much difference to you.\nR is designed for data analysis. It comes with special data structures and data types that make handling of missing data and statistical factors convenient.\nR can connect to spreadsheets, databases, and many other data formats, on your computer or on the web.\n\n\n\nThe plotting functionalities in R are extensive, and allow you to adjust any aspect of your graph to convey most effectively the message from your data.\n\n\n\nThousands of people use R daily. Many of them are willing to help you through mailing lists and websites such as Stack Overflow, or on the RStudio community. These broad user communities extend to specialised areas such as bioinformatics. One such subset of the R community is Bioconductor, a scientific project for analysis and comprehension “of data from current and emerging biological assays.” Another example is R-Ladies, a worldwide organization whose mission is to promote gender diversity in the R community. It is one of the largest organizations of R users and likely has a chapter near you!\n\n\n\nAnyone can inspect the source code to see how R works. Because of this transparency, there is less chance for mistakes, and if you (or someone else) find some, you can report and fix bugs."
  },
  {
    "objectID": "readings/readings.html#sources-and-references",
    "href": "readings/readings.html#sources-and-references",
    "title": "Introduction",
    "section": "",
    "text": "Most of the materials in this workbook have been adapted from The Epidemiologist R Handbook with some changes made and materials incorporated from other sources. These additional sources are attributed in the chapters they are a part of. The Epidemiologist R Handbook is licensed by Applied Epi Incorporated under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)."
  },
  {
    "objectID": "readings/readings.html#footnotes",
    "href": "readings/readings.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. add-ons that confer R with new functionality, such as bioinformatics data analysis.↩︎"
  },
  {
    "objectID": "readings/missing-data.html",
    "href": "readings/missing-data.html",
    "title": "Missing data",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\n\n\n\n\n\n\n\n\nThis chapter will cover how to assess missingness, filter out rows by missingness, and handle how NA is displayed in plots.\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\npacman::p_load(\n  rio,           # import/export\n  tidyverse,     # data mgmt and viz\n  naniar,        # assess and visualize missingness\n  mice           # missing data imputation\n)\n\n\n\n\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.\n\n\n\n\n\n\n\n\n\n\nWhen importing your data, be aware of values that should be classified as missing. For example, 99, 999, “Missing”, blank cells (““), or cells with an empty space (” “). You can convert these to NA (R’s version of missing data) during the data import command.\n\n\n\n\n\n\n\nBelow we explore ways that missingness is presented and assessed in R, along with some adjacent values and functions.\n\n\nIn R, missing values are represented by a reserved (special) value - NA. Note that this is typed without quotes. “NA” is different and is just a normal character value (also a Beatles lyric from the song Hey Jude).\nYour data may have other ways of representing missingness, such as “99”, or “Missing”, or “Unknown” - you may even have empty character value “” which looks “blank”, or a single space ” “. Be aware of these and consider whether to convert them to NA during import or during data cleaning with na_if().\nIn your data cleaning, you may also want to convert the other way - changing all NA to “Missing” or similar with replace_na() or with fct_explicit_na() for factors.\n\n\n\nMost of the time, NA represents a missing value and everything works fine. However, in some circumstances you may encounter the need for variations of NA specific to an object class (character, numeric, etc). This will be rare, but you should be aware.\nThe typical scenario for this is when creating a new column with the dplyr function case_when(). This function evaluates every row in the data frame, assess whether the rows meets specified logical criteria (right side of the code), and assigns the correct new value (left side of the code). Importantly: all values on the right side must be the same class.\n\nlinelist &lt;- linelist %&gt;% \n  \n  # Create new \"age_years\" column from \"age\" column\n  mutate(age_years = case_when(\n    age_unit == \"years\"  ~ age,       # if age is given in years, assign original value\n    age_unit == \"months\" ~ age/12,    # if age is given in months, divide by 12\n    is.na(age_unit)      ~ age,       # if age UNIT is missing, assume years\n    TRUE                 ~ NA_real_)) # any other circumstance, assign missing\n\nIf you want NA on the right side, you may need to specify one of the special NA options listed below. If the other right side values are character, consider using “Missing” instead or otherwise use NA_character_. If they are all numeric, use NA_real_. If they are all dates or logical, you can use NA.\n\nNA - use for dates or logical TRUE/FALSE\nNA_character_ - use for characters\n\nNA_real_ - use for numeric\n\nAgain, it is not likely you will encounter these variations unless you are using case_when() to create a new column. See the R documentation on NA for more information.\n\n\n\nNULL is another reserved value in R. It is the logical representation of a statement that is neither true nor false. It is returned by expressions or functions whose values are undefined. Generally do not assign NULL as a value, unless writing functions or perhaps writing a [shiny app][Dashboards with Shiny] to return NULL in specific scenarios.\nNull-ness can be assessed using is.null() and conversion can made with as.null().\nSee this blog post on the difference between NULL and NA.\n\n\n\nImpossible values are represented by the special value NaN. An example of this is when you force R to divide 0 by 0. You can assess this with is.nan(). You may also encounter complementary functions including is.infinite() and is.finite().\n\n\n\nInf represents an infinite value, such as when you divide a number by 0.\nAs an example of how this might impact your work: let’s say you have a vector/column z that contains these values: z &lt;- c(1, 22, NA, Inf, NaN, 5)\nIf you want to use max() on the column to find the highest value, you can use the na.rm = TRUE to remove the NA from the calculation, but the Inf and NaN remain and Inf will be returned. To resolve this, you can use brackets [ ] and is.finite() to subset such that only finite values are used for the calculation: max(z[is.finite(z)]).\n\nz &lt;- c(1, 22, NA, Inf, NaN, 5)\nmax(z)                           # returns NA\nmax(z, na.rm=T)                  # returns Inf\nmax(z[is.finite(z)])             # returns 22\n\n\n\n\n\n\n\n\n\n\n\nR command\nOutcome\n\n\n\n\n5 / 0\nInf\n\n\n0 / 0\nNaN\n\n\n5 / NA\nNA\n\n\n5 / Inf |0NA - 5|NAInf / 5|Infclass(NA)| \"logical\"class(NaN)| \"numeric\"class(Inf)| \"numeric\"class(NULL)`\n“NULL”\n\n\n\n“NAs introduced by coercion” is a common warning message. This can happen if you attempt to make an illegal conversion like inserting a character value into a vector that is otherwise numeric.\n\nas.numeric(c(\"10\", \"20\", \"thirty\", \"40\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 10 20 NA 40\n\n\nNULL is ignored in a vector.\n\nmy_vector &lt;- c(25, NA, 10, NULL)  # define\nmy_vector                         # print\n\n[1] 25 NA 10\n\n\nVariance of one number results in NA.\n\nvar(22)\n\n[1] NA\n\n\n\n\n\n\n\nThe following are useful base R functions when assessing or handling missing values:\n\n\nUse is.na()to identify missing values, or use its opposite (with ! in front) to identify non-missing values. These both return a logical value (TRUE or FALSE). Remember that you can sum() the resulting vector to count the number TRUE, e.g. sum(is.na(linelist$date_outcome)).\n\nmy_vector &lt;- c(1, 4, 56, NA, 5, NA, 22)\nis.na(my_vector)\n\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n!is.na(my_vector)\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n\nsum(is.na(my_vector))\n\n[1] 2\n\n\n\n\n\nThis function, if applied to a data frame, will remove rows with any missing values. It is also from base R.\nIf applied to a vector, it will remove NA values from the vector it is applied to. For example:\n\nna.omit(my_vector)\n\n[1]  1  4 56  5 22\nattr(,\"na.action\")\n[1] 4 6\nattr(,\"class\")\n[1] \"omit\"\n\n\n\n\n\nThis is a tidyr function that is useful in a [data cleaning pipeline][Cleaning data and core functions]. If run with the parentheses empty, it removes rows with any missing values. If column names are specified in the parentheses, rows with missing values in those columns will be dropped. You can also use “tidyselect” syntax to specify the columns.\n\nlinelist %&gt;% \n  drop_na(case_id, date_onset, age) # drops rows missing values for any of these columns\n\n\n\n\nWhen you run a mathematical function such as max(), min(), sum() or mean(), if there are any NA values present the returned value will be NA. This default behavior is intentional, so that you are alerted if any of your data are missing.\nYou can avoid this by removing missing values from the calculation. To do this, include the argument na.rm = TRUE (“na.rm” stands for “remove NA”).\n\nmy_vector &lt;- c(1, 4, 56, NA, 5, NA, 22)\n\nmean(my_vector)     \n\n[1] NA\n\nmean(my_vector, na.rm = TRUE)\n\n[1] 17.6\n\n\n\n\n\n\n\n\nTo quickly remove rows with missing values, use the dplyr function drop_na().\nThe original linelist has nrow(linelist) rows. The adjusted number of rows is shown below:\n\nlinelist %&gt;% \n  drop_na() %&gt;%     # remove rows with ANY missing values\n  nrow()\n\n[1] 1818\n\n\nYou can specify to drop rows with missingness in certain columns:\n\nlinelist %&gt;% \n  drop_na(date_onset) %&gt;% # remove rows missing date_onset \n  nrow()\n\n[1] 5632\n\n\nYou can list columns one after the other, or use “tidyselect” helper functions:\n\nlinelist %&gt;% \n  drop_na(contains(\"date\")) %&gt;% # remove rows missing values in any \"date\" column \n  nrow()\n\n[1] 3029\n\n\n\n\n\n\nIt is often wise to report the number of values excluded from a plot in a caption. Below is an example:\nIn ggplot(), you can add labs() and within it a caption =. In the caption, you can use str_glue() from stringr package to paste values together into a sentence dynamically so they will adjust to the data. An example is below:\n\nNote the use of \\n for a new line.\n\nNote that if multiple column would contribute to values not being plotted (e.g. age or sex if those are reflected in the plot), then you must filter on those columns as well to correctly calculate the number not shown.\n\n\nlabs(\n  title = \"\",\n  y = \"\",\n  x = \"\",\n  caption  = stringr::str_glue(\n  \"n = {nrow(central_data)} from Central Hospital;\n  {nrow(central_data %&gt;% filter(is.na(date_onset)))} cases missing date of onset and not shown.\"))  \n\nSometimes, it can be easier to save the string as an object in commands prior to the ggplot() command, and simply reference the named string object within the str_glue()."
  },
  {
    "objectID": "readings/missing-data.html#missing-values-in-r",
    "href": "readings/missing-data.html#missing-values-in-r",
    "title": "Missing data",
    "section": "",
    "text": "Below we explore ways that missingness is presented and assessed in R, along with some adjacent values and functions.\n\n\nIn R, missing values are represented by a reserved (special) value - NA. Note that this is typed without quotes. “NA” is different and is just a normal character value (also a Beatles lyric from the song Hey Jude).\nYour data may have other ways of representing missingness, such as “99”, or “Missing”, or “Unknown” - you may even have empty character value “” which looks “blank”, or a single space ” “. Be aware of these and consider whether to convert them to NA during import or during data cleaning with na_if().\nIn your data cleaning, you may also want to convert the other way - changing all NA to “Missing” or similar with replace_na() or with fct_explicit_na() for factors.\n\n\n\nMost of the time, NA represents a missing value and everything works fine. However, in some circumstances you may encounter the need for variations of NA specific to an object class (character, numeric, etc). This will be rare, but you should be aware.\nThe typical scenario for this is when creating a new column with the dplyr function case_when(). This function evaluates every row in the data frame, assess whether the rows meets specified logical criteria (right side of the code), and assigns the correct new value (left side of the code). Importantly: all values on the right side must be the same class.\n\nlinelist &lt;- linelist %&gt;% \n  \n  # Create new \"age_years\" column from \"age\" column\n  mutate(age_years = case_when(\n    age_unit == \"years\"  ~ age,       # if age is given in years, assign original value\n    age_unit == \"months\" ~ age/12,    # if age is given in months, divide by 12\n    is.na(age_unit)      ~ age,       # if age UNIT is missing, assume years\n    TRUE                 ~ NA_real_)) # any other circumstance, assign missing\n\nIf you want NA on the right side, you may need to specify one of the special NA options listed below. If the other right side values are character, consider using “Missing” instead or otherwise use NA_character_. If they are all numeric, use NA_real_. If they are all dates or logical, you can use NA.\n\nNA - use for dates or logical TRUE/FALSE\nNA_character_ - use for characters\n\nNA_real_ - use for numeric\n\nAgain, it is not likely you will encounter these variations unless you are using case_when() to create a new column. See the R documentation on NA for more information.\n\n\n\nNULL is another reserved value in R. It is the logical representation of a statement that is neither true nor false. It is returned by expressions or functions whose values are undefined. Generally do not assign NULL as a value, unless writing functions or perhaps writing a [shiny app][Dashboards with Shiny] to return NULL in specific scenarios.\nNull-ness can be assessed using is.null() and conversion can made with as.null().\nSee this blog post on the difference between NULL and NA.\n\n\n\nImpossible values are represented by the special value NaN. An example of this is when you force R to divide 0 by 0. You can assess this with is.nan(). You may also encounter complementary functions including is.infinite() and is.finite().\n\n\n\nInf represents an infinite value, such as when you divide a number by 0.\nAs an example of how this might impact your work: let’s say you have a vector/column z that contains these values: z &lt;- c(1, 22, NA, Inf, NaN, 5)\nIf you want to use max() on the column to find the highest value, you can use the na.rm = TRUE to remove the NA from the calculation, but the Inf and NaN remain and Inf will be returned. To resolve this, you can use brackets [ ] and is.finite() to subset such that only finite values are used for the calculation: max(z[is.finite(z)]).\n\nz &lt;- c(1, 22, NA, Inf, NaN, 5)\nmax(z)                           # returns NA\nmax(z, na.rm=T)                  # returns Inf\nmax(z[is.finite(z)])             # returns 22\n\n\n\n\n\n\n\n\n\n\n\nR command\nOutcome\n\n\n\n\n5 / 0\nInf\n\n\n0 / 0\nNaN\n\n\n5 / NA\nNA\n\n\n5 / Inf |0NA - 5|NAInf / 5|Infclass(NA)| \"logical\"class(NaN)| \"numeric\"class(Inf)| \"numeric\"class(NULL)`\n“NULL”\n\n\n\n“NAs introduced by coercion” is a common warning message. This can happen if you attempt to make an illegal conversion like inserting a character value into a vector that is otherwise numeric.\n\nas.numeric(c(\"10\", \"20\", \"thirty\", \"40\"))\n\nWarning: NAs introduced by coercion\n\n\n[1] 10 20 NA 40\n\n\nNULL is ignored in a vector.\n\nmy_vector &lt;- c(25, NA, 10, NULL)  # define\nmy_vector                         # print\n\n[1] 25 NA 10\n\n\nVariance of one number results in NA.\n\nvar(22)\n\n[1] NA"
  },
  {
    "objectID": "readings/missing-data.html#useful-functions",
    "href": "readings/missing-data.html#useful-functions",
    "title": "Missing data",
    "section": "",
    "text": "The following are useful base R functions when assessing or handling missing values:\n\n\nUse is.na()to identify missing values, or use its opposite (with ! in front) to identify non-missing values. These both return a logical value (TRUE or FALSE). Remember that you can sum() the resulting vector to count the number TRUE, e.g. sum(is.na(linelist$date_outcome)).\n\nmy_vector &lt;- c(1, 4, 56, NA, 5, NA, 22)\nis.na(my_vector)\n\n[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE\n\n!is.na(my_vector)\n\n[1]  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE\n\nsum(is.na(my_vector))\n\n[1] 2\n\n\n\n\n\nThis function, if applied to a data frame, will remove rows with any missing values. It is also from base R.\nIf applied to a vector, it will remove NA values from the vector it is applied to. For example:\n\nna.omit(my_vector)\n\n[1]  1  4 56  5 22\nattr(,\"na.action\")\n[1] 4 6\nattr(,\"class\")\n[1] \"omit\"\n\n\n\n\n\nThis is a tidyr function that is useful in a [data cleaning pipeline][Cleaning data and core functions]. If run with the parentheses empty, it removes rows with any missing values. If column names are specified in the parentheses, rows with missing values in those columns will be dropped. You can also use “tidyselect” syntax to specify the columns.\n\nlinelist %&gt;% \n  drop_na(case_id, date_onset, age) # drops rows missing values for any of these columns\n\n\n\n\nWhen you run a mathematical function such as max(), min(), sum() or mean(), if there are any NA values present the returned value will be NA. This default behavior is intentional, so that you are alerted if any of your data are missing.\nYou can avoid this by removing missing values from the calculation. To do this, include the argument na.rm = TRUE (“na.rm” stands for “remove NA”).\n\nmy_vector &lt;- c(1, 4, 56, NA, 5, NA, 22)\n\nmean(my_vector)     \n\n[1] NA\n\nmean(my_vector, na.rm = TRUE)\n\n[1] 17.6"
  },
  {
    "objectID": "readings/missing-data.html#using-data-with-missing-values",
    "href": "readings/missing-data.html#using-data-with-missing-values",
    "title": "Missing data",
    "section": "",
    "text": "To quickly remove rows with missing values, use the dplyr function drop_na().\nThe original linelist has nrow(linelist) rows. The adjusted number of rows is shown below:\n\nlinelist %&gt;% \n  drop_na() %&gt;%     # remove rows with ANY missing values\n  nrow()\n\n[1] 1818\n\n\nYou can specify to drop rows with missingness in certain columns:\n\nlinelist %&gt;% \n  drop_na(date_onset) %&gt;% # remove rows missing date_onset \n  nrow()\n\n[1] 5632\n\n\nYou can list columns one after the other, or use “tidyselect” helper functions:\n\nlinelist %&gt;% \n  drop_na(contains(\"date\")) %&gt;% # remove rows missing values in any \"date\" column \n  nrow()\n\n[1] 3029\n\n\n\n\n\n\nIt is often wise to report the number of values excluded from a plot in a caption. Below is an example:\nIn ggplot(), you can add labs() and within it a caption =. In the caption, you can use str_glue() from stringr package to paste values together into a sentence dynamically so they will adjust to the data. An example is below:\n\nNote the use of \\n for a new line.\n\nNote that if multiple column would contribute to values not being plotted (e.g. age or sex if those are reflected in the plot), then you must filter on those columns as well to correctly calculate the number not shown.\n\n\nlabs(\n  title = \"\",\n  y = \"\",\n  x = \"\",\n  caption  = stringr::str_glue(\n  \"n = {nrow(central_data)} from Central Hospital;\n  {nrow(central_data %&gt;% filter(is.na(date_onset)))} cases missing date of onset and not shown.\"))  \n\nSometimes, it can be easier to save the string as an object in commands prior to the ggplot() command, and simply reference the named string object within the str_glue()."
  },
  {
    "objectID": "readings/joining.html",
    "href": "readings/joining.html",
    "title": "Joining data",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\n\n\n\n\n\nAbove: an animated example of a left join (image source)\nThis page describes ways to “join”, “match”, “link” “bind”, and otherwise combine data frames.\nIt is uncommon that your epidemiological analysis or workflow does not involve multiple sources of data, and the linkage of multiple datasets. Perhaps you need to connect laboratory data to patient clinical outcomes, or Google mobility data to infectious disease trends, or even a dataset at one stage of analysis to a transformed version of itself.\nIn this page we demonstrate code to:\n\nConduct joins of two data frames such that rows are matched based on common values in identifier columns\n\nJoin two data frames based on probabilistic (likely) matches between values\n\nExpand a data frame by directly binding or (“appending”) rows or columns from another data frame\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\npacman::p_load(\n  rio,            # import and export\n  here,           # locate files \n  tidyverse,      # data management and visualisation\n  RecordLinkage,  # probabilistic matches\n  fastLink        # probabilistic matches\n)\n\n\n\n\nTo begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file).\n\n# import case linelist \nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\nThe first 50 rows of the linelist are displayed below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the joining section below, we will use the following datasets:\n\nA “miniature” version of the case linelist, containing only the columns case_id, date_onset, and hospital, and only the first 10 rows\n\nA separate data frame named hosp_info, which contains more details about each hospital\n\nIn the section on probabilistic matching, we will use two different small datasets. The code to create those datasets is given in that section.\n\n\nBelow is the the miniature case linelist, which contains only 10 rows and only columns case_id, date_onset, and hospital.\n\nlinelist_mini &lt;- linelist %&gt;%                 # start with original linelist\n  select(case_id, date_onset, hospital) %&gt;%   # select columns\n  head(10)                                    # only take the first 10 rows\n\n\n\n\n\n\n\n\n\n\n\nBelow is the code to create a separate data frame with additional information about seven hospitals (the catchment population, and the level of care available). Note that the name “Military Hospital” belongs to two different hospitals - one a primary level serving 10000 residents and the other a secondary level serving 50280 residents.\n\n# Make the hospital information data frame\nhosp_info = data.frame(\n  hosp_name     = c(\"central hospital\", \"military\", \"military\", \"port\", \"St. Mark's\", \"ignace\", \"sisters\"),\n  catchment_pop = c(1950280, 40500, 10000, 50280, 12000, 5000, 4200),\n  level         = c(\"Tertiary\", \"Secondary\", \"Primary\", \"Secondary\", \"Secondary\", \"Primary\", \"Primary\")\n)\n\nHere is this data frame:\n\n\n\n\n\n\n\n\n\n\n\n\nTraditional joins (non-probabilistic) are case-sensitive and require exact character matches between values in the two data frames. To demonstrate some of the cleaning steps you might need to do before initiating a join, we will clean and align the linelist_mini and hosp_info datasets now.\nIdentify differences\nWe need the values of the hosp_name column in the hosp_info data frame to match the values of the hospital column in the linelist_mini data frame.\nHere are the values in the linelist_mini data frame, printed with the base R function unique():\n\nunique(linelist_mini$hospital)\n\n[1] \"Other\"                               \n[2] \"Missing\"                             \n[3] \"St. Mark's Maternity Hospital (SMMH)\"\n[4] \"Port Hospital\"                       \n[5] \"Military Hospital\"                   \n\n\nand here are the values in the hosp_info data frame:\n\nunique(hosp_info$hosp_name)\n\n[1] \"central hospital\" \"military\"         \"port\"             \"St. Mark's\"      \n[5] \"ignace\"           \"sisters\"         \n\n\nYou can see that while some of the hospitals exist in both data frames, there are many differences in spelling.\nAlign values\nWe begin by cleaning the values in the hosp_info data frame. We can re-code values with logical criteria using dplyr’s case_when() function. For the four hospitals that exist in both data frames we change the values to align with the values in linelist_mini. The other hospitals we leave the values as they are (TRUE ~ hosp_name).\nCAUTION: Typically when cleaning one should create a new column (e.g. hosp_name_clean), but for ease of demonstration we show modification of the old column\n\nhosp_info &lt;- hosp_info %&gt;% \n  mutate(\n    hosp_name = case_when(\n      # criteria                         # new value\n      hosp_name == \"military\"          ~ \"Military Hospital\",\n      hosp_name == \"port\"              ~ \"Port Hospital\",\n      hosp_name == \"St. Mark's\"        ~ \"St. Mark's Maternity Hospital (SMMH)\",\n      hosp_name == \"central hospital\"  ~ \"Central Hospital\",\n      TRUE                             ~ hosp_name\n      )\n    )\n\nThe hospital names that appear in both data frames are aligned. There are two hospitals in hosp_info that are not present in linelist_mini - we will deal with these later, in the join.\n\nunique(hosp_info$hosp_name)\n\n[1] \"Central Hospital\"                    \n[2] \"Military Hospital\"                   \n[3] \"Port Hospital\"                       \n[4] \"St. Mark's Maternity Hospital (SMMH)\"\n[5] \"ignace\"                              \n[6] \"sisters\"                             \n\n\nPrior to a join, it is often easiest to convert a column to all lowercase or all uppercase. If you need to convert all values in a column to UPPER or lower case, use mutate() and wrap the column with one of these functions from stringr.\nstr_to_upper()\nstr_to_upper()\nstr_to_title()\n\n\n\n\nThe dplyr package offers several different join functions. dplyr is included in the tidyverse package. These join functions are described below, with simple use cases.\nMany thanks to https://github.com/gadenbuie for the informative gifs!\n\n\n\nThe join commands can be run as standalone commands to join two data frames into a new object, or they can be used within a pipe chain (%&gt;%) to merge one data frame into another as it is being cleaned or otherwise modified.\nIn the example below, the function left_join() is used as a standalone command to create the a new joined_data data frame. The inputs are data frames 1 and 2 (df1 and df2). The first data frame listed is the baseline data frame, and the second one listed is joined to it.\nThe third argument by = is where you specify the columns in each data frame that will be used to aligns the rows in the two data frames. If the names of these columns are different, provide them within a c() vector as shown below, where the rows are matched on the basis of common values between the column ID in df1 and the column identifier in df2.\n\n# Join based on common values between column \"ID\" (first data frame) and column \"identifier\" (second data frame)\njoined_data &lt;- left_join(df1, df2, by = c(\"ID\" = \"identifier\"))\n\nIf the by columns in both data frames have the exact same name, you can just provide this one name, within quotes.\n\n# Joint based on common values in column \"ID\" in both data frames\njoined_data &lt;- left_join(df1, df2, by = \"ID\")\n\nIf you are joining the data frames based on common values across multiple fields, list these fields within the c() vector. This example joins rows if the values in three columns in each dataset align exactly.\n\n# join based on same first name, last name, and age\njoined_data &lt;- left_join(df1, df2, by = c(\"name\" = \"firstname\", \"surname\" = \"lastname\", \"Age\" = \"age\"))\n\nThe join commands can also be run within a pipe chain. This will modify the data frame being piped.\nIn the example below, df1 is is passed through the pipes, df2 is joined to it, and df is thus modified and re-defined.\n\ndf1 &lt;- df1 %&gt;%\n  filter(date_onset &lt; as.Date(\"2020-03-05\")) %&gt;% # miscellaneous cleaning \n  left_join(df2, by = c(\"ID\" = \"identifier\"))    # join df2 to df1\n\n\n\n\n\n\n\nWarning\n\n\n\nJoins are case-specific! Therefore it is useful to convert all values to lowercase or uppercase prior to joining.\n\n\n\n\n\n\nA left or right join is commonly used to add information to a data frame - new information is added only to rows that already existed in the baseline data frame. These are common joins in epidemiological work as they are used to add information from one dataset into another.\nIn using these joins, the written order of the data frames in the command is important*.\n\nIn a left join, the first data frame written is the baseline\n\nIn a right join, the second data frame written is the baseline\n\nAll rows of the baseline data frame are kept. Information in the other (secondary) data frame is joined to the baseline data frame only if there is a match via the identifier column(s). In addition:\n\nRows in the secondary data frame that do not match are dropped.\n\nIf there are many baseline rows that match to one row in the secondary data frame (many-to-one), the secondary information is added to each matching baseline row.\n\nIf a baseline row matches to multiple rows in the secondary data frame (one-to-many), all combinations are given, meaning new rows may be added to your returned data frame!\n\nAnimated examples of left and right joins (image source)\n\n\n\n\n\n\n\n\nExample\nBelow is the output of a left_join() of hosp_info (secondary data frame, view here) into linelist_mini (baseline data frame, view here). The original linelist_mini has nrow(linelist_mini) rows. The modified linelist_mini is displayed. Note the following:\n\nTwo new columns, catchment_pop and level have been added on the left side of linelist_mini\n\nAll original rows of the baseline data frame linelist_mini are kept\n\nAny original rows of linelist_mini for “Military Hospital” are duplicated because it matched to two rows in the secondary data frame, so both combinations are returned\n\nThe join identifier column of the secondary dataset (hosp_name) has disappeared because it is redundant with the identifier column in the primary dataset (hospital)\n\nWhen a baseline row did not match to any secondary row (e.g. when hospital is “Other” or “Missing”), NA (blank) fills in the columns from the secondary data frame\n\nRows in the secondary data frame with no match to the baseline data frame (“sisters” and “ignace” hospitals) were dropped\n\n\nlinelist_mini %&gt;% \n  left_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in left_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\nTo answer the above question, ask yourself “which data frame should retain all of its rows?” - use this one as the baseline. A left join keep all the rows in the first data frame written in the command, whereas a right join keeps all the rows in the second data frame.\nThe two commands below achieve the same output - 10 rows of hosp_info joined into a linelist_mini baseline, but they use different joins. The result is that the column order will differ based on whether hosp_info arrives from the right (in the left join) or arrives from the left (in the right join). The order of the rows may also shift accordingly. But both of these consequences can be subsequently addressed, using select() to re-order columns or arrange() to sort rows.\n\n# The two commands below achieve the same data, but with differently ordered rows and columns\nleft_join(linelist_mini, hosp_info, by = c(\"hospital\" = \"hosp_name\"))\nright_join(hosp_info, linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\nHere is the result of hosp_info into linelist_mini via a left join (new columns incoming from the right)\n\n\nWarning in left_join(linelist_mini, hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\nHere is the result of hosp_info into linelist_mini via a right join (new columns incoming from the left)\n\n\nWarning in right_join(hosp_info, linelist_mini, by = c(hosp_name = \"hospital\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 4 of `x` matches multiple rows in `y`.\nℹ Row 5 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\nAlso consider whether your use-case is within a pipe chain (%&gt;%). If the dataset in the pipes is the baseline, you will likely use a left join to add data to it.\n\n\n\n\n\nA full join is the most inclusive of the joins - it returns all rows from both data frames.\nIf there are any rows present in one and not the other (where no match was found), the data frame will include them and become longer. NA missing values are used to fill-in any gaps created. As you join, watch the number of columns and rows carefully to troubleshoot case-sensitivity and exact character matches.\nThe “baseline” data frame is the one written first in the command. Adjustment of this will not impact which records are returned by the join, but it can impact the resulting column order, row order, and which identifier columns are retained.\n\n\n\n\n\nAnimated example of a full join (image source)\nExample\nBelow is the output of a full_join() of hosp_info (originally nrow(hosp_info), view here) into linelist_mini (originally nrow(linelist_mini), view here). Note the following:\n\nAll baseline rows are kept (linelist_mini)\n\nRows in the secondary that do not match to the baseline are kept (“ignace” and “sisters”), with values in the corresponding baseline columns case_id and onset filled in with missing values\n\nLikewise, rows in the baseline data frame that do not match to the secondary (“Other” and “Missing”) are kept, with secondary columns catchment_pop and level filled-in with missing values\n\nIn the case of one-to-many or many-to-one matches (e.g. rows for “Military Hospital”), all possible combinations are returned (lengthening the final data frame)\n\nOnly the identifier column from the baseline is kept (hospital)\n\n\nlinelist_mini %&gt;% \n  full_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in full_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\n\n\nAn inner join is the most restrictive of the joins - it returns only rows with matches across both data frames.\nThis means that the number of rows in the baseline data frame may actually reduce. Adjustment of which data frame is the “baseline” (written first in the function) will not impact which rows are returned, but it will impact the column order, row order, and which identifier columns are retained.\n\n\n\n\n\nAnimated example of an inner join (image source)\nExample\nBelow is the output of an inner_join() of linelist_mini (baseline) with hosp_info (secondary). Note the following:\n\nBaseline rows with no match to the secondary data are removed (rows where hospital is “Missing” or “Other”)\n\nLikewise, rows from the secondary data frame that had no match in the baseline are removed (rows where hosp_name is “sisters” or “ignace”)\n\nOnly the identifier column from the baseline is kept (hospital)\n\n\nlinelist_mini %&gt;% \n  inner_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in inner_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\n\n\nA semi join is a “filtering join” which uses another dataset not to add rows or columns, but to perform filtering.\nA semi-join keeps all observations in the baseline data frame that have a match in the secondary data frame (but does not add new columns nor duplicate any rows for multiple matches). Read more about these “filtering” joins here.\n\n\n\n\n\nAnimated example of a semi join (image source)\nAs an example, the below code returns rows from the hosp_info data frame that have matches in linelist_mini based on hospital name.\n\nhosp_info %&gt;% \n  semi_join(linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\n                             hosp_name catchment_pop     level\n1                    Military Hospital         40500 Secondary\n2                    Military Hospital         10000   Primary\n3                        Port Hospital         50280 Secondary\n4 St. Mark's Maternity Hospital (SMMH)         12000 Secondary\n\n\n\n\n\n\nThe anti join is another “filtering join” that returns rows in the baseline data frame that do not have a match in the secondary data frame.\nRead more about filtering joins here.\nCommon scenarios for an anti-join include identifying records not present in another data frame, troubleshooting spelling in a join (reviewing records that should have matched), and examining records that were excluded after another join.\nAs with right_join() and left_join(), the baseline data frame (listed first) is important. The returned rows are from the baseline data frame only. Notice in the gif below that row in the secondary data frame (purple row 4) is not returned even though it does not match with the baseline.\n\n\n\n\n\nAnimated example of an anti join (image source)\n\n\nFor a simple example, let’s find the hosp_info hospitals that do not have any cases present in linelist_mini. We list hosp_info first, as the baseline data frame. The hospitals which are not present in linelist_mini are returned.\n\nhosp_info %&gt;% \n  anti_join(linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\n\n\n\n\n\n\n\n\n\n\nFor another example, let us say we ran an inner_join() between linelist_mini and hosp_info. This returns only a subset of the original linelist_mini records, as some are not present in hosp_info.\n\nlinelist_mini %&gt;% \n  inner_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in inner_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\nTo review the linelist_mini records that were excluded during the inner join, we can run an anti-join with the same settings (linelist_mini as the baseline).\n\nlinelist_mini %&gt;% \n  anti_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\n\n\n\n\n\nTo see the hosp_info records that were excluded in the inner join, we could also run an anti-join with hosp_info as the baseline data frame.\n\n\n\n\n\nAnother method of combining two data frames is “binding” them together. You can also think of this as “appending” or “adding” rows or columns.\nThis section will also discuss how to “align” the order of rows of one data frame to the order in another data frame. This topic is discussed below in the section on Binding columns.\n\n\nTo bind rows of one data frame to the bottom of another data frame, use bind_rows() from dplyr. It is very inclusive, so any column present in either data frame will be included in the output. A few notes:\n\nUnlike the base R version row.bind(), dplyr’s bind_rows() does not require that the order of columns be the same in both data frames. As long as the column names are spelled identically, it will align them correctly.\n\nYou can optionally specify the argument .id =. Provide a character column name. This will produce a new column that serves to identify which data frame each row originally came from.\n\nYou can use bind_rows() on a list of similarly-structured data frames to combine them into one data frame.\n\nOne common example of row binding is to bind a “total” row onto a descriptive table made with dplyr’s summarise() function. Below we create a table of case counts and median CT values by hospital with a total row.\nThe function summarise() is used on data grouped by hospital to return a summary data frame by hospital. But the function summarise() does not automatically produce a “totals” row, so we create it by summarising the data again, but with the data not grouped by hospital. This produces a second data frame of just one row. We can then bind these data frames together to achieve the final table.\n\n# Create core table\n###################\nhosp_summary &lt;- linelist %&gt;% \n  group_by(hospital) %&gt;%                        # Group data by hospital\n  summarise(                                    # Create new summary columns of indicators of interest\n    cases = n(),                                  # Number of rows per hospital-outcome group     \n    ct_value_med = median(ct_blood, na.rm=T))     # median CT value per group\n\nHere is the hosp_summary data frame:\n\n\n\n\n\n\n\nCreate a data frame with the “total” statistics (not grouped by hospital). This will return just one row.\n\n# create totals\n###############\ntotals &lt;- linelist %&gt;% \n  summarise(\n    cases = n(),                               # Number of rows for whole dataset     \n    ct_value_med = median(ct_blood, na.rm=T))  # Median CT for whole dataset\n\nAnd below is that totals data frame. Note how there are only two columns. These columns are also in hosp_summary, but there is one column in hosp_summary that is not in totals (hospital).\n\n\n\n\n\n\n\nNow we can bind the rows together with bind_rows().\n\n# Bind data frames together\ncombined &lt;- bind_rows(hosp_summary, totals)\n\nNow we can view the result. See how in the final row, an empty NA value fills in for the column hospital that was not in hosp_summary. You could “fill-in” this cell with “Total” using replace_na().\n\n\n\n\n\n\n\n\n\n\nThere is a similar dplyr function bind_cols() which you can use to combine two data frames sideways. Note that rows are matched to each other by position (not like a join above) - for example the 12th row in each data frame will be aligned.\nFor an example, we bind several summary tables together. In order to do this, we also demonstrate how to re-arrange the order of rows in one data frame to match the order in another data frame, with match().\nHere we define case_info as a summary data frame of linelist cases, by hospital, with the number of cases and the number of deaths.\n\n# Case information\ncase_info &lt;- linelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    cases = n(),\n    deaths = sum(outcome == \"Death\", na.rm=T)\n  )\n\n\n\n\n\n\n\n\nAnd let’s say that here is a different data frame contact_fu containing information on the percent of exposed contacts investigated and “followed-up”, again by hospital.\n\ncontact_fu &lt;- data.frame(\n  hospital = c(\"St. Mark's Maternity Hospital (SMMH)\", \"Military Hospital\", \"Missing\", \"Central Hospital\", \"Port Hospital\", \"Other\"),\n  investigated = c(\"80%\", \"82%\", NA, \"78%\", \"64%\", \"55%\"),\n  per_fu = c(\"60%\", \"25%\", NA, \"20%\", \"75%\", \"80%\")\n)\n\n\n\n\n\n\n\n\nNote that the hospitals are the same, but are in different orders in each data frame. The easiest solution would be to use a left_join() on the hospital column, but you could also use bind_cols() with one extra step.\n\n\nBecause the row orders are different, a simple bind_cols() command would result in a mis-match of data. To fix this we can use match() from base R to align the rows of a data frame in the same order as in another. We assume for this approach that there are no duplicate values in either data frame.\nWhen we use match(), the syntax is match(TARGET ORDER VECTOR, DATA FRAME COLUMN TO CHANGE), where the first argument is the desired order (either a stand-alone vector, or in this case a column in a data frame), and the second argument is the data frame column in the data frame that will be re-ordered. The output of match() is a vector of numbers representing the correct position ordering. You can read more with ?match.\n\nmatch(case_info$hospital, contact_fu$hospital)\n\n[1] 4 2 3 6 5 1\n\n\nYou can use this numeric vector to re-order the data frame - place it within subset brackets [ ] before the comma. The command below creates a new data frame, defined as the old one in which the rows are ordered in the numeric vector above.\n\ncontact_fu_aligned &lt;- contact_fu[match(case_info$hospital, contact_fu$hospital),]\n\n\n\n\n\n\n\n\nNow we can bind the data frame columns together, with the correct row order. Note that some columns are duplicated and will require cleaning with rename(). Read more aboout bind_rows() here.\n\nbind_cols(case_info, contact_fu)\n\nNew names:\n• `hospital` -&gt; `hospital...1`\n• `hospital` -&gt; `hospital...4`\n\n\n# A tibble: 6 × 6\n  hospital...1                     cases deaths hospital...4 investigated per_fu\n  &lt;chr&gt;                            &lt;int&gt;  &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; \n1 Central Hospital                   454    193 St. Mark's … 80%          60%   \n2 Military Hospital                  896    399 Military Ho… 82%          25%   \n3 Missing                           1469    611 Missing      &lt;NA&gt;         &lt;NA&gt;  \n4 Other                              885    395 Central Hos… 78%          20%   \n5 Port Hospital                     1762    785 Port Hospit… 64%          75%   \n6 St. Mark's Maternity Hospital (…   422    199 Other        55%          80%   \n\n\nA base R alternative to bind_cols is cbind(), which performs the same operation."
  },
  {
    "objectID": "readings/joining.html#dplyr-joins",
    "href": "readings/joining.html#dplyr-joins",
    "title": "Joining data",
    "section": "",
    "text": "The dplyr package offers several different join functions. dplyr is included in the tidyverse package. These join functions are described below, with simple use cases.\nMany thanks to https://github.com/gadenbuie for the informative gifs!\n\n\n\nThe join commands can be run as standalone commands to join two data frames into a new object, or they can be used within a pipe chain (%&gt;%) to merge one data frame into another as it is being cleaned or otherwise modified.\nIn the example below, the function left_join() is used as a standalone command to create the a new joined_data data frame. The inputs are data frames 1 and 2 (df1 and df2). The first data frame listed is the baseline data frame, and the second one listed is joined to it.\nThe third argument by = is where you specify the columns in each data frame that will be used to aligns the rows in the two data frames. If the names of these columns are different, provide them within a c() vector as shown below, where the rows are matched on the basis of common values between the column ID in df1 and the column identifier in df2.\n\n# Join based on common values between column \"ID\" (first data frame) and column \"identifier\" (second data frame)\njoined_data &lt;- left_join(df1, df2, by = c(\"ID\" = \"identifier\"))\n\nIf the by columns in both data frames have the exact same name, you can just provide this one name, within quotes.\n\n# Joint based on common values in column \"ID\" in both data frames\njoined_data &lt;- left_join(df1, df2, by = \"ID\")\n\nIf you are joining the data frames based on common values across multiple fields, list these fields within the c() vector. This example joins rows if the values in three columns in each dataset align exactly.\n\n# join based on same first name, last name, and age\njoined_data &lt;- left_join(df1, df2, by = c(\"name\" = \"firstname\", \"surname\" = \"lastname\", \"Age\" = \"age\"))\n\nThe join commands can also be run within a pipe chain. This will modify the data frame being piped.\nIn the example below, df1 is is passed through the pipes, df2 is joined to it, and df is thus modified and re-defined.\n\ndf1 &lt;- df1 %&gt;%\n  filter(date_onset &lt; as.Date(\"2020-03-05\")) %&gt;% # miscellaneous cleaning \n  left_join(df2, by = c(\"ID\" = \"identifier\"))    # join df2 to df1\n\n\n\n\n\n\n\nWarning\n\n\n\nJoins are case-specific! Therefore it is useful to convert all values to lowercase or uppercase prior to joining.\n\n\n\n\n\n\nA left or right join is commonly used to add information to a data frame - new information is added only to rows that already existed in the baseline data frame. These are common joins in epidemiological work as they are used to add information from one dataset into another.\nIn using these joins, the written order of the data frames in the command is important*.\n\nIn a left join, the first data frame written is the baseline\n\nIn a right join, the second data frame written is the baseline\n\nAll rows of the baseline data frame are kept. Information in the other (secondary) data frame is joined to the baseline data frame only if there is a match via the identifier column(s). In addition:\n\nRows in the secondary data frame that do not match are dropped.\n\nIf there are many baseline rows that match to one row in the secondary data frame (many-to-one), the secondary information is added to each matching baseline row.\n\nIf a baseline row matches to multiple rows in the secondary data frame (one-to-many), all combinations are given, meaning new rows may be added to your returned data frame!\n\nAnimated examples of left and right joins (image source)\n\n\n\n\n\n\n\n\nExample\nBelow is the output of a left_join() of hosp_info (secondary data frame, view here) into linelist_mini (baseline data frame, view here). The original linelist_mini has nrow(linelist_mini) rows. The modified linelist_mini is displayed. Note the following:\n\nTwo new columns, catchment_pop and level have been added on the left side of linelist_mini\n\nAll original rows of the baseline data frame linelist_mini are kept\n\nAny original rows of linelist_mini for “Military Hospital” are duplicated because it matched to two rows in the secondary data frame, so both combinations are returned\n\nThe join identifier column of the secondary dataset (hosp_name) has disappeared because it is redundant with the identifier column in the primary dataset (hospital)\n\nWhen a baseline row did not match to any secondary row (e.g. when hospital is “Other” or “Missing”), NA (blank) fills in the columns from the secondary data frame\n\nRows in the secondary data frame with no match to the baseline data frame (“sisters” and “ignace” hospitals) were dropped\n\n\nlinelist_mini %&gt;% \n  left_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in left_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\nTo answer the above question, ask yourself “which data frame should retain all of its rows?” - use this one as the baseline. A left join keep all the rows in the first data frame written in the command, whereas a right join keeps all the rows in the second data frame.\nThe two commands below achieve the same output - 10 rows of hosp_info joined into a linelist_mini baseline, but they use different joins. The result is that the column order will differ based on whether hosp_info arrives from the right (in the left join) or arrives from the left (in the right join). The order of the rows may also shift accordingly. But both of these consequences can be subsequently addressed, using select() to re-order columns or arrange() to sort rows.\n\n# The two commands below achieve the same data, but with differently ordered rows and columns\nleft_join(linelist_mini, hosp_info, by = c(\"hospital\" = \"hosp_name\"))\nright_join(hosp_info, linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\nHere is the result of hosp_info into linelist_mini via a left join (new columns incoming from the right)\n\n\nWarning in left_join(linelist_mini, hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\nHere is the result of hosp_info into linelist_mini via a right join (new columns incoming from the left)\n\n\nWarning in right_join(hosp_info, linelist_mini, by = c(hosp_name = \"hospital\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 4 of `x` matches multiple rows in `y`.\nℹ Row 5 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\nAlso consider whether your use-case is within a pipe chain (%&gt;%). If the dataset in the pipes is the baseline, you will likely use a left join to add data to it.\n\n\n\n\n\nA full join is the most inclusive of the joins - it returns all rows from both data frames.\nIf there are any rows present in one and not the other (where no match was found), the data frame will include them and become longer. NA missing values are used to fill-in any gaps created. As you join, watch the number of columns and rows carefully to troubleshoot case-sensitivity and exact character matches.\nThe “baseline” data frame is the one written first in the command. Adjustment of this will not impact which records are returned by the join, but it can impact the resulting column order, row order, and which identifier columns are retained.\n\n\n\n\n\nAnimated example of a full join (image source)\nExample\nBelow is the output of a full_join() of hosp_info (originally nrow(hosp_info), view here) into linelist_mini (originally nrow(linelist_mini), view here). Note the following:\n\nAll baseline rows are kept (linelist_mini)\n\nRows in the secondary that do not match to the baseline are kept (“ignace” and “sisters”), with values in the corresponding baseline columns case_id and onset filled in with missing values\n\nLikewise, rows in the baseline data frame that do not match to the secondary (“Other” and “Missing”) are kept, with secondary columns catchment_pop and level filled-in with missing values\n\nIn the case of one-to-many or many-to-one matches (e.g. rows for “Military Hospital”), all possible combinations are returned (lengthening the final data frame)\n\nOnly the identifier column from the baseline is kept (hospital)\n\n\nlinelist_mini %&gt;% \n  full_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in full_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\n\n\nAn inner join is the most restrictive of the joins - it returns only rows with matches across both data frames.\nThis means that the number of rows in the baseline data frame may actually reduce. Adjustment of which data frame is the “baseline” (written first in the function) will not impact which rows are returned, but it will impact the column order, row order, and which identifier columns are retained.\n\n\n\n\n\nAnimated example of an inner join (image source)\nExample\nBelow is the output of an inner_join() of linelist_mini (baseline) with hosp_info (secondary). Note the following:\n\nBaseline rows with no match to the secondary data are removed (rows where hospital is “Missing” or “Other”)\n\nLikewise, rows from the secondary data frame that had no match in the baseline are removed (rows where hosp_name is “sisters” or “ignace”)\n\nOnly the identifier column from the baseline is kept (hospital)\n\n\nlinelist_mini %&gt;% \n  inner_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in inner_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\n\n\n\n\nA semi join is a “filtering join” which uses another dataset not to add rows or columns, but to perform filtering.\nA semi-join keeps all observations in the baseline data frame that have a match in the secondary data frame (but does not add new columns nor duplicate any rows for multiple matches). Read more about these “filtering” joins here.\n\n\n\n\n\nAnimated example of a semi join (image source)\nAs an example, the below code returns rows from the hosp_info data frame that have matches in linelist_mini based on hospital name.\n\nhosp_info %&gt;% \n  semi_join(linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\n                             hosp_name catchment_pop     level\n1                    Military Hospital         40500 Secondary\n2                    Military Hospital         10000   Primary\n3                        Port Hospital         50280 Secondary\n4 St. Mark's Maternity Hospital (SMMH)         12000 Secondary\n\n\n\n\n\n\nThe anti join is another “filtering join” that returns rows in the baseline data frame that do not have a match in the secondary data frame.\nRead more about filtering joins here.\nCommon scenarios for an anti-join include identifying records not present in another data frame, troubleshooting spelling in a join (reviewing records that should have matched), and examining records that were excluded after another join.\nAs with right_join() and left_join(), the baseline data frame (listed first) is important. The returned rows are from the baseline data frame only. Notice in the gif below that row in the secondary data frame (purple row 4) is not returned even though it does not match with the baseline.\n\n\n\n\n\nAnimated example of an anti join (image source)\n\n\nFor a simple example, let’s find the hosp_info hospitals that do not have any cases present in linelist_mini. We list hosp_info first, as the baseline data frame. The hospitals which are not present in linelist_mini are returned.\n\nhosp_info %&gt;% \n  anti_join(linelist_mini, by = c(\"hosp_name\" = \"hospital\"))\n\n\n\n\n\n\n\n\n\n\n\nFor another example, let us say we ran an inner_join() between linelist_mini and hosp_info. This returns only a subset of the original linelist_mini records, as some are not present in hosp_info.\n\nlinelist_mini %&gt;% \n  inner_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\nWarning in inner_join(., hosp_info, by = c(hospital = \"hosp_name\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 5 of `x` matches multiple rows in `y`.\nℹ Row 4 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\n\n\n\n\nTo review the linelist_mini records that were excluded during the inner join, we can run an anti-join with the same settings (linelist_mini as the baseline).\n\nlinelist_mini %&gt;% \n  anti_join(hosp_info, by = c(\"hospital\" = \"hosp_name\"))\n\n\n\n\n\n\n\n\nTo see the hosp_info records that were excluded in the inner join, we could also run an anti-join with hosp_info as the baseline data frame."
  },
  {
    "objectID": "readings/joining.html#binding-and-aligning",
    "href": "readings/joining.html#binding-and-aligning",
    "title": "Joining data",
    "section": "",
    "text": "Another method of combining two data frames is “binding” them together. You can also think of this as “appending” or “adding” rows or columns.\nThis section will also discuss how to “align” the order of rows of one data frame to the order in another data frame. This topic is discussed below in the section on Binding columns.\n\n\nTo bind rows of one data frame to the bottom of another data frame, use bind_rows() from dplyr. It is very inclusive, so any column present in either data frame will be included in the output. A few notes:\n\nUnlike the base R version row.bind(), dplyr’s bind_rows() does not require that the order of columns be the same in both data frames. As long as the column names are spelled identically, it will align them correctly.\n\nYou can optionally specify the argument .id =. Provide a character column name. This will produce a new column that serves to identify which data frame each row originally came from.\n\nYou can use bind_rows() on a list of similarly-structured data frames to combine them into one data frame.\n\nOne common example of row binding is to bind a “total” row onto a descriptive table made with dplyr’s summarise() function. Below we create a table of case counts and median CT values by hospital with a total row.\nThe function summarise() is used on data grouped by hospital to return a summary data frame by hospital. But the function summarise() does not automatically produce a “totals” row, so we create it by summarising the data again, but with the data not grouped by hospital. This produces a second data frame of just one row. We can then bind these data frames together to achieve the final table.\n\n# Create core table\n###################\nhosp_summary &lt;- linelist %&gt;% \n  group_by(hospital) %&gt;%                        # Group data by hospital\n  summarise(                                    # Create new summary columns of indicators of interest\n    cases = n(),                                  # Number of rows per hospital-outcome group     \n    ct_value_med = median(ct_blood, na.rm=T))     # median CT value per group\n\nHere is the hosp_summary data frame:\n\n\n\n\n\n\n\nCreate a data frame with the “total” statistics (not grouped by hospital). This will return just one row.\n\n# create totals\n###############\ntotals &lt;- linelist %&gt;% \n  summarise(\n    cases = n(),                               # Number of rows for whole dataset     \n    ct_value_med = median(ct_blood, na.rm=T))  # Median CT for whole dataset\n\nAnd below is that totals data frame. Note how there are only two columns. These columns are also in hosp_summary, but there is one column in hosp_summary that is not in totals (hospital).\n\n\n\n\n\n\n\nNow we can bind the rows together with bind_rows().\n\n# Bind data frames together\ncombined &lt;- bind_rows(hosp_summary, totals)\n\nNow we can view the result. See how in the final row, an empty NA value fills in for the column hospital that was not in hosp_summary. You could “fill-in” this cell with “Total” using replace_na().\n\n\n\n\n\n\n\n\n\n\nThere is a similar dplyr function bind_cols() which you can use to combine two data frames sideways. Note that rows are matched to each other by position (not like a join above) - for example the 12th row in each data frame will be aligned.\nFor an example, we bind several summary tables together. In order to do this, we also demonstrate how to re-arrange the order of rows in one data frame to match the order in another data frame, with match().\nHere we define case_info as a summary data frame of linelist cases, by hospital, with the number of cases and the number of deaths.\n\n# Case information\ncase_info &lt;- linelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    cases = n(),\n    deaths = sum(outcome == \"Death\", na.rm=T)\n  )\n\n\n\n\n\n\n\n\nAnd let’s say that here is a different data frame contact_fu containing information on the percent of exposed contacts investigated and “followed-up”, again by hospital.\n\ncontact_fu &lt;- data.frame(\n  hospital = c(\"St. Mark's Maternity Hospital (SMMH)\", \"Military Hospital\", \"Missing\", \"Central Hospital\", \"Port Hospital\", \"Other\"),\n  investigated = c(\"80%\", \"82%\", NA, \"78%\", \"64%\", \"55%\"),\n  per_fu = c(\"60%\", \"25%\", NA, \"20%\", \"75%\", \"80%\")\n)\n\n\n\n\n\n\n\n\nNote that the hospitals are the same, but are in different orders in each data frame. The easiest solution would be to use a left_join() on the hospital column, but you could also use bind_cols() with one extra step.\n\n\nBecause the row orders are different, a simple bind_cols() command would result in a mis-match of data. To fix this we can use match() from base R to align the rows of a data frame in the same order as in another. We assume for this approach that there are no duplicate values in either data frame.\nWhen we use match(), the syntax is match(TARGET ORDER VECTOR, DATA FRAME COLUMN TO CHANGE), where the first argument is the desired order (either a stand-alone vector, or in this case a column in a data frame), and the second argument is the data frame column in the data frame that will be re-ordered. The output of match() is a vector of numbers representing the correct position ordering. You can read more with ?match.\n\nmatch(case_info$hospital, contact_fu$hospital)\n\n[1] 4 2 3 6 5 1\n\n\nYou can use this numeric vector to re-order the data frame - place it within subset brackets [ ] before the comma. The command below creates a new data frame, defined as the old one in which the rows are ordered in the numeric vector above.\n\ncontact_fu_aligned &lt;- contact_fu[match(case_info$hospital, contact_fu$hospital),]\n\n\n\n\n\n\n\n\nNow we can bind the data frame columns together, with the correct row order. Note that some columns are duplicated and will require cleaning with rename(). Read more aboout bind_rows() here.\n\nbind_cols(case_info, contact_fu)\n\nNew names:\n• `hospital` -&gt; `hospital...1`\n• `hospital` -&gt; `hospital...4`\n\n\n# A tibble: 6 × 6\n  hospital...1                     cases deaths hospital...4 investigated per_fu\n  &lt;chr&gt;                            &lt;int&gt;  &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt; \n1 Central Hospital                   454    193 St. Mark's … 80%          60%   \n2 Military Hospital                  896    399 Military Ho… 82%          25%   \n3 Missing                           1469    611 Missing      &lt;NA&gt;         &lt;NA&gt;  \n4 Other                              885    395 Central Hos… 78%          20%   \n5 Port Hospital                     1762    785 Port Hospit… 64%          75%   \n6 St. Mark's Maternity Hospital (…   422    199 Other        55%          80%   \n\n\nA base R alternative to bind_cols is cbind(), which performs the same operation."
  },
  {
    "objectID": "readings/ggplot-basics.html",
    "href": "readings/ggplot-basics.html",
    "title": "Data visualization",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of these materials from chapters 30 and 31.\n\n\n\n\n\n\n\n\n\n\n\nggplot2 is the most popular data visualisation R package. Its ggplot() function is at the core of this package, and this whole approach is colloquially known as “ggplot” with the resulting figures sometimes affectionately called “ggplots”. The “gg” in these names reflects the “grammar of graphics” used to construct the figures. ggplot2 benefits from a wide variety of supplementary R packages that further enhance its functionality.\nThe data visualization with ggplot cheatsheet from the RStudio website is a great reference to have on-hand when creating pltos. If you want inspiration for ways to creatively visualise your data, we suggest reviewing websites like the R graph gallery and Data-to-viz.\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file)..\nThe first 50 rows of the linelist are displayed below. We will focus on the continuous variables age, wt_kg (weight in kilos), ct_blood (CT values), and days_onset_hosp (difference between onset date and hospitalisation).\n\n\n\n\n\n\n\n\n\n\nSome simple ways we can prepare our data to make it better for plotting can include making the contents of the data better for display - which does not necessarily equate to better for data manipulation. For example:\n\nReplace NA values in a character column with the character string “Unknown”\n\nConsider converting column to class factor so their values have prescribed ordinal levels\n\nClean some columns so that their “data friendly” values with underscores etc are changed to normal text or title case.\n\nHere are some examples of this in action:\n\n# make display version of columns with more friendly names\nlinelist &lt;- linelist %&gt;%\n  mutate(\n    gender_disp = case_when(gender == \"m\" ~ \"Male\",        # m to Male \n                            gender == \"f\" ~ \"Female\",      # f to Female,\n                            is.na(gender) ~ \"Unknown\"),    # NA to Unknown\n    \n    outcome_disp = replace_na(outcome, \"Unknown\")          # replace NA outcome with \"unknown\"\n  )\n\n\n\n\nAs a matter of data structure, for ggplot2 we often also want to pivot our data into longer formats. We will learn more about pivoting later; for now it’s enough to be aware that these two data formats exist.\n\n\n\n\n\n\n\n\n\nFor example, say that we want to plot data that are in a “wide” format, such as for each case in the linelist and their symptoms. Below we create a mini-linelist called symptoms_data that contains only the case_id and symptoms columns.\n\nsymptoms_data &lt;- linelist %&gt;% \n  select(c(case_id, fever, chills, cough, aches, vomit))\n\nHere is how the first 50 rows of this mini-linelist look - see how they are formatted “wide” with each symptom as a column:\n\n\n   case_id fever chills cough aches vomit\n1   5fe599    no     no   yes    no   yes\n2   8689b7  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n3   11f8ea  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n4   b8812a    no     no    no    no    no\n5   893f25    no     no   yes    no   yes\n6   be99c8    no     no   yes    no   yes\n7   07e3e8  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n8   369449    no     no   yes    no   yes\n9   f393b4    no     no   yes    no   yes\n10  1389ca    no     no   yes    no    no\n11  2978ac    no     no   yes    no   yes\n12  57a565    no     no   yes    no    no\n13  fc15ef    no     no   yes    no    no\n14  2eaa9a    no     no   yes    no    no\n15  bbfa93    no     no   yes    no   yes\n16  c97dd9    no     no   yes   yes    no\n17  f50e8a    no    yes   yes    no    no\n18  3a7673    no     no   yes    no    no\n19  7f5a01  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n20  ddddee    no     no   yes    no    no\n21  99e8fa    no     no   yes    no   yes\n22  567136    no     no   yes    no    no\n23  9371a9    no    yes   yes    no    no\n24  bc2adf    no     no   yes    no    no\n25  403057  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n26  8bd1e8    no     no   yes    no    no\n27  f327be    no     no   yes    no    no\n28  42e1a9    no    yes   yes    no    no\n29  90e5fe  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n30  959170  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n31  8ebf6e    no     no   yes    no    no\n32  e56412    no     no   yes    no   yes\n33  6d788e  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n34  a47529    no     no   yes    no   yes\n35  67be4e    no     no   yes    no   yes\n36  da8ecb  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n37  148f18  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n38  2cb9a5    no     no   yes   yes   yes\n39  f5c142    no     no   yes   yes   yes\n40  70a9fe  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n41  3ad520    no     no   yes    no   yes\n42  062638    no     no   yes    no   yes\n43  c76676  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n44  baacc1    no     no   yes    no   yes\n45  497372    no    yes   yes    no   yes\n46  23e499    no     no   yes    no    no\n47  38cc4a    no     no   yes    no   yes\n48  3789ee    no     no   yes    no   yes\n49  c71dcd    no     no    no    no   yes\n50  6b70f0  &lt;NA&gt;   &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;\n\n\nIf we wanted to plot the number of cases with specific symptoms, we are limited by the fact that each symptom is a specific column. However, we can pivot the symptoms columns to a longer format like this:\n\nsymptoms_data_long &lt;- symptoms_data %&gt;%    # begin with \"mini\" linelist called symptoms_data\n  \n  pivot_longer(\n    cols = -case_id,                       # pivot all columns except case_id (all the symptoms columns)\n    names_to = \"symptom_name\",             # assign name for new column that holds the symptoms\n    values_to = \"symptom_is_present\") %&gt;%  # assign name for new column that holds the values (yes/no)\n  \n  mutate(symptom_is_present = replace_na(symptom_is_present, \"unknown\")) # convert NA to \"unknown\"\n\nHere are the first 50 rows. Note that case has 5 rows - one for each possible symptom. The new columns symptom_name and symptom_is_present are the result of the pivot. Note that this format may not be very useful for other operations, but is useful for plotting.\n\n\n# A tibble: 50 × 3\n   case_id symptom_name symptom_is_present\n   &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;             \n 1 5fe599  fever        no                \n 2 5fe599  chills       no                \n 3 5fe599  cough        yes               \n 4 5fe599  aches        no                \n 5 5fe599  vomit        yes               \n 6 8689b7  fever        unknown           \n 7 8689b7  chills       unknown           \n 8 8689b7  cough        unknown           \n 9 8689b7  aches        unknown           \n10 8689b7  vomit        unknown           \n# ℹ 40 more rows\n\n\n\n\n\n\n\n\n\n“Grammar of graphics” - ggplot2\nPlotting with ggplot2 is based on “adding” plot layers and design elements on top of one another, with each command added to the previous ones with a plus symbol (+). The result is a multi-layer plot object that can be saved, modified, printed, exported, etc.\n\nThe idea behind the Grammar of Graphics it is that you can build every graph from the same 3 components: (1) a data set, (2) a coordinate system, and (3) geoms — i.e. visual marks that represent data points [source]\n\nggplot objects can be highly complex, but the basic order of layers will usually look like this:\n\nBegin with the baseline ggplot() command - this “opens” the ggplot and allow subsequent functions to be added with +. Typically the dataset is also specified in this command\n\nAdd “geom” layers - these functions visualize the data as geometries (shapes), e.g. as a bar graph, line plot, scatter plot, histogram (or a combination!). These functions all start with geom_ as a prefix.\n\nAdd design elements to the plot such as axis labels, title, fonts, sizes, color schemes, legends, or axes rotation\n\nIn code this amounts to the basic template:\n\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +  &lt;GEOM_FUNCTION&gt;()\n\nWe can further expand this template to include aspects of the visualization such as theme and labels:\n\n# plot data from my_data columns as red points\nggplot(data = my_data)+                   # use the dataset \"my_data\"\n  geom_point(                             # add a layer of points (dots)\n    mapping = aes(x = col1, y = col2),    # \"map\" data column to axes\n    color = \"red\")+                       # other specification for the geom\n  labs()+                                 # here you add titles, axes labels, etc.\n  theme()                                 # here you adjust color, font, size etc of non-data plot elements (axes, title, etc.) \n\nWe will explain each component in the sections below.\n\n\n\nThe opening command of any ggplot2 plot is ggplot(). This command simply creates a blank canvas upon which to add layers. It “opens” the way for further layers to be added with a + symbol.\nTypically, the command ggplot() includes the data = argument for the plot. This sets the default dataset to be used for subsequent layers of the plot.\nThis command will end with a + after its closing parentheses. This leaves the command “open”. The ggplot will only execute/appear when the full command includes a final layer without a + at the end.\n\n# This will create plot that is a blank canvas\nggplot(data = linelist)\n\n\n\n\nA blank canvas is certainly not sufficient - we need to create geometries (shapes) from our data (e.g. bar plots, histograms, scatter plots, box plots).\nThis is done by adding layers “geoms” to the initial ggplot() command. There are many ggplot2 functions that create “geoms”. Each of these functions begins with “geom_”, so we will refer to them generically as geom_XXXX(). There are over 40 geoms in ggplot2 and many others created by fans. View them at the ggplot2 gallery. Some common geoms are listed below:\n\nHistograms - geom_histogram()\n\nBar charts - geom_bar() or geom_col() (see “Bar plot” section)\n\nBox plots - geom_boxplot()\n\nPoints (e.g. scatter plots) - geom_point()\n\nLine graphs - geom_line() or geom_path()\n\nTrend lines - geom_smooth()\n\nIn one plot you can display one or multiple geoms. Each is added to previous ggplot2 commands with a +, and they are plotted sequentially such that later geoms are plotted on top of previous ones.\n\n\n\nMost geom functions must be told what to use to create their shapes - so you must tell them how they should map (assign) columns in your data to components of the plot like the axes, shape colors, shape sizes, etc. For most geoms, the essential components that must be mapped to columns in the data are the x-axis, and (if necessary) the y-axis.\nThis “mapping” occurs with the mapping = argument. The mappings you provide to mapping must be wrapped in the aes() function, so you would write something like mapping = aes(x = col1, y = col2), as shown below.\nBelow, in the ggplot() command the data are set as the case linelist. In the mapping = aes() argument the column age is mapped to the x-axis, and the column wt_kg is mapped to the y-axis.\nAfter a +, the plotting commands continue. A shape is created with the “geom” function geom_point(). This geom inherits the mappings from the ggplot() command above - it knows the axis-column assignments and proceeds to visualize those relationships as points on the canvas.\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+\n  geom_point()\n\n\n\n\nAs another example, the following commands utilize the same data, a slightly different mapping, and a different geom. The geom_histogram() function only requires a column mapped to the x-axis, as the counts y-axis is generated automatically.\n\nggplot(data = linelist, mapping = aes(x = age))+\n  geom_histogram()\n\n\n\n\n\n\nIn ggplot terminology a plot “aesthetic” has a specific meaning. It refers to a visual property of plotted data. Note that “aesthetic” here refers to the data being plotted in geoms/shapes - not the surrounding display such as titles, axis labels, background color, that you might associate with the word “aesthetics” in common English. In ggplot those details are called “themes” and are adjusted within a theme() command (see this section).\nTherefore, plot object aesthetics can be colors, sizes, transparencies, placement, etc. of the plotted data. Not all geoms will have the same aesthetic options, but many can be used by most geoms. Here are some examples:\n\nshape = Display a point with geom_point() as a dot, star, triangle, or square…\n\nfill = The interior color (e.g. of a bar or boxplot)\n\ncolor = The exterior line of a bar, boxplot, etc., or the point color if using geom_point()\n\nsize = Size (e.g. line thickness, point size)\n\nalpha = Transparency (1 = opaque, 0 = invisible)\n\nbinwidth = Width of histogram bins\n\nwidth = Width of “bar plot” columns\n\nlinetype = Line type (e.g. solid, dashed, dotted)\n\nThese plot object aesthetics can be assigned values in two ways:\n\nAssigned a static value (e.g. color = \"blue\") to apply across all plotted observations\n\nAssigned to a column of the data (e.g. color = hospital) such that display of each observation depends on its value in that column\n\n\n\n\nIf you want the plot object aesthetic to be static, that is - to be the same for every observation in the data, you write its assignment within the geom but outside of any mapping = aes() statement. These assignments could look like size = 1 or color = \"blue\". Here are two examples:\n\nIn the first example, the mapping = aes() is in the ggplot() command and the axes are mapped to age and weight columns in the data. The plot aesthetics color =, size =, and alpha = (transparency) are assigned to static values. For clarity, this is done in the geom_point() function, as you may add other geoms afterward that would take different values for their plot aesthetics.\n\nIn the second example, the histogram requires only the x-axis mapped to a column. The histogram binwidth =, color =, fill = (internal color), and alpha = are again set within the geom to static values.\n\n\n# scatterplot\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  # set data and axes mapping\n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)         # set static point aesthetics\n\n# histogram\nggplot(data = linelist, mapping = aes(x = age))+       # set data and axes\n  geom_histogram(              # display histogram\n    binwidth = 7,                # width of bins\n    color = \"red\",               # bin line color\n    fill = \"blue\",               # bin interior color\n    alpha = 0.1)                 # bin transparency\n\n\n\n\n\n\n\n\n\n\nThe alternative is to scale the plot object aesthetic by the values in a column. In this approach, the display of this aesthetic will depend on that observation’s value in that column of the data. If the column values are continuous, the display scale (legend) for that aesthetic will be continuous. If the column values are discrete, the legend will display each value and the plotted data will appear as distinctly “grouped” (read more in the grouping section of this page).\nTo achieve this, you map that plot aesthetic to a column name (not in quotes). This must be done within a mapping = aes() function (note: there are several places in the code you can make these mapping assignments, as discussed below).\nTwo examples are below.\n\nIn the first example, the color = aesthetic (of each point) is mapped to the column age - and a scale has appeared in a legend! For now just note that the scale exists - we will show how to modify it in later sections.\n\nIn the second example two new plot aesthetics are also mapped to columns (color = and size =), while the plot aesthetics shape = and alpha = are mapped to static values outside of any mapping = aes() function.\n\n\n# scatterplot\nggplot(data = linelist,   # set data\n       mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age)\n       )+     # map color to age\n  geom_point()         # display data as points \n\n# scatterplot\nggplot(data = linelist,   # set data\n       mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age,       # map color to age\n         size = age))+      # map size to age\n  geom_point(             # display data as points\n    shape = \"diamond\",      # points display as diamonds\n    alpha = 0.3)            # point transparency at 30%\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAxes assignments are always assigned to columns in the data (not to static values), and this is always done within mapping = aes().\n\n\nIt becomes important to keep track of your plot layers and aesthetics when making more complex plots - for example plots with multiple geoms. In the example below, the size = aesthetic is assigned twice - once for geom_point() and once for geom_smooth() - both times as a static value.\n\nggplot(data = linelist,\n       mapping = aes(           # map aesthetics to columns\n         x = age,\n         y = wt_kg,\n         color = age_years)\n       ) + \n  geom_point(                   # add points for each row of data\n    size = 1,\n    alpha = 0.5) +  \n  geom_smooth(                  # add a trend line \n    method = \"lm\",              # with linear method\n    size = 2)                   # size (width of line) of 2\n\n\n\n\n\n\n\nAesthetic mapping within mapping = aes() can be written in several places in your plotting commands and can even be written more than once. This can be written in the top ggplot() command, and/or for each individual geom beneath. The nuances include:\n\nMapping assignments made in the top ggplot() command will be inherited as defaults across any geom below, like how x = and y = are inherited\nMapping assignments made within one geom apply only to that geom\n\nLikewise, data = specified in the top ggplot() will apply by default to any geom below, but you could also specify data for each geom (but this is more difficult).\nThus, each of the following commands will create the same plot:\n\n# These commands will produce the exact same plot\nggplot(data = linelist, mapping = aes(x = age))+\n  geom_histogram()\n\nggplot(data = linelist)+\n  geom_histogram(mapping = aes(x = age))\n\nggplot()+\n  geom_histogram(data = linelist, mapping = aes(x = age))\n\n\n\n\nYou can easily group the data and “plot by group”. In fact, you have already done this!\nAssign the “grouping” column to the appropriate plot aesthetic, within a mapping = aes(). Above, we demonstrated this using continuous values when we assigned point size = to the column age. However this works the same way for discrete/categorical columns.\nFor example, if you want points to be displayed by gender, you would set mapping = aes(color = gender). A legend automatically appears. This assignment can be made within the mapping = aes() in the top ggplot() command (and be inherited by the geom), or it could be set in a separate mapping = aes() within the geom. Both approaches are shown below:\n\nggplot(data = linelist,\n       mapping = aes(x = age, y = wt_kg, color = gender))+\n  geom_point(alpha = 0.5)\n\n\n\n\n\n# This alternative code produces the same plot\nggplot(data = linelist,\n       mapping = aes(x = age, y = wt_kg))+\n  geom_point(\n    mapping = aes(color = gender),\n    alpha = 0.5)\n\nNote that depending on the geom, you will need to use different arguments to group the data. For geom_point() you will most likely use color =, shape = or size =. Whereas for geom_bar() you are more likely to use fill =. This just depends on the geom and what plot aesthetic you want to reflect the groupings.\nFor your information - the most basic way of grouping the data is by using only the group = argument within mapping = aes(). However, this by itself will not change the colors, fill, or shapes. Nor will it create a legend. Yet the data are grouped, so statistical displays may be affected.\n\n\n\n\nFacets, or “small-multiples”, are used to split one plot into a multi-panel figure, with one panel (“facet”) per group of data. The same type of plot is created multiple times, each one using a sub-group of the same dataset.\nFaceting is a functionality that comes with ggplot2, so the legends and axes of the facet “panels” are automatically aligned. We would need to use other packages to combine completely different plots (cowplot and patchwork) into one figure.\nFaceting is done with one of the following ggplot2 functions:\n\nfacet_wrap() To show a different panel for each level of a single variable. One example of this could be showing a different epidemic curve for each hospital in a region. Facets are ordered alphabetically, unless the variable is a factor with other ordering defined.\n\n\n\nYou can invoke certain options to determine the layout of the facets, e.g. nrow = 1 or ncol = 1 to control the number of rows or columns that the faceted plots are arranged within.\n\n\nfacet_grid() This is used when you want to bring a second variable into the faceting arrangement. Here each panel of a grid shows the intersection between values in two columns. For example, epidemic curves for each hospital-age group combination with hospitals along the top (columns) and age groups along the sides (rows).\n\n\n\nnrow and ncol are not relevant, as the subgroups are presented in a grid\n\nEach of these functions accept a formula syntax to specify the column(s) for faceting. Both accept up to two columns, one on each side of a tilde ~.\n\nFor facet_wrap() most often you will write only one column preceded by a tilde ~ like facet_wrap(~hospital). However you can write two columns facet_wrap(outcome ~ hospital) - each unique combination will display in a separate panel, but they will not be arranged in a grid. The headings will show combined terms and these won’t be specific logic to the columns vs. rows. If you are providing only one faceting variable, a period . is used as a placeholder on the other side of the formula - see the code examples.\nFor facet_grid() you can also specify one or two columns to the formula (grid rows ~ columns). If you only want to specify one, you can place a period . on the other side of the tilde like facet_grid(. ~ hospital) or facet_grid(hospital ~ .).\n\nFacets can quickly contain an overwhelming amount of information - its good to ensure you don’t have too many levels of each variable that you choose to facet by. Here are some quick examples with the malaria dataset which consists of daily case counts of malaria for facilities, by age group.\nBelow we import and do some quick modifications for simplicity:\n\n# These data are daily counts of malaria cases, by facility-day\nmalaria_data &lt;- import(\"data/malaria_facility_count_data.rds\") %&gt;%  # import\n  select(-submitted_date, -Province, -newid)                                 # remove unneeded columns\n\nThe first 50 rows of the malaria data are below. Note there is a column malaria_tot, but also columns for counts by age group (these will be used in the second, facet_grid() example).\n\n\n# A tibble: 50 × 7\n   location_name data_date  District `malaria_rdt_0-4` `malaria_rdt_5-14`\n   &lt;chr&gt;         &lt;date&gt;     &lt;chr&gt;                &lt;int&gt;              &lt;int&gt;\n 1 Facility 1    2020-08-11 Spring                  11                 12\n 2 Facility 2    2020-08-11 Bolo                    11                 10\n 3 Facility 3    2020-08-11 Dingo                    8                  5\n 4 Facility 4    2020-08-11 Bolo                    16                 16\n 5 Facility 5    2020-08-11 Bolo                     9                  2\n 6 Facility 6    2020-08-11 Dingo                    3                  1\n 7 Facility 6    2020-08-10 Dingo                    4                  0\n 8 Facility 5    2020-08-10 Bolo                    15                 14\n 9 Facility 5    2020-08-09 Bolo                    11                 11\n10 Facility 5    2020-08-08 Bolo                    19                 15\n# ℹ 40 more rows\n# ℹ 2 more variables: malaria_rdt_15 &lt;int&gt;, malaria_tot &lt;int&gt;\n\n\n\n\nFor the moment, let’s focus on the columns malaria_tot and District. Ignore the age-specific count columns for now. We will plot epidemic curves with geom_col(), which produces a column for each day at the specified y-axis height given in column malaria_tot (the data are already daily counts, so we use geom_col() - see the “Bar plot” section below).\nWhen we add the command facet_wrap(), we specify a tilde and then the column to facet on (District in this case). You can place another column on the left side of the tilde, - this will create one facet for each combination - but we recommend you do this with facet_grid() instead. In this use case, one facet is created for each unique value of District.\n\n# A plot with facets by district\nggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1, fill = \"darkred\") +       # plot the count data as columns\n  theme_minimal()+                              # simplify the background panels\n  labs(                                         # add plot labels, title, etc.\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district\") +\n  facet_wrap(~District)                       # the facets are created\n\n\n\n\n\n\n\nWe can use a facet_grid() approach to cross two variables. Let’s say we want to cross District and age. Well, we need to do some data transformations on the age columns to get these data into ggplot-preferred “long” format. The age groups all have their own columns - we want them in a single column called age_group and another called num_cases.\n\nmalaria_age &lt;- malaria_data %&gt;%\n  select(-malaria_tot) %&gt;% \n  pivot_longer(\n    cols = c(starts_with(\"malaria_rdt_\")),  # choose columns to pivot longer\n    names_to = \"age_group\",      # column names become age group\n    values_to = \"num_cases\"      # values to a single column (num_cases)\n  ) %&gt;%\n  mutate(\n    age_group = str_replace(age_group, \"malaria_rdt_\", \"\"),\n    age_group = forcats::fct_relevel(age_group, \"5-14\", after = 1))\n\nNow the first 50 rows of data look like this:\n\n\n# A tibble: 50 × 5\n   location_name data_date  District age_group num_cases\n   &lt;chr&gt;         &lt;date&gt;     &lt;chr&gt;    &lt;fct&gt;         &lt;int&gt;\n 1 Facility 1    2020-08-11 Spring   0-4              11\n 2 Facility 1    2020-08-11 Spring   5-14             12\n 3 Facility 1    2020-08-11 Spring   15               23\n 4 Facility 2    2020-08-11 Bolo     0-4              11\n 5 Facility 2    2020-08-11 Bolo     5-14             10\n 6 Facility 2    2020-08-11 Bolo     15                5\n 7 Facility 3    2020-08-11 Dingo    0-4               8\n 8 Facility 3    2020-08-11 Dingo    5-14              5\n 9 Facility 3    2020-08-11 Dingo    15                5\n10 Facility 4    2020-08-11 Bolo     0-4              16\n# ℹ 40 more rows\n\n\nWhen you pass the two variables to facet_grid(), easiest is to use formula notation (e.g. x ~ y) where x is rows and y is columns. Here is the plot, using facet_grid() to show the plots for each combination of the columns age_group and District.\n\nggplot(malaria_age, aes(x = data_date, y = num_cases)) +\n  geom_col(fill = \"darkred\", width = 1) +\n  theme_minimal()+\n  labs(\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district and age group\"\n  ) +\n  facet_grid(District ~ age_group)\n\n\n\n\n\n\n\nThe axes scales displayed when faceting are by default the same (fixed) across all the facets. This is helpful for cross-comparison, but not always appropriate.\nWhen using facet_wrap() or facet_grid(), we can add scales = \"free_y\" to “free” or release the y-axes of the panels to scale appropriately to their data subset. This is particularly useful if the actual counts are small for one of the subcategories and trends are otherwise hard to see. Instead of “free_y” we can also write “free_x” to do the same for the x-axis (e.g. for dates) or “free” for both axes. Note that in facet_grid, the y scales will be the same for facets in the same row, and the x scales will be the same for facets in the same column.\nWhen using facet_grid only, we can add space = \"free_y\" or space = \"free_x\" so that the actual height or width of the facet is weighted to the values of the figure within. This only works if scales = \"free\" (y or x) is already applied.\n\n# Free y-axis\nggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1, fill = \"darkred\") +       # plot the count data as columns\n  theme_minimal()+                              # simplify the background panels\n  labs(                                         # add plot labels, title, etc.\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district - 'free' x and y axes\") +\n  facet_wrap(~District, scales = \"free\")        # the facets are created\n\n\n\n\n\n\n\n\nExporting ggplots is made easy with the ggsave() function from ggplot2. It can work in two ways, either:\n\nSpecify the name of the plot object, then the file path and name with extension\n\nFor example: ggsave(my_plot, \"plots/my_plot.png\"))\n\n\nRun the command with only a file path, to save the last plot that was printed\n\nFor example: ggsave(\"plots/my_plot.png\"))\n\n\nYou can export as png, pdf, jpeg, tiff, bmp, svg, or several other file types, by specifying the file extension in the file path.\nYou can also specify the arguments width =, height =, and units = (either “in”, “cm”, or “mm”). You can also specify dpi = with a number for plot resolution (e.g. 300). See the function details by entering ?ggsave or reading the documentation online.\n\n\n\nSurely you will want to add or adjust the plot’s labels. These are most easily done within the labs() function which is added to the plot with + just as the geoms were.\nWithin labs() you can provide character strings to these arguements:\n\nx = and y = The x-axis and y-axis title (labels)\n\ntitle = The main plot title\n\nsubtitle = The subtitle of the plot, in smaller text below the title\n\ncaption = The caption of the plot, in bottom-right by default\n\nHere is a plot we made earlier, but with nicer labels:\n\nage_by_wt &lt;- ggplot(\n  data = linelist,   # set data\n  mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age))+     # map color to age\n  geom_point()+           # display data as points\n  labs(\n    title = \"Age and weight distribution\",\n    subtitle = \"Fictional Ebola outbreak, 2014\",\n    x = \"Age in years\",\n    y = \"Weight in kilos\",\n    color = \"Age\",\n    caption = stringr::str_glue(\"Data as of {max(linelist$date_hospitalisation, na.rm=T)}\"))\n\nage_by_wt\n\n\n\n\nNote how in the caption assignment we used str_glue() from the stringr package to implant dynamic R code within the string text. The caption will show the “Data as of:” date that reflects the maximum hospitalization date in the linelist.\n\n\n\nThroughout this page, you have already seen many examples of plotting continuous data. Here we briefly consolidate these and present a few variations.\nVisualisations covered here include:\n\nPlots for one continuous variable:\n\nHistogram, a classic graph to present the distribution of a continuous variable.\nBox plot (also called box and whisker), to show the 25th, 50th, and 75th percentiles, tail ends of the distribution, and outliers (important limitations).\n\nJitter plot, to show all values as points that are ‘jittered’ so they can (mostly) all be seen, even where two have the same value.\n\nViolin plot, show the distribution of a continuous variable based on the symmetrical width of the ‘violin’.\nSina plot, are a combination of jitter and violin plots, where individual points are shown but in the symmetrical shape of the distribution (via ggforce package).\n\n\nScatter plot for two continuous variables.\n\nHeat plots for three continuous variables\n\n\n\nHistograms may look like bar charts, but are distinct because they measure the distribution of a continuous variable. There are no spaces between the “bars”, and only one column is provided to geom_histogram().\nBelow is code for generating histograms, which group continuous data into ranges and display in adjacent bars of varying height. This is done using geom_histogram(). See the “Bar plot” section of the ggplot basics page to understand difference between geom_histogram(), geom_bar(), and geom_col().\nWe will show the distribution of ages of cases. Within mapping = aes() specify which column you want to see the distribution of. You can assign this column to either the x or the y axis.\nThe rows will be assigned to “bins” based on their numeric age, and these bins will be graphically represented by bars. If you specify a number of bins with the bins = plot aesthetic, the break points are evenly spaced between the minimum and maximum values of the histogram. If bins = is unspecified, an appropriate number of bins will be guessed and this message displayed after the plot:\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nIf you do not want to specify a number of bins to bins =, you could alternatively specify binwidth = in the units of the axis. We give a few examples showing different bins and bin widths:\n\n# A) Regular histogram\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram()+\n  labs(title = \"A) Default histogram (30 bins)\")\n\n# B) More bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(bins = 50)+\n  labs(title = \"B) Set to 50 bins\")\n\n# C) Fewer bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(bins = 5)+\n  labs(title = \"C) Set to 5 bins\")\n\n\n# D) More bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(binwidth = 1)+\n  labs(title = \"D) binwidth of 1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo get smoothed proportions, you can use geom_density():\n\n# Frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age)) +\n  geom_density(size = 2, alpha = 0.2)+\n  labs(title = \"Proportional density\")\n\n# Stacked frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age, fill = gender)) +\n  geom_density(size = 2, alpha = 0.2, position = \"stack\")+\n  labs(title = \"'Stacked' proportional densities\")\n\n\n\n\n\n\n\nTo get a “stacked” histogram (of a continuous column of data), you can do one of the following:\n\nUse geom_histogram() with the fill = argument within aes() and assigned to the grouping column, or\n\nUse geom_freqpoly(), which is likely easier to read (you can still set binwidth =)\n\nTo see proportions of all values, set the y = after_stat(density) (use this syntax exactly - not changed for your data). Note: these proportions will show per group.\n\nEach is shown below (*note use of color = vs. fill = in each):\n\n# \"Stacked\" histogram\nggplot(data = linelist, mapping = aes(x = age, fill = gender)) +\n  geom_histogram(binwidth = 2)+\n  labs(title = \"'Stacked' histogram\")\n\n# Frequency \nggplot(data = linelist, mapping = aes(x = age, color = gender)) +\n  geom_freqpoly(binwidth = 2, size = 2)+\n  labs(title = \"Freqpoly\")\n\n# Frequency with proportion axis\nggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), color = gender)) +\n  geom_freqpoly(binwidth = 5, size = 2)+\n  labs(title = \"Proportional freqpoly\")\n\n# Frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), fill = gender)) +\n  geom_density(size = 2, alpha = 0.2)+\n  labs(title = \"Proportional, smoothed with geom_density()\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to have some fun, try geom_density_ridges from the ggridges package (vignette here.\nRead more in detail about histograms at the tidyverse page on geom_histogram().\n\n\n\nBox plots are common, but have important limitations. They can obscure the actual distribution - e.g. a bi-modal distribution. See this R graph gallery and this data-to-viz article for more details. However, they do nicely display the inter-quartile range and outliers - so they can be overlaid on top of other types of plots that show the distribution in more detail.\nBelow we remind you of the various components of a boxplot:\n\n\n\n\n\n\n\n\n\nWhen using geom_boxplot() to create a box plot, you generally map only one axis (x or y) within aes(). The axis specified determines if the plots are horizontal or vertical.\nIn most geoms, you create a plot per group by mapping an aesthetic like color = or fill = to a column within aes(). However, for box plots achieve this by assigning the grouping column to the un-assigned axis (x or y). Below is code for a boxplot of all age values in the dataset, and second is code to display one box plot for each (non-missing) gender in the dataset. Note that NA (missing) values will appear as a separate box plot unless removed. In this example we also set the fill to the column outcome so each plot is a different color - but this is not necessary.\n\n# A) Overall boxplot\nggplot(data = linelist)+  \n  geom_boxplot(mapping = aes(y = age))+   # only y axis mapped (not x)\n  labs(title = \"A) Overall boxplot\")\n\n# B) Box plot by group\nggplot(data = linelist, mapping = aes(y = age, x = gender, fill = gender)) + \n  geom_boxplot()+                     \n  theme(legend.position = \"none\")+   # remove legend (redundant)\n  labs(title = \"B) Boxplot by gender\")      \n\n\n\n\n\n\n\n\n\n\nBelow is code for creating violin plots (geom_violin) and jitter plots (geom_jitter) to show distributions. You can specify that the fill or color is also determined by the data, by inserting these options within aes().\n\n# A) Jitter plot by group\nggplot(data = linelist %&gt;% drop_na(outcome),      # remove missing values\n       mapping = aes(y = age,                     # Continuous variable\n           x = outcome,                           # Grouping variable\n           color = outcome))+                     # Color variable\n  geom_jitter()+                                  # Create the violin plot\n  labs(title = \"A) jitter plot by gender\")     \n\n\n\n# B) Violin plot by group\nggplot(data = linelist %&gt;% drop_na(outcome),       # remove missing values\n       mapping = aes(y = age,                      # Continuous variable\n           x = outcome,                            # Grouping variable\n           fill = outcome))+                       # fill variable (color)\n  geom_violin()+                                   # create the violin plot\n  labs(title = \"B) violin plot by gender\")    \n\n\n\n\n\n\n\nYou can combine the two using the geom_sina() function from the ggforce package. The sina plots the jitter points in the shape of the violin plot. When overlaid on the violin plot (adjusting the transparencies) this can be easier to visually interpret.\n\n# A) Sina plot by group\nggplot(\n  data = linelist %&gt;% drop_na(outcome), \n  aes(y = age,           # numeric variable\n      x = outcome)) +    # group variable\n  geom_violin(\n    aes(fill = outcome), # fill (color of violin background)\n    color = \"white\",     # white outline\n    alpha = 0.2)+        # transparency\n  geom_sina(\n    size=1,                # Change the size of the jitter\n    aes(color = outcome))+ # color (color of dots)\n  scale_fill_manual(       # Define fill for violin background by death/recover\n    values = c(\"Death\" = \"#bf5300\", \n              \"Recover\" = \"#11118c\")) + \n  scale_color_manual(      # Define colours for points by death/recover\n    values = c(\"Death\" = \"#bf5300\", \n              \"Recover\" = \"#11118c\")) + \n  theme_minimal() +                                # Remove the gray background\n  theme(legend.position = \"none\") +                # Remove unnecessary legend\n  labs(title = \"B) violin and sina plot by gender, with extra formatting\")      \n\n\n\n\n\n\n\nFollowing similar syntax, geom_point() will allow you to plot two continuous variables against each other in a scatter plot. This is useful for showing actual values rather than their distributions. A basic scatter plot of age vs weight is shown in (A). In (B) we again use facet_grid() to show the relationship between two continuous variables in the linelist.\n\n# Basic scatter plot of weight and age\nggplot(data = linelist, \n       mapping = aes(y = wt_kg, x = age))+\n  geom_point() +\n  labs(title = \"A) Scatter plot of weight and age\")\n\n# Scatter plot of weight and age by gender and Ebola outcome\nggplot(data = linelist %&gt;% drop_na(gender, outcome), # filter retains non-missing gender/outcome\n       mapping = aes(y = wt_kg, x = age))+\n  geom_point() +\n  labs(title = \"B) Scatter plot of weight and age faceted by gender and outcome\")+\n  facet_grid(gender ~ outcome) \n\n\n\n\n\n\n\n\n\n\nYou can display three continuous variables by utilizing the fill = argument to create a heat plot. The color of each “cell” will reflect the value of the third continuous column of data. There are ways to make 3D plots in R, but for applied epidemiology these are often difficult to interpret and therefore less useful for decision-making.\n\n\n\n\nCategorical data can be character values, could be logical (TRUE/FALSE), or factors.\n\n\n\n\nThe first thing to understand about your categorical data is whether it exists as raw observations like a linelist of cases, or as a summary or aggregate data frame that holds counts or proportions. The state of your data will impact which plotting function you use:\n\nIf your data are raw observations with one row per observation, you will likely use geom_bar()\n\nIf your data are already aggregated into counts or proportions, you will likely use geom_col()\n\n\n\n\nNext, examine the class of the columns you want to plot. We look at hospital, first with class() from base R, and with tabyl() from janitor.\n\n# View class of hospital column - we can see it is a character\nclass(linelist$hospital)\n\n[1] \"character\"\n\n# Look at values and proportions within hospital column\nlinelist %&gt;% \n  tabyl(hospital)\n\n                             hospital    n    percent\n                     Central Hospital  454 0.07710598\n                    Military Hospital  896 0.15217391\n                              Missing 1469 0.24949049\n                                Other  885 0.15030571\n                        Port Hospital 1762 0.29925272\n St. Mark's Maternity Hospital (SMMH)  422 0.07167120\n\n\nWe can see the values within are characters, as they are hospital names, and by default they are ordered alphabetically. There are ‘other’ and ‘missing’ values, which we would prefer to be the last subcategories when presenting breakdowns. So we change this column into a factor and re-order it.\n\n# Convert to factor and define level order so \"Other\" and \"Missing\" are last\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    hospital = fct_relevel(hospital, \n      \"St. Mark's Maternity Hospital (SMMH)\",\n      \"Port Hospital\", \n      \"Central Hospital\",\n      \"Military Hospital\",\n      \"Other\",\n      \"Missing\"))\n\n\nlevels(linelist$hospital)\n\n[1] \"St. Mark's Maternity Hospital (SMMH)\"\n[2] \"Port Hospital\"                       \n[3] \"Central Hospital\"                    \n[4] \"Military Hospital\"                   \n[5] \"Other\"                               \n[6] \"Missing\"                             \n\n\n\n\n\n\nUse geom_bar() if you want bar height (or the height of stacked bar components) to reflect the number of relevant rows in the data. These bars will have gaps between them, unless the width = plot aesthetic is adjusted.\n\nProvide only one axis column assignment (typically x-axis). If you provide x and y, you will get Error: stat_count() can only have an x or y aesthetic.\n\nYou can create stacked bars by adding a fill = column assignment within mapping = aes()\n\nThe opposite axis will be titled “count” by default, because it represents the number of rows\n\nBelow, we have assigned outcome to the y-axis, but it could just as easily be on the x-axis. If you have longer character values, it can sometimes look better to flip the bars sideways and put the legend on the bottom. This may impact how your factor levels are ordered - in this case we reverse them with fct_rev() to put missing and other at the bottom.\n\n# A) Outcomes in all cases\nggplot(linelist %&gt;% drop_na(outcome)) + \n  geom_bar(aes(y = fct_rev(hospital)), width = 0.7) +\n  theme_minimal()+\n  labs(title = \"A) Number of cases by hospital\",\n       y = \"Hospital\")\n\n\n# B) Outcomes in all cases by hosptial\nggplot(linelist %&gt;% drop_na(outcome)) + \n  geom_bar(aes(y = fct_rev(hospital), fill = outcome), width = 0.7) +\n  theme_minimal()+\n  theme(legend.position = \"bottom\") +\n  labs(title = \"B) Number of recovered and dead Ebola cases, by hospital\",\n       y = \"Hospital\")\n\n\n\n\n\n\n\n\n\n\nUse geom_col() if you want bar height (or height of stacked bar components) to reflect pre-calculated values that exists in the data. Often, these are summary or “aggregated” counts, or proportions.\nProvide column assignments for both axes to geom_col(). Typically your x-axis column is discrete and your y-axis column is numeric.\nLet’s say we have this dataset outcomes:\n\n\n# A tibble: 2 × 3\n  outcome     n proportion\n  &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 Death    1022       56.2\n2 Recover   796       43.8\n\n\nBelow is code using geom_col for creating simple bar charts to show the distribution of Ebola patient outcomes. With geom_col, both x and y need to be specified. Here x is the categorical variable along the x axis, and y is the generated proportions column proportion.\n\n# Outcomes in all cases\nggplot(outcomes) + \n  geom_col(aes(x=outcome, y = proportion)) +\n  labs(subtitle = \"Number of recovered and dead Ebola cases\")\n\n\n\n\nTo show breakdowns by hospital, we would need our table to contain more information, and to be in “long” format. We create this table with the frequencies of the combined categories outcome and hospital.\n\noutcomes2 &lt;- linelist %&gt;% \n  drop_na(outcome) %&gt;% \n  count(hospital, outcome) %&gt;%  # get counts by hospital and outcome\n  group_by(hospital) %&gt;%        # Group so proportions are out of hospital total\n  mutate(proportion = n/sum(n)*100) # calculate proportions of hospital total\n\nhead(outcomes2) # Preview data\n\n# A tibble: 6 × 4\n# Groups:   hospital [3]\n  hospital                             outcome     n proportion\n  &lt;fct&gt;                                &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 St. Mark's Maternity Hospital (SMMH) Death     199       61.2\n2 St. Mark's Maternity Hospital (SMMH) Recover   126       38.8\n3 Port Hospital                        Death     785       57.6\n4 Port Hospital                        Recover   579       42.4\n5 Central Hospital                     Death     193       53.9\n6 Central Hospital                     Recover   165       46.1\n\n\nWe then create the ggplot with some added formatting:\n\nAxis flip: Swapped the axis around with coord_flip() so that we can read the hospital names.\nColumns side-by-side: Added a position = \"dodge\" argument so that the bars for death and recover are presented side by side rather than stacked. Note stacked bars are the default.\nColumn width: Specified ‘width’, so the columns are half as thin as the full possible width.\nColumn order: Reversed the order of the categories on the y axis so that ‘Other’ and ‘Missing’ are at the bottom, with scale_x_discrete(limits=rev). Note that we used that rather than scale_y_discrete because hospital is stated in the x argument of aes(), even if visually it is on the y axis. We do this because Ggplot seems to present categories backwards unless we tell it not to.\n\nOther details: Labels/titles and colours added within labs and scale_fill_color respectively.\n\n\n# Outcomes in all cases by hospital\nggplot(outcomes2) +  \n  geom_col(\n    mapping = aes(\n      x = proportion,                 # show pre-calculated proportion values\n      y = fct_rev(hospital),          # reverse level order so missing/other at bottom\n      fill = outcome),                # stacked by outcome\n    width = 0.5)+                    # thinner bars (out of 1)\n  theme_minimal() +                  # Minimal theme \n  theme(legend.position = \"bottom\")+\n  labs(subtitle = \"Number of recovered and dead Ebola cases, by hospital\",\n       fill = \"Outcome\",             # legend title\n       y = \"Count\",                  # y axis title\n       x = \"Hospital of admission\")+ # x axis title\n  scale_fill_manual(                 # adding colors manually\n    values = c(\"Death\"= \"#3B1c8C\",\n               \"Recover\" = \"#21908D\" )) \n\n\n\n\nNote that the proportions are binary, so we may prefer to drop ‘recover’ and just show the proportion who died. This is just for illustration purposes.\nIf using geom_col() with dates data (e.g. an epicurve from aggregated data) - you will want to adjust the width = argument to remove the “gap” lines between the bars. If using daily data set width = 1. If weekly, width = 7. Months are not possible because each month has a different number of days."
  },
  {
    "objectID": "readings/ggplot-basics.html#basics-of-ggplot",
    "href": "readings/ggplot-basics.html#basics-of-ggplot",
    "title": "Data visualization",
    "section": "",
    "text": "“Grammar of graphics” - ggplot2\nPlotting with ggplot2 is based on “adding” plot layers and design elements on top of one another, with each command added to the previous ones with a plus symbol (+). The result is a multi-layer plot object that can be saved, modified, printed, exported, etc.\n\nThe idea behind the Grammar of Graphics it is that you can build every graph from the same 3 components: (1) a data set, (2) a coordinate system, and (3) geoms — i.e. visual marks that represent data points [source]\n\nggplot objects can be highly complex, but the basic order of layers will usually look like this:\n\nBegin with the baseline ggplot() command - this “opens” the ggplot and allow subsequent functions to be added with +. Typically the dataset is also specified in this command\n\nAdd “geom” layers - these functions visualize the data as geometries (shapes), e.g. as a bar graph, line plot, scatter plot, histogram (or a combination!). These functions all start with geom_ as a prefix.\n\nAdd design elements to the plot such as axis labels, title, fonts, sizes, color schemes, legends, or axes rotation\n\nIn code this amounts to the basic template:\n\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +  &lt;GEOM_FUNCTION&gt;()\n\nWe can further expand this template to include aspects of the visualization such as theme and labels:\n\n# plot data from my_data columns as red points\nggplot(data = my_data)+                   # use the dataset \"my_data\"\n  geom_point(                             # add a layer of points (dots)\n    mapping = aes(x = col1, y = col2),    # \"map\" data column to axes\n    color = \"red\")+                       # other specification for the geom\n  labs()+                                 # here you add titles, axes labels, etc.\n  theme()                                 # here you adjust color, font, size etc of non-data plot elements (axes, title, etc.) \n\nWe will explain each component in the sections below."
  },
  {
    "objectID": "readings/ggplot-basics.html#ggplot",
    "href": "readings/ggplot-basics.html#ggplot",
    "title": "Data visualization",
    "section": "",
    "text": "The opening command of any ggplot2 plot is ggplot(). This command simply creates a blank canvas upon which to add layers. It “opens” the way for further layers to be added with a + symbol.\nTypically, the command ggplot() includes the data = argument for the plot. This sets the default dataset to be used for subsequent layers of the plot.\nThis command will end with a + after its closing parentheses. This leaves the command “open”. The ggplot will only execute/appear when the full command includes a final layer without a + at the end.\n\n# This will create plot that is a blank canvas\nggplot(data = linelist)"
  },
  {
    "objectID": "readings/ggplot-basics.html#geoms",
    "href": "readings/ggplot-basics.html#geoms",
    "title": "Data visualization",
    "section": "",
    "text": "A blank canvas is certainly not sufficient - we need to create geometries (shapes) from our data (e.g. bar plots, histograms, scatter plots, box plots).\nThis is done by adding layers “geoms” to the initial ggplot() command. There are many ggplot2 functions that create “geoms”. Each of these functions begins with “geom_”, so we will refer to them generically as geom_XXXX(). There are over 40 geoms in ggplot2 and many others created by fans. View them at the ggplot2 gallery. Some common geoms are listed below:\n\nHistograms - geom_histogram()\n\nBar charts - geom_bar() or geom_col() (see “Bar plot” section)\n\nBox plots - geom_boxplot()\n\nPoints (e.g. scatter plots) - geom_point()\n\nLine graphs - geom_line() or geom_path()\n\nTrend lines - geom_smooth()\n\nIn one plot you can display one or multiple geoms. Each is added to previous ggplot2 commands with a +, and they are plotted sequentially such that later geoms are plotted on top of previous ones."
  },
  {
    "objectID": "readings/ggplot-basics.html#ggplot_basics_mapping",
    "href": "readings/ggplot-basics.html#ggplot_basics_mapping",
    "title": "Data visualization",
    "section": "",
    "text": "Most geom functions must be told what to use to create their shapes - so you must tell them how they should map (assign) columns in your data to components of the plot like the axes, shape colors, shape sizes, etc. For most geoms, the essential components that must be mapped to columns in the data are the x-axis, and (if necessary) the y-axis.\nThis “mapping” occurs with the mapping = argument. The mappings you provide to mapping must be wrapped in the aes() function, so you would write something like mapping = aes(x = col1, y = col2), as shown below.\nBelow, in the ggplot() command the data are set as the case linelist. In the mapping = aes() argument the column age is mapped to the x-axis, and the column wt_kg is mapped to the y-axis.\nAfter a +, the plotting commands continue. A shape is created with the “geom” function geom_point(). This geom inherits the mappings from the ggplot() command above - it knows the axis-column assignments and proceeds to visualize those relationships as points on the canvas.\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+\n  geom_point()\n\n\n\n\nAs another example, the following commands utilize the same data, a slightly different mapping, and a different geom. The geom_histogram() function only requires a column mapped to the x-axis, as the counts y-axis is generated automatically.\n\nggplot(data = linelist, mapping = aes(x = age))+\n  geom_histogram()\n\n\n\n\n\n\nIn ggplot terminology a plot “aesthetic” has a specific meaning. It refers to a visual property of plotted data. Note that “aesthetic” here refers to the data being plotted in geoms/shapes - not the surrounding display such as titles, axis labels, background color, that you might associate with the word “aesthetics” in common English. In ggplot those details are called “themes” and are adjusted within a theme() command (see this section).\nTherefore, plot object aesthetics can be colors, sizes, transparencies, placement, etc. of the plotted data. Not all geoms will have the same aesthetic options, but many can be used by most geoms. Here are some examples:\n\nshape = Display a point with geom_point() as a dot, star, triangle, or square…\n\nfill = The interior color (e.g. of a bar or boxplot)\n\ncolor = The exterior line of a bar, boxplot, etc., or the point color if using geom_point()\n\nsize = Size (e.g. line thickness, point size)\n\nalpha = Transparency (1 = opaque, 0 = invisible)\n\nbinwidth = Width of histogram bins\n\nwidth = Width of “bar plot” columns\n\nlinetype = Line type (e.g. solid, dashed, dotted)\n\nThese plot object aesthetics can be assigned values in two ways:\n\nAssigned a static value (e.g. color = \"blue\") to apply across all plotted observations\n\nAssigned to a column of the data (e.g. color = hospital) such that display of each observation depends on its value in that column\n\n\n\n\nIf you want the plot object aesthetic to be static, that is - to be the same for every observation in the data, you write its assignment within the geom but outside of any mapping = aes() statement. These assignments could look like size = 1 or color = \"blue\". Here are two examples:\n\nIn the first example, the mapping = aes() is in the ggplot() command and the axes are mapped to age and weight columns in the data. The plot aesthetics color =, size =, and alpha = (transparency) are assigned to static values. For clarity, this is done in the geom_point() function, as you may add other geoms afterward that would take different values for their plot aesthetics.\n\nIn the second example, the histogram requires only the x-axis mapped to a column. The histogram binwidth =, color =, fill = (internal color), and alpha = are again set within the geom to static values.\n\n\n# scatterplot\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  # set data and axes mapping\n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)         # set static point aesthetics\n\n# histogram\nggplot(data = linelist, mapping = aes(x = age))+       # set data and axes\n  geom_histogram(              # display histogram\n    binwidth = 7,                # width of bins\n    color = \"red\",               # bin line color\n    fill = \"blue\",               # bin interior color\n    alpha = 0.1)                 # bin transparency\n\n\n\n\n\n\n\n\n\n\nThe alternative is to scale the plot object aesthetic by the values in a column. In this approach, the display of this aesthetic will depend on that observation’s value in that column of the data. If the column values are continuous, the display scale (legend) for that aesthetic will be continuous. If the column values are discrete, the legend will display each value and the plotted data will appear as distinctly “grouped” (read more in the grouping section of this page).\nTo achieve this, you map that plot aesthetic to a column name (not in quotes). This must be done within a mapping = aes() function (note: there are several places in the code you can make these mapping assignments, as discussed below).\nTwo examples are below.\n\nIn the first example, the color = aesthetic (of each point) is mapped to the column age - and a scale has appeared in a legend! For now just note that the scale exists - we will show how to modify it in later sections.\n\nIn the second example two new plot aesthetics are also mapped to columns (color = and size =), while the plot aesthetics shape = and alpha = are mapped to static values outside of any mapping = aes() function.\n\n\n# scatterplot\nggplot(data = linelist,   # set data\n       mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age)\n       )+     # map color to age\n  geom_point()         # display data as points \n\n# scatterplot\nggplot(data = linelist,   # set data\n       mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age,       # map color to age\n         size = age))+      # map size to age\n  geom_point(             # display data as points\n    shape = \"diamond\",      # points display as diamonds\n    alpha = 0.3)            # point transparency at 30%\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAxes assignments are always assigned to columns in the data (not to static values), and this is always done within mapping = aes().\n\n\nIt becomes important to keep track of your plot layers and aesthetics when making more complex plots - for example plots with multiple geoms. In the example below, the size = aesthetic is assigned twice - once for geom_point() and once for geom_smooth() - both times as a static value.\n\nggplot(data = linelist,\n       mapping = aes(           # map aesthetics to columns\n         x = age,\n         y = wt_kg,\n         color = age_years)\n       ) + \n  geom_point(                   # add points for each row of data\n    size = 1,\n    alpha = 0.5) +  \n  geom_smooth(                  # add a trend line \n    method = \"lm\",              # with linear method\n    size = 2)                   # size (width of line) of 2\n\n\n\n\n\n\n\nAesthetic mapping within mapping = aes() can be written in several places in your plotting commands and can even be written more than once. This can be written in the top ggplot() command, and/or for each individual geom beneath. The nuances include:\n\nMapping assignments made in the top ggplot() command will be inherited as defaults across any geom below, like how x = and y = are inherited\nMapping assignments made within one geom apply only to that geom\n\nLikewise, data = specified in the top ggplot() will apply by default to any geom below, but you could also specify data for each geom (but this is more difficult).\nThus, each of the following commands will create the same plot:\n\n# These commands will produce the exact same plot\nggplot(data = linelist, mapping = aes(x = age))+\n  geom_histogram()\n\nggplot(data = linelist)+\n  geom_histogram(mapping = aes(x = age))\n\nggplot()+\n  geom_histogram(data = linelist, mapping = aes(x = age))\n\n\n\n\nYou can easily group the data and “plot by group”. In fact, you have already done this!\nAssign the “grouping” column to the appropriate plot aesthetic, within a mapping = aes(). Above, we demonstrated this using continuous values when we assigned point size = to the column age. However this works the same way for discrete/categorical columns.\nFor example, if you want points to be displayed by gender, you would set mapping = aes(color = gender). A legend automatically appears. This assignment can be made within the mapping = aes() in the top ggplot() command (and be inherited by the geom), or it could be set in a separate mapping = aes() within the geom. Both approaches are shown below:\n\nggplot(data = linelist,\n       mapping = aes(x = age, y = wt_kg, color = gender))+\n  geom_point(alpha = 0.5)\n\n\n\n\n\n# This alternative code produces the same plot\nggplot(data = linelist,\n       mapping = aes(x = age, y = wt_kg))+\n  geom_point(\n    mapping = aes(color = gender),\n    alpha = 0.5)\n\nNote that depending on the geom, you will need to use different arguments to group the data. For geom_point() you will most likely use color =, shape = or size =. Whereas for geom_bar() you are more likely to use fill =. This just depends on the geom and what plot aesthetic you want to reflect the groupings.\nFor your information - the most basic way of grouping the data is by using only the group = argument within mapping = aes(). However, this by itself will not change the colors, fill, or shapes. Nor will it create a legend. Yet the data are grouped, so statistical displays may be affected."
  },
  {
    "objectID": "readings/ggplot-basics.html#ggplot_basics_facet",
    "href": "readings/ggplot-basics.html#ggplot_basics_facet",
    "title": "Data visualization",
    "section": "",
    "text": "Facets, or “small-multiples”, are used to split one plot into a multi-panel figure, with one panel (“facet”) per group of data. The same type of plot is created multiple times, each one using a sub-group of the same dataset.\nFaceting is a functionality that comes with ggplot2, so the legends and axes of the facet “panels” are automatically aligned. We would need to use other packages to combine completely different plots (cowplot and patchwork) into one figure.\nFaceting is done with one of the following ggplot2 functions:\n\nfacet_wrap() To show a different panel for each level of a single variable. One example of this could be showing a different epidemic curve for each hospital in a region. Facets are ordered alphabetically, unless the variable is a factor with other ordering defined.\n\n\n\nYou can invoke certain options to determine the layout of the facets, e.g. nrow = 1 or ncol = 1 to control the number of rows or columns that the faceted plots are arranged within.\n\n\nfacet_grid() This is used when you want to bring a second variable into the faceting arrangement. Here each panel of a grid shows the intersection between values in two columns. For example, epidemic curves for each hospital-age group combination with hospitals along the top (columns) and age groups along the sides (rows).\n\n\n\nnrow and ncol are not relevant, as the subgroups are presented in a grid\n\nEach of these functions accept a formula syntax to specify the column(s) for faceting. Both accept up to two columns, one on each side of a tilde ~.\n\nFor facet_wrap() most often you will write only one column preceded by a tilde ~ like facet_wrap(~hospital). However you can write two columns facet_wrap(outcome ~ hospital) - each unique combination will display in a separate panel, but they will not be arranged in a grid. The headings will show combined terms and these won’t be specific logic to the columns vs. rows. If you are providing only one faceting variable, a period . is used as a placeholder on the other side of the formula - see the code examples.\nFor facet_grid() you can also specify one or two columns to the formula (grid rows ~ columns). If you only want to specify one, you can place a period . on the other side of the tilde like facet_grid(. ~ hospital) or facet_grid(hospital ~ .).\n\nFacets can quickly contain an overwhelming amount of information - its good to ensure you don’t have too many levels of each variable that you choose to facet by. Here are some quick examples with the malaria dataset which consists of daily case counts of malaria for facilities, by age group.\nBelow we import and do some quick modifications for simplicity:\n\n# These data are daily counts of malaria cases, by facility-day\nmalaria_data &lt;- import(\"data/malaria_facility_count_data.rds\") %&gt;%  # import\n  select(-submitted_date, -Province, -newid)                                 # remove unneeded columns\n\nThe first 50 rows of the malaria data are below. Note there is a column malaria_tot, but also columns for counts by age group (these will be used in the second, facet_grid() example).\n\n\n# A tibble: 50 × 7\n   location_name data_date  District `malaria_rdt_0-4` `malaria_rdt_5-14`\n   &lt;chr&gt;         &lt;date&gt;     &lt;chr&gt;                &lt;int&gt;              &lt;int&gt;\n 1 Facility 1    2020-08-11 Spring                  11                 12\n 2 Facility 2    2020-08-11 Bolo                    11                 10\n 3 Facility 3    2020-08-11 Dingo                    8                  5\n 4 Facility 4    2020-08-11 Bolo                    16                 16\n 5 Facility 5    2020-08-11 Bolo                     9                  2\n 6 Facility 6    2020-08-11 Dingo                    3                  1\n 7 Facility 6    2020-08-10 Dingo                    4                  0\n 8 Facility 5    2020-08-10 Bolo                    15                 14\n 9 Facility 5    2020-08-09 Bolo                    11                 11\n10 Facility 5    2020-08-08 Bolo                    19                 15\n# ℹ 40 more rows\n# ℹ 2 more variables: malaria_rdt_15 &lt;int&gt;, malaria_tot &lt;int&gt;\n\n\n\n\nFor the moment, let’s focus on the columns malaria_tot and District. Ignore the age-specific count columns for now. We will plot epidemic curves with geom_col(), which produces a column for each day at the specified y-axis height given in column malaria_tot (the data are already daily counts, so we use geom_col() - see the “Bar plot” section below).\nWhen we add the command facet_wrap(), we specify a tilde and then the column to facet on (District in this case). You can place another column on the left side of the tilde, - this will create one facet for each combination - but we recommend you do this with facet_grid() instead. In this use case, one facet is created for each unique value of District.\n\n# A plot with facets by district\nggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1, fill = \"darkred\") +       # plot the count data as columns\n  theme_minimal()+                              # simplify the background panels\n  labs(                                         # add plot labels, title, etc.\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district\") +\n  facet_wrap(~District)                       # the facets are created\n\n\n\n\n\n\n\nWe can use a facet_grid() approach to cross two variables. Let’s say we want to cross District and age. Well, we need to do some data transformations on the age columns to get these data into ggplot-preferred “long” format. The age groups all have their own columns - we want them in a single column called age_group and another called num_cases.\n\nmalaria_age &lt;- malaria_data %&gt;%\n  select(-malaria_tot) %&gt;% \n  pivot_longer(\n    cols = c(starts_with(\"malaria_rdt_\")),  # choose columns to pivot longer\n    names_to = \"age_group\",      # column names become age group\n    values_to = \"num_cases\"      # values to a single column (num_cases)\n  ) %&gt;%\n  mutate(\n    age_group = str_replace(age_group, \"malaria_rdt_\", \"\"),\n    age_group = forcats::fct_relevel(age_group, \"5-14\", after = 1))\n\nNow the first 50 rows of data look like this:\n\n\n# A tibble: 50 × 5\n   location_name data_date  District age_group num_cases\n   &lt;chr&gt;         &lt;date&gt;     &lt;chr&gt;    &lt;fct&gt;         &lt;int&gt;\n 1 Facility 1    2020-08-11 Spring   0-4              11\n 2 Facility 1    2020-08-11 Spring   5-14             12\n 3 Facility 1    2020-08-11 Spring   15               23\n 4 Facility 2    2020-08-11 Bolo     0-4              11\n 5 Facility 2    2020-08-11 Bolo     5-14             10\n 6 Facility 2    2020-08-11 Bolo     15                5\n 7 Facility 3    2020-08-11 Dingo    0-4               8\n 8 Facility 3    2020-08-11 Dingo    5-14              5\n 9 Facility 3    2020-08-11 Dingo    15                5\n10 Facility 4    2020-08-11 Bolo     0-4              16\n# ℹ 40 more rows\n\n\nWhen you pass the two variables to facet_grid(), easiest is to use formula notation (e.g. x ~ y) where x is rows and y is columns. Here is the plot, using facet_grid() to show the plots for each combination of the columns age_group and District.\n\nggplot(malaria_age, aes(x = data_date, y = num_cases)) +\n  geom_col(fill = \"darkred\", width = 1) +\n  theme_minimal()+\n  labs(\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district and age group\"\n  ) +\n  facet_grid(District ~ age_group)\n\n\n\n\n\n\n\nThe axes scales displayed when faceting are by default the same (fixed) across all the facets. This is helpful for cross-comparison, but not always appropriate.\nWhen using facet_wrap() or facet_grid(), we can add scales = \"free_y\" to “free” or release the y-axes of the panels to scale appropriately to their data subset. This is particularly useful if the actual counts are small for one of the subcategories and trends are otherwise hard to see. Instead of “free_y” we can also write “free_x” to do the same for the x-axis (e.g. for dates) or “free” for both axes. Note that in facet_grid, the y scales will be the same for facets in the same row, and the x scales will be the same for facets in the same column.\nWhen using facet_grid only, we can add space = \"free_y\" or space = \"free_x\" so that the actual height or width of the facet is weighted to the values of the figure within. This only works if scales = \"free\" (y or x) is already applied.\n\n# Free y-axis\nggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1, fill = \"darkred\") +       # plot the count data as columns\n  theme_minimal()+                              # simplify the background panels\n  labs(                                         # add plot labels, title, etc.\n    x = \"Date of report\",\n    y = \"Malaria cases\",\n    title = \"Malaria cases by district - 'free' x and y axes\") +\n  facet_wrap(~District, scales = \"free\")        # the facets are created"
  },
  {
    "objectID": "readings/ggplot-basics.html#exporting-plots",
    "href": "readings/ggplot-basics.html#exporting-plots",
    "title": "Data visualization",
    "section": "",
    "text": "Exporting ggplots is made easy with the ggsave() function from ggplot2. It can work in two ways, either:\n\nSpecify the name of the plot object, then the file path and name with extension\n\nFor example: ggsave(my_plot, \"plots/my_plot.png\"))\n\n\nRun the command with only a file path, to save the last plot that was printed\n\nFor example: ggsave(\"plots/my_plot.png\"))\n\n\nYou can export as png, pdf, jpeg, tiff, bmp, svg, or several other file types, by specifying the file extension in the file path.\nYou can also specify the arguments width =, height =, and units = (either “in”, “cm”, or “mm”). You can also specify dpi = with a number for plot resolution (e.g. 300). See the function details by entering ?ggsave or reading the documentation online."
  },
  {
    "objectID": "readings/ggplot-basics.html#labels",
    "href": "readings/ggplot-basics.html#labels",
    "title": "Data visualization",
    "section": "",
    "text": "Surely you will want to add or adjust the plot’s labels. These are most easily done within the labs() function which is added to the plot with + just as the geoms were.\nWithin labs() you can provide character strings to these arguements:\n\nx = and y = The x-axis and y-axis title (labels)\n\ntitle = The main plot title\n\nsubtitle = The subtitle of the plot, in smaller text below the title\n\ncaption = The caption of the plot, in bottom-right by default\n\nHere is a plot we made earlier, but with nicer labels:\n\nage_by_wt &lt;- ggplot(\n  data = linelist,   # set data\n  mapping = aes(     # map aesthetics to column values\n         x = age,           # map x-axis to age            \n         y = wt_kg,         # map y-axis to weight\n         color = age))+     # map color to age\n  geom_point()+           # display data as points\n  labs(\n    title = \"Age and weight distribution\",\n    subtitle = \"Fictional Ebola outbreak, 2014\",\n    x = \"Age in years\",\n    y = \"Weight in kilos\",\n    color = \"Age\",\n    caption = stringr::str_glue(\"Data as of {max(linelist$date_hospitalisation, na.rm=T)}\"))\n\nage_by_wt\n\n\n\n\nNote how in the caption assignment we used str_glue() from the stringr package to implant dynamic R code within the string text. The caption will show the “Data as of:” date that reflects the maximum hospitalization date in the linelist."
  },
  {
    "objectID": "readings/ggplot-basics.html#plot-continuous-data",
    "href": "readings/ggplot-basics.html#plot-continuous-data",
    "title": "Data visualization",
    "section": "",
    "text": "Throughout this page, you have already seen many examples of plotting continuous data. Here we briefly consolidate these and present a few variations.\nVisualisations covered here include:\n\nPlots for one continuous variable:\n\nHistogram, a classic graph to present the distribution of a continuous variable.\nBox plot (also called box and whisker), to show the 25th, 50th, and 75th percentiles, tail ends of the distribution, and outliers (important limitations).\n\nJitter plot, to show all values as points that are ‘jittered’ so they can (mostly) all be seen, even where two have the same value.\n\nViolin plot, show the distribution of a continuous variable based on the symmetrical width of the ‘violin’.\nSina plot, are a combination of jitter and violin plots, where individual points are shown but in the symmetrical shape of the distribution (via ggforce package).\n\n\nScatter plot for two continuous variables.\n\nHeat plots for three continuous variables\n\n\n\nHistograms may look like bar charts, but are distinct because they measure the distribution of a continuous variable. There are no spaces between the “bars”, and only one column is provided to geom_histogram().\nBelow is code for generating histograms, which group continuous data into ranges and display in adjacent bars of varying height. This is done using geom_histogram(). See the “Bar plot” section of the ggplot basics page to understand difference between geom_histogram(), geom_bar(), and geom_col().\nWe will show the distribution of ages of cases. Within mapping = aes() specify which column you want to see the distribution of. You can assign this column to either the x or the y axis.\nThe rows will be assigned to “bins” based on their numeric age, and these bins will be graphically represented by bars. If you specify a number of bins with the bins = plot aesthetic, the break points are evenly spaced between the minimum and maximum values of the histogram. If bins = is unspecified, an appropriate number of bins will be guessed and this message displayed after the plot:\n## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nIf you do not want to specify a number of bins to bins =, you could alternatively specify binwidth = in the units of the axis. We give a few examples showing different bins and bin widths:\n\n# A) Regular histogram\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram()+\n  labs(title = \"A) Default histogram (30 bins)\")\n\n# B) More bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(bins = 50)+\n  labs(title = \"B) Set to 50 bins\")\n\n# C) Fewer bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(bins = 5)+\n  labs(title = \"C) Set to 5 bins\")\n\n\n# D) More bins\nggplot(data = linelist, aes(x = age))+  # provide x variable\n  geom_histogram(binwidth = 1)+\n  labs(title = \"D) binwidth of 1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo get smoothed proportions, you can use geom_density():\n\n# Frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age)) +\n  geom_density(size = 2, alpha = 0.2)+\n  labs(title = \"Proportional density\")\n\n# Stacked frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age, fill = gender)) +\n  geom_density(size = 2, alpha = 0.2, position = \"stack\")+\n  labs(title = \"'Stacked' proportional densities\")\n\n\n\n\n\n\n\nTo get a “stacked” histogram (of a continuous column of data), you can do one of the following:\n\nUse geom_histogram() with the fill = argument within aes() and assigned to the grouping column, or\n\nUse geom_freqpoly(), which is likely easier to read (you can still set binwidth =)\n\nTo see proportions of all values, set the y = after_stat(density) (use this syntax exactly - not changed for your data). Note: these proportions will show per group.\n\nEach is shown below (*note use of color = vs. fill = in each):\n\n# \"Stacked\" histogram\nggplot(data = linelist, mapping = aes(x = age, fill = gender)) +\n  geom_histogram(binwidth = 2)+\n  labs(title = \"'Stacked' histogram\")\n\n# Frequency \nggplot(data = linelist, mapping = aes(x = age, color = gender)) +\n  geom_freqpoly(binwidth = 2, size = 2)+\n  labs(title = \"Freqpoly\")\n\n# Frequency with proportion axis\nggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), color = gender)) +\n  geom_freqpoly(binwidth = 5, size = 2)+\n  labs(title = \"Proportional freqpoly\")\n\n# Frequency with proportion axis, smoothed\nggplot(data = linelist, mapping = aes(x = age, y = after_stat(density), fill = gender)) +\n  geom_density(size = 2, alpha = 0.2)+\n  labs(title = \"Proportional, smoothed with geom_density()\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to have some fun, try geom_density_ridges from the ggridges package (vignette here.\nRead more in detail about histograms at the tidyverse page on geom_histogram().\n\n\n\nBox plots are common, but have important limitations. They can obscure the actual distribution - e.g. a bi-modal distribution. See this R graph gallery and this data-to-viz article for more details. However, they do nicely display the inter-quartile range and outliers - so they can be overlaid on top of other types of plots that show the distribution in more detail.\nBelow we remind you of the various components of a boxplot:\n\n\n\n\n\n\n\n\n\nWhen using geom_boxplot() to create a box plot, you generally map only one axis (x or y) within aes(). The axis specified determines if the plots are horizontal or vertical.\nIn most geoms, you create a plot per group by mapping an aesthetic like color = or fill = to a column within aes(). However, for box plots achieve this by assigning the grouping column to the un-assigned axis (x or y). Below is code for a boxplot of all age values in the dataset, and second is code to display one box plot for each (non-missing) gender in the dataset. Note that NA (missing) values will appear as a separate box plot unless removed. In this example we also set the fill to the column outcome so each plot is a different color - but this is not necessary.\n\n# A) Overall boxplot\nggplot(data = linelist)+  \n  geom_boxplot(mapping = aes(y = age))+   # only y axis mapped (not x)\n  labs(title = \"A) Overall boxplot\")\n\n# B) Box plot by group\nggplot(data = linelist, mapping = aes(y = age, x = gender, fill = gender)) + \n  geom_boxplot()+                     \n  theme(legend.position = \"none\")+   # remove legend (redundant)\n  labs(title = \"B) Boxplot by gender\")      \n\n\n\n\n\n\n\n\n\n\nBelow is code for creating violin plots (geom_violin) and jitter plots (geom_jitter) to show distributions. You can specify that the fill or color is also determined by the data, by inserting these options within aes().\n\n# A) Jitter plot by group\nggplot(data = linelist %&gt;% drop_na(outcome),      # remove missing values\n       mapping = aes(y = age,                     # Continuous variable\n           x = outcome,                           # Grouping variable\n           color = outcome))+                     # Color variable\n  geom_jitter()+                                  # Create the violin plot\n  labs(title = \"A) jitter plot by gender\")     \n\n\n\n# B) Violin plot by group\nggplot(data = linelist %&gt;% drop_na(outcome),       # remove missing values\n       mapping = aes(y = age,                      # Continuous variable\n           x = outcome,                            # Grouping variable\n           fill = outcome))+                       # fill variable (color)\n  geom_violin()+                                   # create the violin plot\n  labs(title = \"B) violin plot by gender\")    \n\n\n\n\n\n\n\nYou can combine the two using the geom_sina() function from the ggforce package. The sina plots the jitter points in the shape of the violin plot. When overlaid on the violin plot (adjusting the transparencies) this can be easier to visually interpret.\n\n# A) Sina plot by group\nggplot(\n  data = linelist %&gt;% drop_na(outcome), \n  aes(y = age,           # numeric variable\n      x = outcome)) +    # group variable\n  geom_violin(\n    aes(fill = outcome), # fill (color of violin background)\n    color = \"white\",     # white outline\n    alpha = 0.2)+        # transparency\n  geom_sina(\n    size=1,                # Change the size of the jitter\n    aes(color = outcome))+ # color (color of dots)\n  scale_fill_manual(       # Define fill for violin background by death/recover\n    values = c(\"Death\" = \"#bf5300\", \n              \"Recover\" = \"#11118c\")) + \n  scale_color_manual(      # Define colours for points by death/recover\n    values = c(\"Death\" = \"#bf5300\", \n              \"Recover\" = \"#11118c\")) + \n  theme_minimal() +                                # Remove the gray background\n  theme(legend.position = \"none\") +                # Remove unnecessary legend\n  labs(title = \"B) violin and sina plot by gender, with extra formatting\")      \n\n\n\n\n\n\n\nFollowing similar syntax, geom_point() will allow you to plot two continuous variables against each other in a scatter plot. This is useful for showing actual values rather than their distributions. A basic scatter plot of age vs weight is shown in (A). In (B) we again use facet_grid() to show the relationship between two continuous variables in the linelist.\n\n# Basic scatter plot of weight and age\nggplot(data = linelist, \n       mapping = aes(y = wt_kg, x = age))+\n  geom_point() +\n  labs(title = \"A) Scatter plot of weight and age\")\n\n# Scatter plot of weight and age by gender and Ebola outcome\nggplot(data = linelist %&gt;% drop_na(gender, outcome), # filter retains non-missing gender/outcome\n       mapping = aes(y = wt_kg, x = age))+\n  geom_point() +\n  labs(title = \"B) Scatter plot of weight and age faceted by gender and outcome\")+\n  facet_grid(gender ~ outcome) \n\n\n\n\n\n\n\n\n\n\nYou can display three continuous variables by utilizing the fill = argument to create a heat plot. The color of each “cell” will reflect the value of the third continuous column of data. There are ways to make 3D plots in R, but for applied epidemiology these are often difficult to interpret and therefore less useful for decision-making."
  },
  {
    "objectID": "readings/ggplot-basics.html#plot-categorical-data",
    "href": "readings/ggplot-basics.html#plot-categorical-data",
    "title": "Data visualization",
    "section": "",
    "text": "Categorical data can be character values, could be logical (TRUE/FALSE), or factors.\n\n\n\n\nThe first thing to understand about your categorical data is whether it exists as raw observations like a linelist of cases, or as a summary or aggregate data frame that holds counts or proportions. The state of your data will impact which plotting function you use:\n\nIf your data are raw observations with one row per observation, you will likely use geom_bar()\n\nIf your data are already aggregated into counts or proportions, you will likely use geom_col()\n\n\n\n\nNext, examine the class of the columns you want to plot. We look at hospital, first with class() from base R, and with tabyl() from janitor.\n\n# View class of hospital column - we can see it is a character\nclass(linelist$hospital)\n\n[1] \"character\"\n\n# Look at values and proportions within hospital column\nlinelist %&gt;% \n  tabyl(hospital)\n\n                             hospital    n    percent\n                     Central Hospital  454 0.07710598\n                    Military Hospital  896 0.15217391\n                              Missing 1469 0.24949049\n                                Other  885 0.15030571\n                        Port Hospital 1762 0.29925272\n St. Mark's Maternity Hospital (SMMH)  422 0.07167120\n\n\nWe can see the values within are characters, as they are hospital names, and by default they are ordered alphabetically. There are ‘other’ and ‘missing’ values, which we would prefer to be the last subcategories when presenting breakdowns. So we change this column into a factor and re-order it.\n\n# Convert to factor and define level order so \"Other\" and \"Missing\" are last\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    hospital = fct_relevel(hospital, \n      \"St. Mark's Maternity Hospital (SMMH)\",\n      \"Port Hospital\", \n      \"Central Hospital\",\n      \"Military Hospital\",\n      \"Other\",\n      \"Missing\"))\n\n\nlevels(linelist$hospital)\n\n[1] \"St. Mark's Maternity Hospital (SMMH)\"\n[2] \"Port Hospital\"                       \n[3] \"Central Hospital\"                    \n[4] \"Military Hospital\"                   \n[5] \"Other\"                               \n[6] \"Missing\"                             \n\n\n\n\n\n\nUse geom_bar() if you want bar height (or the height of stacked bar components) to reflect the number of relevant rows in the data. These bars will have gaps between them, unless the width = plot aesthetic is adjusted.\n\nProvide only one axis column assignment (typically x-axis). If you provide x and y, you will get Error: stat_count() can only have an x or y aesthetic.\n\nYou can create stacked bars by adding a fill = column assignment within mapping = aes()\n\nThe opposite axis will be titled “count” by default, because it represents the number of rows\n\nBelow, we have assigned outcome to the y-axis, but it could just as easily be on the x-axis. If you have longer character values, it can sometimes look better to flip the bars sideways and put the legend on the bottom. This may impact how your factor levels are ordered - in this case we reverse them with fct_rev() to put missing and other at the bottom.\n\n# A) Outcomes in all cases\nggplot(linelist %&gt;% drop_na(outcome)) + \n  geom_bar(aes(y = fct_rev(hospital)), width = 0.7) +\n  theme_minimal()+\n  labs(title = \"A) Number of cases by hospital\",\n       y = \"Hospital\")\n\n\n# B) Outcomes in all cases by hosptial\nggplot(linelist %&gt;% drop_na(outcome)) + \n  geom_bar(aes(y = fct_rev(hospital), fill = outcome), width = 0.7) +\n  theme_minimal()+\n  theme(legend.position = \"bottom\") +\n  labs(title = \"B) Number of recovered and dead Ebola cases, by hospital\",\n       y = \"Hospital\")\n\n\n\n\n\n\n\n\n\n\nUse geom_col() if you want bar height (or height of stacked bar components) to reflect pre-calculated values that exists in the data. Often, these are summary or “aggregated” counts, or proportions.\nProvide column assignments for both axes to geom_col(). Typically your x-axis column is discrete and your y-axis column is numeric.\nLet’s say we have this dataset outcomes:\n\n\n# A tibble: 2 × 3\n  outcome     n proportion\n  &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 Death    1022       56.2\n2 Recover   796       43.8\n\n\nBelow is code using geom_col for creating simple bar charts to show the distribution of Ebola patient outcomes. With geom_col, both x and y need to be specified. Here x is the categorical variable along the x axis, and y is the generated proportions column proportion.\n\n# Outcomes in all cases\nggplot(outcomes) + \n  geom_col(aes(x=outcome, y = proportion)) +\n  labs(subtitle = \"Number of recovered and dead Ebola cases\")\n\n\n\n\nTo show breakdowns by hospital, we would need our table to contain more information, and to be in “long” format. We create this table with the frequencies of the combined categories outcome and hospital.\n\noutcomes2 &lt;- linelist %&gt;% \n  drop_na(outcome) %&gt;% \n  count(hospital, outcome) %&gt;%  # get counts by hospital and outcome\n  group_by(hospital) %&gt;%        # Group so proportions are out of hospital total\n  mutate(proportion = n/sum(n)*100) # calculate proportions of hospital total\n\nhead(outcomes2) # Preview data\n\n# A tibble: 6 × 4\n# Groups:   hospital [3]\n  hospital                             outcome     n proportion\n  &lt;fct&gt;                                &lt;chr&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 St. Mark's Maternity Hospital (SMMH) Death     199       61.2\n2 St. Mark's Maternity Hospital (SMMH) Recover   126       38.8\n3 Port Hospital                        Death     785       57.6\n4 Port Hospital                        Recover   579       42.4\n5 Central Hospital                     Death     193       53.9\n6 Central Hospital                     Recover   165       46.1\n\n\nWe then create the ggplot with some added formatting:\n\nAxis flip: Swapped the axis around with coord_flip() so that we can read the hospital names.\nColumns side-by-side: Added a position = \"dodge\" argument so that the bars for death and recover are presented side by side rather than stacked. Note stacked bars are the default.\nColumn width: Specified ‘width’, so the columns are half as thin as the full possible width.\nColumn order: Reversed the order of the categories on the y axis so that ‘Other’ and ‘Missing’ are at the bottom, with scale_x_discrete(limits=rev). Note that we used that rather than scale_y_discrete because hospital is stated in the x argument of aes(), even if visually it is on the y axis. We do this because Ggplot seems to present categories backwards unless we tell it not to.\n\nOther details: Labels/titles and colours added within labs and scale_fill_color respectively.\n\n\n# Outcomes in all cases by hospital\nggplot(outcomes2) +  \n  geom_col(\n    mapping = aes(\n      x = proportion,                 # show pre-calculated proportion values\n      y = fct_rev(hospital),          # reverse level order so missing/other at bottom\n      fill = outcome),                # stacked by outcome\n    width = 0.5)+                    # thinner bars (out of 1)\n  theme_minimal() +                  # Minimal theme \n  theme(legend.position = \"bottom\")+\n  labs(subtitle = \"Number of recovered and dead Ebola cases, by hospital\",\n       fill = \"Outcome\",             # legend title\n       y = \"Count\",                  # y axis title\n       x = \"Hospital of admission\")+ # x axis title\n  scale_fill_manual(                 # adding colors manually\n    values = c(\"Death\"= \"#3B1c8C\",\n               \"Recover\" = \"#21908D\" )) \n\n\n\n\nNote that the proportions are binary, so we may prefer to drop ‘recover’ and just show the proportion who died. This is just for illustration purposes.\nIf using geom_col() with dates data (e.g. an epicurve from aggregated data) - you will want to adjust the width = argument to remove the “gap” lines between the bars. If using daily data set width = 1. If weekly, width = 7. Months are not possible because each month has a different number of days."
  },
  {
    "objectID": "readings/descriptive-statistics.html",
    "href": "readings/descriptive-statistics.html",
    "title": "Descriptive analysis",
    "section": "",
    "text": "This page demonstrates the use of janitor, dplyr, and base R to summarise data and create tables with descriptive statistics.\nHere we’ll learn how to create the underlying tables, whereas in the next chapter we’ll see how to nicely format and print them.\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\npacman::p_load(\n  rio,          # File import\n  here,         # File locator\n  skimr,        # get overview of data\n  tidyverse,    # data management + ggplot2 graphics \n  gtsummary,    # summary statistics and tests\n  janitor,      # adding totals and percents to tables\n  scales,       # easily convert proportions to percents  \n  flextable     # converting tables to pretty images\n  )\n\n\n\n\nWe import the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\n\n\n\n\n\n\n\n\n\nYou can use base R functions to return summary statistics on a numeric column. You can return most of the useful summary statistics for a numeric column using summary(), as below. Note that the data frame name must also be specified as shown below.\n\nsummary(linelist$age_years)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.02   23.00   84.00      86 \n\n\nYou can access and save one specific part of it with index brackets [ ]:\n\nsummary(linelist$age_years)[[2]]            # return only the 2nd element\n\n[1] 6\n\n# equivalent, alternative to above by element name\n# summary(linelist$age_years)[[\"1st Qu.\"]]  \n\nYou can return individual statistics with base R functions like max(), min(), median(), mean(), quantile(), sd(), and range().\n\n\n\n\n\n\nWarning\n\n\n\nIf your data contain missing values, R wants you to know this and so will return NA unless you specify to the above mathematical functions that you want R to ignore missing values, via the argument na.rm = TRUE.\n\n\n\n\n\n\nThe janitor packages offers the tabyl() function to produce tabulations and cross-tabulations, which can be “adorned” or modified with helper functions to display percents, proportions, counts, etc.\nBelow, we pipe the linelist data frame to janitor functions and print the result. If desired, you can also save the resulting tables with the assignment operator &lt;-.\n\n\nThe default use of tabyl() on a specific column produces the unique values, counts, and column-wise “percents” (actually proportions). The proportions may have many digits. You can adjust the number of decimals with adorn_rounding() as described below.\n\nlinelist %&gt;% tabyl(age_cat)\n\n age_cat    n     percent valid_percent\n     0-4 1095 0.185971467   0.188728025\n     5-9 1095 0.185971467   0.188728025\n   10-14  941 0.159816576   0.162185453\n   15-19  743 0.126188859   0.128059290\n   20-29 1073 0.182235054   0.184936229\n   30-49  754 0.128057065   0.129955188\n   50-69   95 0.016134511   0.016373664\n     70+    6 0.001019022   0.001034126\n    &lt;NA&gt;   86 0.014605978            NA\n\n\nAs you can see above, if there are missing values they display in a row labeled &lt;NA&gt;. You can suppress them with show_na = FALSE. If there are no missing values, this row will not appear. If there are missing values, all proportions are given as both raw (denominator inclusive of NA counts) and “valid” (denominator excludes NA counts).\nIf the column is class Factor and only certain levels are present in your data, all levels will still appear in the table. You can suppress this feature by specifying show_missing_levels = FALSE.\n\n\n\nCross-tabulation counts are achieved by adding one or more additional columns within tabyl(). Note that now only counts are returned - proportions and percents can be added with additional steps shown below.\n\nlinelist %&gt;% tabyl(age_cat, gender)\n\n age_cat   f   m NA_\n     0-4 640 416  39\n     5-9 641 412  42\n   10-14 518 383  40\n   15-19 359 364  20\n   20-29 468 575  30\n   30-49 179 557  18\n   50-69   2  91   2\n     70+   0   5   1\n    &lt;NA&gt;   0   0  86\n\n\n\n\n\nUse janitor’s “adorn” functions to add totals or convert to proportions, percents, or otherwise adjust the display. Often, you will pipe the tabyl through several of these functions.\n\n\n\n\n\n\n\nFunction\nOutcome\n\n\n\n\nadorn_totals()\nAdds totals (where = “row”, “col”, or “both”). Set name = for “Total”.\n\n\nadorn_percentages()\nConvert counts to proportions, with denominator = “row”, “col”, or “all”\n\n\nadorn_pct_formatting()\nConverts proportions to percents. Specify digits =. Remove the “%” symbol with affix_sign = FALSE.\n\n\nadorn_rounding()\nTo round proportions to digits = places. To round percents use adorn_pct_formatting() with digits =.\n\n\nadorn_ns()\nAdd counts to a table of proportions or percents. Indicate position = “rear” to show counts in parentheses, or “front” to put the percents in parentheses.\n\n\nadorn_title()\nAdd string via arguments row_name = and/or col_name =\n\n\n\nBe conscious of the order you apply the above functions. Below are some examples.\nA simple one-way table with percents instead of the default proportions.\n\nlinelist %&gt;%               # case linelist\n  tabyl(age_cat) %&gt;%       # tabulate counts and proportions by age category\n  adorn_pct_formatting()   # convert proportions to percents\n\n age_cat    n percent valid_percent\n     0-4 1095   18.6%         18.9%\n     5-9 1095   18.6%         18.9%\n   10-14  941   16.0%         16.2%\n   15-19  743   12.6%         12.8%\n   20-29 1073   18.2%         18.5%\n   30-49  754   12.8%         13.0%\n   50-69   95    1.6%          1.6%\n     70+    6    0.1%          0.1%\n    &lt;NA&gt;   86    1.5%             -\n\n\nA cross-tabulation with a total row and row percents.\n\nlinelist %&gt;%                                  \n  tabyl(age_cat, gender) %&gt;%                  # counts by age and gender\n  adorn_totals(where = \"row\") %&gt;%             # add total row\n  adorn_percentages(denominator = \"row\") %&gt;%  # convert counts to proportions\n  adorn_pct_formatting(digits = 1)            # convert proportions to percents\n\n age_cat     f     m    NA_\n     0-4 58.4% 38.0%   3.6%\n     5-9 58.5% 37.6%   3.8%\n   10-14 55.0% 40.7%   4.3%\n   15-19 48.3% 49.0%   2.7%\n   20-29 43.6% 53.6%   2.8%\n   30-49 23.7% 73.9%   2.4%\n   50-69  2.1% 95.8%   2.1%\n     70+  0.0% 83.3%  16.7%\n    &lt;NA&gt;  0.0%  0.0% 100.0%\n   Total 47.7% 47.6%   4.7%\n\n\nA cross-tabulation adjusted so that both counts and percents are displayed.\n\nlinelist %&gt;%                                  # case linelist\n  tabyl(age_cat, gender) %&gt;%                  # cross-tabulate counts\n  adorn_totals(where = \"row\") %&gt;%             # add a total row\n  adorn_percentages(denominator = \"col\") %&gt;%  # convert to proportions\n  adorn_pct_formatting() %&gt;%                  # convert to percents\n  adorn_ns(position = \"front\") %&gt;%            # display as: \"count (percent)\"\n  adorn_title(                                # adjust titles\n    row_name = \"Age Category\",\n    col_name = \"Gender\")\n\n                      Gender                            \n Age Category              f              m          NA_\n          0-4   640  (22.8%)   416  (14.8%)  39  (14.0%)\n          5-9   641  (22.8%)   412  (14.7%)  42  (15.1%)\n        10-14   518  (18.5%)   383  (13.7%)  40  (14.4%)\n        15-19   359  (12.8%)   364  (13.0%)  20   (7.2%)\n        20-29   468  (16.7%)   575  (20.5%)  30  (10.8%)\n        30-49   179   (6.4%)   557  (19.9%)  18   (6.5%)\n        50-69     2   (0.1%)    91   (3.2%)   2   (0.7%)\n          70+     0   (0.0%)     5   (0.2%)   1   (0.4%)\n         &lt;NA&gt;     0   (0.0%)     0   (0.0%)  86  (30.9%)\n        Total 2,807 (100.0%) 2,803 (100.0%) 278 (100.0%)\n\n\n\n\n\nBy default, the tabyl will print raw to your R console.\nAlternatively, you can pass the tabyl to flextable or similar package to print as a “pretty” image in the RStudio Viewer, which could be exported as .png, .jpeg, .html, etc. Note that if printing in this manner and using adorn_titles(), you must specify placement = \"combined\".\n\nlinelist %&gt;%\n  tabyl(age_cat, gender) %&gt;% \n  adorn_totals(where = \"col\") %&gt;% \n  adorn_percentages(denominator = \"col\") %&gt;% \n  adorn_pct_formatting() %&gt;% \n  adorn_ns(position = \"front\") %&gt;% \n  adorn_title(\n    row_name = \"Age Category\",\n    col_name = \"Gender\",\n    placement = \"combined\") %&gt;% # this is necessary to print as image\n  flextable::flextable() %&gt;%    # convert to pretty image\n  flextable::autofit()          # format to one line per row \n\n\nAge Category/GenderfmNA_Total0-4640 (22.8%)416 (14.8%)39 (14.0%)1,095 (18.6%)5-9641 (22.8%)412 (14.7%)42 (15.1%)1,095 (18.6%)10-14518 (18.5%)383 (13.7%)40 (14.4%)941 (16.0%)15-19359 (12.8%)364 (13.0%)20  (7.2%)743 (12.6%)20-29468 (16.7%)575 (20.5%)30 (10.8%)1,073 (18.2%)30-49179  (6.4%)557 (19.9%)18  (6.5%)754 (12.8%)50-692  (0.1%)91  (3.2%)2  (0.7%)95  (1.6%)70+0  (0.0%)5  (0.2%)1  (0.4%)6  (0.1%)0  (0.0%)0  (0.0%)86 (30.9%)86  (1.5%)\n\n\n\n\n\nYou can use janitor’s adorn_*() functions on other tables, such as those created by summarise() and count() from dplyr, or table() from base R. Simply pipe the table to the desired janitor function. For example:\n\nlinelist %&gt;% \n  count(hospital) %&gt;%   # dplyr function\n  adorn_totals()        # janitor function\n\n                             hospital    n\n                     Central Hospital  454\n                    Military Hospital  896\n                              Missing 1469\n                                Other  885\n                        Port Hospital 1762\n St. Mark's Maternity Hospital (SMMH)  422\n                                Total 5888\n\n\n\n\n\nIf you convert the table to a “pretty” image with a package like flextable, you can save it with functions from that package - like save_as_html(), save_as_word(), save_as_ppt(), and save_as_image() from flextable. Below, the table is saved as a Word document, in which it can be further hand-edited.\n\nlinelist %&gt;%\n  tabyl(age_cat, gender) %&gt;% \n  adorn_totals(where = \"col\") %&gt;% \n  adorn_percentages(denominator = \"col\") %&gt;% \n  adorn_pct_formatting() %&gt;% \n  adorn_ns(position = \"front\") %&gt;% \n  adorn_title(\n    row_name = \"Age Category\",\n    col_name = \"Gender\",\n    placement = \"combined\") %&gt;% \n  flextable::flextable() %&gt;%                     # convert to image\n  flextable::autofit() %&gt;%                       # ensure only one line per row\n  flextable::save_as_docx(path = \"tabyl.docx\")   # save as Word document to filepath\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can apply statistical tests on tabyls, like chisq.test() or fisher.test() from the stats package, as shown below. Note missing values are not allowed so they are excluded from the tabyl with show_na = FALSE.\n\nage_by_outcome &lt;- linelist %&gt;% \n  tabyl(age_cat, outcome, show_na = FALSE) \n\nchisq.test(age_by_outcome)\n\n\n    Pearson's Chi-squared test\n\ndata:  age_by_outcome\nX-squared = 6.4931, df = 7, p-value = 0.4835\n\n\n\n\n\n\ndplyr is part of the tidyverse packages and is an very common data management tool. Creating tables with dplyr functions summarise() and count() is a useful approach to calculating summary statistics, summarize by group, or pass tables to ggplot().\nsummarise() creates a new, summary data frame. If the data are ungrouped, it will return a one-row dataframe with the specified summary statistics of the entire data frame. If the data are grouped, the new data frame will have one row per group.\nWithin the summarise() parentheses, you provide the names of each new summary column followed by an equals sign and a statistical function to apply.\n\n\n\n\n\n\nTip\n\n\n\nThe summarise function works with both UK and US spelling (summarise() and summarize()).\n\n\n\n\nThe most simple function to apply within summarise() is n(). Leave the parentheses empty to count the number of rows.\n\nlinelist %&gt;%                 # begin with linelist\n  summarise(n_rows = n())    # return new summary dataframe with column n_rows\n\n  n_rows\n1   5888\n\n\nThis gets more interesting if we have grouped the data beforehand.\n\nlinelist %&gt;% \n  group_by(age_cat) %&gt;%     # group data by unique values in column age_cat\n  summarise(n_rows = n())   # return number of rows *per group*\n\n# A tibble: 9 × 2\n  age_cat n_rows\n  &lt;fct&gt;    &lt;int&gt;\n1 0-4       1095\n2 5-9       1095\n3 10-14      941\n4 15-19      743\n5 20-29     1073\n6 30-49      754\n7 50-69       95\n8 70+          6\n9 &lt;NA&gt;        86\n\n\nThe above command can be shortened by using the count() function instead. count() does the following:\n\nGroups the data by the columns provided to it\n\nSummarises them with n() (creating column n)\n\nUn-groups the data\n\n\nlinelist %&gt;% \n  count(age_cat)\n\n  age_cat    n\n1     0-4 1095\n2     5-9 1095\n3   10-14  941\n4   15-19  743\n5   20-29 1073\n6   30-49  754\n7   50-69   95\n8     70+    6\n9    &lt;NA&gt;   86\n\n\nYou can change the name of the counts column from the default n to something else by specifying it to name =.\nTabulating counts of two or more grouping columns are still returned in “long” format, with the counts in the n column.\n\nlinelist %&gt;% \n  count(age_cat, outcome)\n\n   age_cat outcome   n\n1      0-4   Death 471\n2      0-4 Recover 364\n3      0-4    &lt;NA&gt; 260\n4      5-9   Death 476\n5      5-9 Recover 391\n6      5-9    &lt;NA&gt; 228\n7    10-14   Death 438\n8    10-14 Recover 303\n9    10-14    &lt;NA&gt; 200\n10   15-19   Death 323\n11   15-19 Recover 251\n12   15-19    &lt;NA&gt; 169\n13   20-29   Death 477\n14   20-29 Recover 367\n15   20-29    &lt;NA&gt; 229\n16   30-49   Death 329\n17   30-49 Recover 238\n18   30-49    &lt;NA&gt; 187\n19   50-69   Death  33\n20   50-69 Recover  38\n21   50-69    &lt;NA&gt;  24\n22     70+   Death   3\n23     70+ Recover   3\n24    &lt;NA&gt;   Death  32\n25    &lt;NA&gt; Recover  28\n26    &lt;NA&gt;    &lt;NA&gt;  26\n\n\n\n\n\nIf you are tabling a column of class factor you can ensure that all levels are shown (not just the levels with values in the data) by adding .drop = FALSE into the summarise() or count() command.\nThis technique is useful to standardise your tables/plots. For example if you are creating figures for multiple sub-groups, or repeatedly creating the figure for routine reports. In each of these circumstances, the presence of values in the data may fluctuate, but you can define levels that remain constant.\n\n\n\nProportions can be added by piping the table to mutate() to create a new column. Define the new column as the counts column (n by default) divided by the sum() of the counts column (this will return a proportion).\nNote that in this case, sum() in the mutate() command will return the sum of the whole column n for use as the proportion denominator. If sum() is used in grouped data (e.g. if the mutate() immediately followed a group_by() command), it will return sums by group. As stated just above, count() finishes its actions by ungrouping. Thus, in this scenario we get full column proportions.\nTo easily display percents, you can wrap the proportion in the function percent() from the package scales (note this convert to class character).\n\nage_summary &lt;- linelist %&gt;% \n  count(age_cat) %&gt;%                     # group and count by gender (produces \"n\" column)\n  mutate(                                # create percent of column - note the denominator\n    percent = scales::percent(n / sum(n))) \n\n# print\nage_summary\n\n  age_cat    n percent\n1     0-4 1095  18.60%\n2     5-9 1095  18.60%\n3   10-14  941  15.98%\n4   15-19  743  12.62%\n5   20-29 1073  18.22%\n6   30-49  754  12.81%\n7   50-69   95   1.61%\n8     70+    6   0.10%\n9    &lt;NA&gt;   86   1.46%\n\n\nBelow is a method to calculate proportions within groups. It relies on different levels of data grouping being selectively applied and removed. First, the data are grouped on outcome via group_by(). Then, count() is applied. This function further groups the data by age_cat and returns counts for each outcome-age-cat combination. Importantly - as it finishes its process, count() also ungroups the age_cat grouping, so the only remaining data grouping is the original grouping by outcome. Thus, the final step of calculating proportions (denominator sum(n)) is still grouped by outcome.\n\nage_by_outcome &lt;- linelist %&gt;%                  # begin with linelist\n  group_by(outcome) %&gt;%                         # group by outcome \n  count(age_cat) %&gt;%                            # group and count by age_cat, and then remove age_cat grouping\n  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group\n\n\n\n\n\n\n\n\n\n\n\nTo display a “long” table output like the above with ggplot() is relatively straight-forward. The data are naturally in “long” format, which is naturally accepted by ggplot().\n\nlinelist %&gt;%                      # begin with linelist\n  count(age_cat, outcome) %&gt;%     # group and tabulate counts by two columns\n  ggplot()+                       # pass new data frame to ggplot\n    geom_col(                     # create bar plot\n      mapping = aes(   \n        x = outcome,              # map outcome to x-axis\n        fill = age_cat,           # map age_cat to the fill\n        y = n))                   # map the counts column `n` to the height\n\n\n\n\n\n\n\nOne major advantage of dplyr and summarise() is the ability to return more advanced statistical summaries like median(), mean(), max(), min(), sd() (standard deviation), and percentiles. You can also use sum() to return the number of rows that meet certain logical criteria. As above, these outputs can be produced for the whole data frame set, or by group.\nThe syntax is the same - within the summarise() parentheses you provide the names of each new summary column followed by an equals sign and a statistical function to apply. Within the statistical function, give the column(s) to be operated on and any relevant arguments (e.g. na.rm = TRUE for most mathematical functions).\nYou can also use sum() to return the number of rows that meet a logical criteria. The expression within is counted if it evaluates to TRUE. For example:\n\nsum(age_years &lt; 18, na.rm=T)\n\nsum(gender == \"male\", na.rm=T)\n\nsum(response %in% c(\"Likely\", \"Very Likely\"))\n\nBelow, linelist data are summarised to describe the days delay from symptom onset to hospital admission (column days_onset_hosp), by hospital.\n\nsummary_table &lt;- linelist %&gt;%                                        # begin with linelist, save out as new object\n  group_by(hospital) %&gt;%                                             # group all calculations by hospital\n  summarise(                                                         # only the below summary columns will be returned\n    cases       = n(),                                                # number of rows per group\n    delay_max   = max(days_onset_hosp, na.rm = T),                    # max delay\n    delay_mean  = round(mean(days_onset_hosp, na.rm=T), digits = 1),  # mean delay, rounded\n    delay_sd    = round(sd(days_onset_hosp, na.rm = T), digits = 1),  # standard deviation of delays, rounded\n    delay_3     = sum(days_onset_hosp &gt;= 3, na.rm = T),               # number of rows with delay of 3 or more days\n    pct_delay_3 = scales::percent(delay_3 / cases)                    # convert previously-defined delay column to percent \n  )\n\nsummary_table  # print\n\n# A tibble: 6 × 7\n  hospital               cases delay_max delay_mean delay_sd delay_3 pct_delay_3\n  &lt;chr&gt;                  &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;      \n1 Central Hospital         454        12        1.9      1.9     108 24%        \n2 Military Hospital        896        15        2.1      2.4     253 28%        \n3 Missing                 1469        22        2.1      2.3     399 27%        \n4 Other                    885        18        2        2.2     234 26%        \n5 Port Hospital           1762        16        2.1      2.2     470 27%        \n6 St. Mark's Maternity …   422        18        2.1      2.3     116 27%        \n\n\nSome tips:\n\nUse sum() with a logic statement to “count” rows that meet certain criteria (==)\n\nNote the use of na.rm = TRUE within mathematical functions like sum(), otherwise NA will be returned if there are any missing values\n\nUse the function percent() from the scales package to easily convert to percents\n\nSet accuracy = to 0.1 or 0.01 to ensure 1 or 2 decimal places respectively\n\n\nUse round() from base R to specify decimals\n\nTo calculate these statistics on the entire dataset, use summarise() without group_by()\n\nYou may create columns for the purposes of later calculations (e.g. denominators) that you eventually drop from your data frame with select().\n\n\n\n\nYou may want to return conditional statistics - e.g. the maximum of rows that meet certain criteria. This can be done by subsetting the column with brackets [ ]. The example below returns the maximum temperature for patients classified having or not having fever. Be aware however - it may be more appropriate to add another column to the group_by() command and pivot_wider() (as demonstrated below).\n\nlinelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    max_temp_fvr = max(temp[fever == \"yes\"], na.rm = T),\n    max_temp_no = max(temp[fever == \"no\"], na.rm = T)\n  )\n\n# A tibble: 6 × 3\n  hospital                             max_temp_fvr max_temp_no\n  &lt;chr&gt;                                       &lt;dbl&gt;       &lt;dbl&gt;\n1 Central Hospital                             40.4        38  \n2 Military Hospital                            40.5        38  \n3 Missing                                      40.6        38  \n4 Other                                        40.8        37.9\n5 Port Hospital                                40.6        38  \n6 St. Mark's Maternity Hospital (SMMH)         40.6        37.9\n\n\n\n\n\nThe function str_glue() from stringr is useful to combine values from several columns into one new column. In this context this is typically used after the summarise() command.\nBelow, the summary_table data frame (created above) is mutated such that columns delay_mean and delay_sd are combined, parentheses formating is added to the new column, and their respective old columns are removed.\nThen, to make the table more presentable, a total row is added with adorn_totals() from janitor (which ignores non-numeric columns). Lastly, we use select() from dplyr to both re-order and rename to nicer column names.\nNow you could pass to flextable and print the table to Word, .png, .jpeg, .html, Powerpoint, RMarkdown, etc.! .\n\nsummary_table %&gt;% \n  mutate(delay = str_glue(\"{delay_mean} ({delay_sd})\")) %&gt;%  # combine and format other values\n  select(-c(delay_mean, delay_sd)) %&gt;%                       # remove two old columns   \n  adorn_totals(where = \"row\") %&gt;%                            # add total row\n  select(                                                    # order and rename cols\n    \"Hospital Name\"   = hospital,\n    \"Cases\"           = cases,\n    \"Max delay\"       = delay_max,\n    \"Mean (sd)\"       = delay,\n    \"Delay 3+ days\"   = delay_3,\n    \"% delay 3+ days\" = pct_delay_3\n    )\n\n                        Hospital Name Cases Max delay Mean (sd) Delay 3+ days\n                     Central Hospital   454        12 1.9 (1.9)           108\n                    Military Hospital   896        15 2.1 (2.4)           253\n                              Missing  1469        22 2.1 (2.3)           399\n                                Other   885        18   2 (2.2)           234\n                        Port Hospital  1762        16 2.1 (2.2)           470\n St. Mark's Maternity Hospital (SMMH)   422        18 2.1 (2.3)           116\n                                Total  5888       101         -          1580\n % delay 3+ days\n             24%\n             28%\n             27%\n             26%\n             27%\n             27%\n               -\n\n\n\n\nPercentiles and quantiles in dplyr deserve a special mention. To return quantiles, use quantile() with the defaults or specify the value(s) you would like with probs =.\n\n# get default percentile values of age (0%, 25%, 50%, 75%, 100%)\nlinelist %&gt;% \n  summarise(age_percentiles = quantile(age_years, na.rm = TRUE))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n  age_percentiles\n1               0\n2               6\n3              13\n4              23\n5              84\n\n# get manually-specified percentile values of age (5%, 50%, 75%, 98%)\nlinelist %&gt;% \n  summarise(\n    age_percentiles = quantile(\n      age_years,\n      probs = c(.05, 0.5, 0.75, 0.98), \n      na.rm=TRUE)\n    )\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n  age_percentiles\n1               1\n2              13\n3              23\n4              48\n\n\nIf you want to return quantiles by group, you may encounter long and less useful outputs if you simply add another column to group_by(). So, try this approach instead - create a column for each quantile level desired.\n\n# get manually-specified percentile values of age (5%, 50%, 75%, 98%)\nlinelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    p05 = quantile(age_years, probs = 0.05, na.rm=T),\n    p50 = quantile(age_years, probs = 0.5, na.rm=T),\n    p75 = quantile(age_years, probs = 0.75, na.rm=T),\n    p98 = quantile(age_years, probs = 0.98, na.rm=T)\n    )\n\n# A tibble: 6 × 5\n  hospital                               p05   p50   p75   p98\n  &lt;chr&gt;                                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Central Hospital                         1    12    21  48  \n2 Military Hospital                        1    13    24  45  \n3 Missing                                  1    13    23  48.2\n4 Other                                    1    13    23  50  \n5 Port Hospital                            1    14    24  49  \n6 St. Mark's Maternity Hospital (SMMH)     2    12    22  50.2\n\n\n\n\n\n\nIf you begin with aggregated data, using n() return the number of rows, not the sum of the aggregated counts. To get sums, use sum() on the data’s counts column.\nFor example, let’s say you are beginning with the data frame of counts below, called linelist_agg - it shows in “long” format the case counts by outcome and gender.\nBelow we create this example data frame of linelist case counts by outcome and gender (missing values removed for clarity).\n\nlinelist_agg &lt;- linelist %&gt;% \n  drop_na(gender, outcome) %&gt;% \n  count(outcome, gender)\n\nlinelist_agg\n\n  outcome gender    n\n1   Death      f 1227\n2   Death      m 1228\n3 Recover      f  953\n4 Recover      m  950\n\n\nTo sum the counts (in column n) by group you can use summarise() but set the new column equal to sum(n, na.rm=T). To add a conditional element to the sum operation, you can use the subset bracket [ ] syntax on the counts column.\n\nlinelist_agg %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(\n    total_cases  = sum(n, na.rm=T),\n    male_cases   = sum(n[gender == \"m\"], na.rm=T),\n    female_cases = sum(n[gender == \"f\"], na.rm=T))\n\n# A tibble: 2 × 4\n  outcome total_cases male_cases female_cases\n  &lt;chr&gt;         &lt;int&gt;      &lt;int&gt;        &lt;int&gt;\n1 Death          2455       1228         1227\n2 Recover        1903        950          953\n\n\n\n\n\nYou can use summarise() across multiple columns using across(). This makes life easier when you want to calculate the same statistics for many columns. Place across() within summarise() and specify the following:\n\n.cols = as either a vector of column names c() or “tidyselect” helper functions (explained below)\n\n.fns = the function to perform (no parentheses) - you can provide multiple within a list()\n\nBelow, mean() is applied to several numeric columns. A vector of columns are named explicitly to .cols = and a single function mean is specified (no parentheses) to .fns =. Any additional arguments for the function (e.g. na.rm=TRUE) are provided after .fns =, separated by a comma.\nIt can be difficult to get the order of parentheses and commas correct when using across(). Remember that within across() you must include the columns, the functions, and any extra arguments needed for the functions.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # columns\n                   .fns = mean,                               # function\n                   na.rm=T))                                  # extra arguments\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(...)`.\nℹ In group 1: `outcome = \"Death\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 3 × 5\n  outcome age_years  temp wt_kg ht_cm\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Death        15.9  38.6  52.6  125.\n2 Recover      16.1  38.6  52.5  125.\n3 &lt;NA&gt;         16.2  38.6  53.0  125.\n\n\nMultiple functions can be run at once. Below the functions mean and sd are provided to .fns = within a list(). You have the opportunity to provide character names (e.g. “mean” and “sd”) which are appended in the new column names.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # columns\n                   .fns = list(\"mean\" = mean, \"sd\" = sd),    # multiple functions \n                   na.rm=T))                                 # extra arguments\n\n# A tibble: 3 × 9\n  outcome age_years_mean age_years_sd temp_mean temp_sd wt_kg_mean wt_kg_sd\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 Death             15.9         12.3      38.6   0.962       52.6     18.4\n2 Recover           16.1         13.0      38.6   0.997       52.5     18.6\n3 &lt;NA&gt;              16.2         12.8      38.6   0.976       53.0     18.9\n# ℹ 2 more variables: ht_cm_mean &lt;dbl&gt;, ht_cm_sd &lt;dbl&gt;\n\n\nHere are those “tidyselect” helper functions you can provide to .cols = to select columns:\n\neverything() - all other columns not mentioned\n\nlast_col() - the last column\n\nwhere() - applies a function to all columns and selects those which are TRUE\n\nstarts_with() - matches to a specified prefix. Example: starts_with(\"date\")\nends_with() - matches to a specified suffix. Example: ends_with(\"_end\")\n\ncontains() - columns containing a character string. Example: contains(\"time\")\nmatches() - to apply a regular expression (regex). Example: contains(\"[pt]al\")\n\nnum_range() -\nany_of() - matches if column is named. Useful if the name might not exist. Example: any_of(date_onset, date_death, cardiac_arrest)\n\nFor example, to return the mean of every numeric column use where() and provide the function as.numeric() (without parentheses). All this remains within the across() command.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(\n    .cols = where(is.numeric),  # all numeric columns in the data frame\n    .fns = mean,\n    na.rm=T))\n\n# A tibble: 3 × 12\n  outcome generation   age age_years   lon   lat wt_kg ht_cm ct_blood  temp\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Death         16.7  15.9      15.9 -13.2  8.47  52.6  125.     21.3  38.6\n2 Recover       16.4  16.2      16.1 -13.2  8.47  52.5  125.     21.1  38.6\n3 &lt;NA&gt;          16.5  16.3      16.2 -13.2  8.47  53.0  125.     21.2  38.6\n# ℹ 2 more variables: bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\n\n\n\nWhen summarise() operates on grouped data it does not automatically produce “total” statistics. Below, two approaches to adding a total row are presented:\n\n\nIf your table consists only of counts or proportions/percents that can be summed into a total, then you can add sum totals using janitor’s adorn_totals() as described in the section above. Note that this function can only sum the numeric columns - if you want to calculate other total summary statistics see the next approach with dplyr.\nBelow, linelist is grouped by gender and summarised into a table that described the number of cases with known outcome, deaths, and recovered. Piping the table to adorn_totals() adds a total row at the bottom reflecting the sum of each column. The further adorn_*() functions adjust the display as noted in the code.\n\nlinelist %&gt;% \n  group_by(gender) %&gt;%\n  summarise(\n    known_outcome = sum(!is.na(outcome)),           # Number of rows in group where outcome is not missing\n    n_death  = sum(outcome == \"Death\", na.rm=T),    # Number of rows in group where outcome is Death\n    n_recover = sum(outcome == \"Recover\", na.rm=T), # Number of rows in group where outcome is Recovered\n  ) %&gt;% \n  adorn_totals() %&gt;%                                # Adorn total row (sums of each numeric column)\n  adorn_percentages(\"col\") %&gt;%                      # Get column proportions\n  adorn_pct_formatting() %&gt;%                        # Convert proportions to percents\n  adorn_ns(position = \"front\")                      # display % and counts (with counts in front)\n\n gender  known_outcome        n_death      n_recover\n      f 2,180  (47.8%) 1,227  (47.5%)   953  (48.1%)\n      m 2,178  (47.7%) 1,228  (47.6%)   950  (47.9%)\n   &lt;NA&gt;   207   (4.5%)   127   (4.9%)    80   (4.0%)\n  Total 4,565 (100.0%) 2,582 (100.0%) 1,983 (100.0%)\n\n\n\n\n\nIf your table consists of summary statistics such as median(), mean(), etc, the adorn_totals() approach shown above will not be sufficient. Instead, to get summary statistics for the entire dataset you must calculate them with a separate summarise() command and then bind the results to the original grouped summary table. You can make a summary table of outcome by hospital with group_by() and summarise() like this:\n\nby_hospital &lt;- linelist %&gt;% \n  filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%  # Remove cases with missing outcome or hospital\n  group_by(hospital, outcome) %&gt;%                      # Group data\n  summarise(                                           # Create new summary columns of indicators of interest\n    N = n(),                                            # Number of rows per hospital-outcome group     \n    ct_value = median(ct_blood, na.rm=T))               # median CT value per group\n  \nby_hospital # print table\n\n# A tibble: 10 × 4\n# Groups:   hospital [5]\n   hospital                             outcome     N ct_value\n   &lt;chr&gt;                                &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;\n 1 Central Hospital                     Death     193       22\n 2 Central Hospital                     Recover   165       22\n 3 Military Hospital                    Death     399       21\n 4 Military Hospital                    Recover   309       22\n 5 Other                                Death     395       22\n 6 Other                                Recover   290       21\n 7 Port Hospital                        Death     785       22\n 8 Port Hospital                        Recover   579       21\n 9 St. Mark's Maternity Hospital (SMMH) Death     199       22\n10 St. Mark's Maternity Hospital (SMMH) Recover   126       22\n\n\nTo get the totals, run the same summarise() command but only group the data by outcome (not by hospital), like this:\n\ntotals &lt;- linelist %&gt;% \n      filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%\n      group_by(outcome) %&gt;%                            # Grouped only by outcome, not by hospital    \n      summarise(\n        N = n(),                                       # These statistics are now by outcome only     \n        ct_value = median(ct_blood, na.rm=T))\n\ntotals # print table\n\n# A tibble: 2 × 3\n  outcome     N ct_value\n  &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 Death    1971       22\n2 Recover  1469       22\n\n\nWe can bind these two data frames together. Note that by_hospital has 4 columns whereas totals has 3 columns. By using bind_rows(), the columns are combined by name, and any extra space is filled in with NA (e.g the column hospital values for the two new totals rows). After binding the rows, we convert these empty spaces to “Total” using replace_na().\n\ntable_long &lt;- bind_rows(by_hospital, totals) %&gt;% \n  mutate(hospital = replace_na(hospital, \"Total\"))\n\nHere is the new table with “Total” rows at the bottom.\n\n\n\n\n\n\n\nThis table is in a “long” format, which may be what you want. Optionally, you can pivot this table wider to make it more readable (which we will learn about later). You can also add more columns, and arrange it nicely. This code is below.\n\ntable_long %&gt;% \n  \n  # Pivot wider and format\n  ########################\n  mutate(hospital = replace_na(hospital, \"Total\")) %&gt;% \n  pivot_wider(                                         # Pivot from long to wide\n    values_from = c(ct_value, N),                       # new values are from ct and count columns\n    names_from = outcome) %&gt;%                           # new column names are from outcomes\n  mutate(                                              # Add new columns\n    N_Known = N_Death + N_Recover,                               # number with known outcome\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # percent who recovered (to 1 decimal)\n  select(                                              # Re-order columns\n    hospital, N_Known,                                   # Intro columns\n    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns\n    N_Death, Pct_Death, ct_value_Death)  %&gt;%             # Death columns\n  arrange(N_Known)                                  # Arrange rows from lowest to highest (Total row at bottom)\n\n# A tibble: 6 × 8\n# Groups:   hospital [6]\n  hospital      N_Known N_Recover Pct_Recover ct_value_Recover N_Death Pct_Death\n  &lt;chr&gt;           &lt;int&gt;     &lt;int&gt; &lt;chr&gt;                  &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;    \n1 St. Mark's M…     325       126 38.8%                     22     199 61.2%    \n2 Central Hosp…     358       165 46.1%                     22     193 53.9%    \n3 Other             685       290 42.3%                     21     395 57.7%    \n4 Military Hos…     708       309 43.6%                     22     399 56.4%    \n5 Port Hospital    1364       579 42.4%                     21     785 57.6%    \n6 Total            3440      1469 42.7%                     22    1971 57.3%    \n# ℹ 1 more variable: ct_value_Death &lt;dbl&gt;\n\n\nIn the next chapter we’ll see how to create an attractive visualization of the table, as shown here:\n\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\n\n\nYou can use the function table() to tabulate and cross-tabulate columns. Unlike the options above, you must specify the dataframe each time you reference a column name, as shown below.\n\n\n\n\n\n\nWarning\n\n\n\nNA (missing) values will not be tabulated unless you include the argument useNA = \"always\" (which could also be set to “no” or “ifany”).\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the %$% from magrittr to remove the need for repeating data frame calls within base functions. For example the below could be written linelist %$% table(outcome, useNA = \"always\")\n\n\n\ntable(linelist$outcome, useNA = \"always\")\n\n\n  Death Recover    &lt;NA&gt; \n   2582    1983    1323 \n\n\nMultiple columns can be cross-tabulated by listing them one after the other, separated by commas. Optionally, you can assign each column a “name” like Outcome = linelist$outcome.\n\nage_by_outcome &lt;- table(linelist$age_cat, linelist$outcome, useNA = \"always\") # save table as object\nage_by_outcome   # print table\n\n       \n        Death Recover &lt;NA&gt;\n  0-4     471     364  260\n  5-9     476     391  228\n  10-14   438     303  200\n  15-19   323     251  169\n  20-29   477     367  229\n  30-49   329     238  187\n  50-69    33      38   24\n  70+       3       3    0\n  &lt;NA&gt;     32      28   26\n\n\n\n\nTo return proportions, passing the above table to the function prop.table(). Use the margins = argument to specify whether you want the proportions to be of rows (1), of columns (2), or of the whole table (3). For clarity, we pipe the table to the round() function from base R, specifying 2 digits.\n\n# get proportions of table defined above, by rows, rounded\nprop.table(age_by_outcome, 1) %&gt;% round(2)\n\n       \n        Death Recover &lt;NA&gt;\n  0-4    0.43    0.33 0.24\n  5-9    0.43    0.36 0.21\n  10-14  0.47    0.32 0.21\n  15-19  0.43    0.34 0.23\n  20-29  0.44    0.34 0.21\n  30-49  0.44    0.32 0.25\n  50-69  0.35    0.40 0.25\n  70+    0.50    0.50 0.00\n  &lt;NA&gt;   0.37    0.33 0.30\n\n\n\n\n\nTo add row and column totals, pass the table to addmargins(). This works for both counts and proportions.\n\naddmargins(age_by_outcome)\n\n       \n        Death Recover &lt;NA&gt;  Sum\n  0-4     471     364  260 1095\n  5-9     476     391  228 1095\n  10-14   438     303  200  941\n  15-19   323     251  169  743\n  20-29   477     367  229 1073\n  30-49   329     238  187  754\n  50-69    33      38   24   95\n  70+       3       3    0    6\n  &lt;NA&gt;     32      28   26   86\n  Sum    2582    1983 1323 5888"
  },
  {
    "objectID": "readings/descriptive-statistics.html#browse-data",
    "href": "readings/descriptive-statistics.html#browse-data",
    "title": "Descriptive analysis",
    "section": "",
    "text": "You can use base R functions to return summary statistics on a numeric column. You can return most of the useful summary statistics for a numeric column using summary(), as below. Note that the data frame name must also be specified as shown below.\n\nsummary(linelist$age_years)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.02   23.00   84.00      86 \n\n\nYou can access and save one specific part of it with index brackets [ ]:\n\nsummary(linelist$age_years)[[2]]            # return only the 2nd element\n\n[1] 6\n\n# equivalent, alternative to above by element name\n# summary(linelist$age_years)[[\"1st Qu.\"]]  \n\nYou can return individual statistics with base R functions like max(), min(), median(), mean(), quantile(), sd(), and range().\n\n\n\n\n\n\nWarning\n\n\n\nIf your data contain missing values, R wants you to know this and so will return NA unless you specify to the above mathematical functions that you want R to ignore missing values, via the argument na.rm = TRUE."
  },
  {
    "objectID": "readings/descriptive-statistics.html#tbl_janitor",
    "href": "readings/descriptive-statistics.html#tbl_janitor",
    "title": "Descriptive analysis",
    "section": "",
    "text": "The janitor packages offers the tabyl() function to produce tabulations and cross-tabulations, which can be “adorned” or modified with helper functions to display percents, proportions, counts, etc.\nBelow, we pipe the linelist data frame to janitor functions and print the result. If desired, you can also save the resulting tables with the assignment operator &lt;-.\n\n\nThe default use of tabyl() on a specific column produces the unique values, counts, and column-wise “percents” (actually proportions). The proportions may have many digits. You can adjust the number of decimals with adorn_rounding() as described below.\n\nlinelist %&gt;% tabyl(age_cat)\n\n age_cat    n     percent valid_percent\n     0-4 1095 0.185971467   0.188728025\n     5-9 1095 0.185971467   0.188728025\n   10-14  941 0.159816576   0.162185453\n   15-19  743 0.126188859   0.128059290\n   20-29 1073 0.182235054   0.184936229\n   30-49  754 0.128057065   0.129955188\n   50-69   95 0.016134511   0.016373664\n     70+    6 0.001019022   0.001034126\n    &lt;NA&gt;   86 0.014605978            NA\n\n\nAs you can see above, if there are missing values they display in a row labeled &lt;NA&gt;. You can suppress them with show_na = FALSE. If there are no missing values, this row will not appear. If there are missing values, all proportions are given as both raw (denominator inclusive of NA counts) and “valid” (denominator excludes NA counts).\nIf the column is class Factor and only certain levels are present in your data, all levels will still appear in the table. You can suppress this feature by specifying show_missing_levels = FALSE.\n\n\n\nCross-tabulation counts are achieved by adding one or more additional columns within tabyl(). Note that now only counts are returned - proportions and percents can be added with additional steps shown below.\n\nlinelist %&gt;% tabyl(age_cat, gender)\n\n age_cat   f   m NA_\n     0-4 640 416  39\n     5-9 641 412  42\n   10-14 518 383  40\n   15-19 359 364  20\n   20-29 468 575  30\n   30-49 179 557  18\n   50-69   2  91   2\n     70+   0   5   1\n    &lt;NA&gt;   0   0  86\n\n\n\n\n\nUse janitor’s “adorn” functions to add totals or convert to proportions, percents, or otherwise adjust the display. Often, you will pipe the tabyl through several of these functions.\n\n\n\n\n\n\n\nFunction\nOutcome\n\n\n\n\nadorn_totals()\nAdds totals (where = “row”, “col”, or “both”). Set name = for “Total”.\n\n\nadorn_percentages()\nConvert counts to proportions, with denominator = “row”, “col”, or “all”\n\n\nadorn_pct_formatting()\nConverts proportions to percents. Specify digits =. Remove the “%” symbol with affix_sign = FALSE.\n\n\nadorn_rounding()\nTo round proportions to digits = places. To round percents use adorn_pct_formatting() with digits =.\n\n\nadorn_ns()\nAdd counts to a table of proportions or percents. Indicate position = “rear” to show counts in parentheses, or “front” to put the percents in parentheses.\n\n\nadorn_title()\nAdd string via arguments row_name = and/or col_name =\n\n\n\nBe conscious of the order you apply the above functions. Below are some examples.\nA simple one-way table with percents instead of the default proportions.\n\nlinelist %&gt;%               # case linelist\n  tabyl(age_cat) %&gt;%       # tabulate counts and proportions by age category\n  adorn_pct_formatting()   # convert proportions to percents\n\n age_cat    n percent valid_percent\n     0-4 1095   18.6%         18.9%\n     5-9 1095   18.6%         18.9%\n   10-14  941   16.0%         16.2%\n   15-19  743   12.6%         12.8%\n   20-29 1073   18.2%         18.5%\n   30-49  754   12.8%         13.0%\n   50-69   95    1.6%          1.6%\n     70+    6    0.1%          0.1%\n    &lt;NA&gt;   86    1.5%             -\n\n\nA cross-tabulation with a total row and row percents.\n\nlinelist %&gt;%                                  \n  tabyl(age_cat, gender) %&gt;%                  # counts by age and gender\n  adorn_totals(where = \"row\") %&gt;%             # add total row\n  adorn_percentages(denominator = \"row\") %&gt;%  # convert counts to proportions\n  adorn_pct_formatting(digits = 1)            # convert proportions to percents\n\n age_cat     f     m    NA_\n     0-4 58.4% 38.0%   3.6%\n     5-9 58.5% 37.6%   3.8%\n   10-14 55.0% 40.7%   4.3%\n   15-19 48.3% 49.0%   2.7%\n   20-29 43.6% 53.6%   2.8%\n   30-49 23.7% 73.9%   2.4%\n   50-69  2.1% 95.8%   2.1%\n     70+  0.0% 83.3%  16.7%\n    &lt;NA&gt;  0.0%  0.0% 100.0%\n   Total 47.7% 47.6%   4.7%\n\n\nA cross-tabulation adjusted so that both counts and percents are displayed.\n\nlinelist %&gt;%                                  # case linelist\n  tabyl(age_cat, gender) %&gt;%                  # cross-tabulate counts\n  adorn_totals(where = \"row\") %&gt;%             # add a total row\n  adorn_percentages(denominator = \"col\") %&gt;%  # convert to proportions\n  adorn_pct_formatting() %&gt;%                  # convert to percents\n  adorn_ns(position = \"front\") %&gt;%            # display as: \"count (percent)\"\n  adorn_title(                                # adjust titles\n    row_name = \"Age Category\",\n    col_name = \"Gender\")\n\n                      Gender                            \n Age Category              f              m          NA_\n          0-4   640  (22.8%)   416  (14.8%)  39  (14.0%)\n          5-9   641  (22.8%)   412  (14.7%)  42  (15.1%)\n        10-14   518  (18.5%)   383  (13.7%)  40  (14.4%)\n        15-19   359  (12.8%)   364  (13.0%)  20   (7.2%)\n        20-29   468  (16.7%)   575  (20.5%)  30  (10.8%)\n        30-49   179   (6.4%)   557  (19.9%)  18   (6.5%)\n        50-69     2   (0.1%)    91   (3.2%)   2   (0.7%)\n          70+     0   (0.0%)     5   (0.2%)   1   (0.4%)\n         &lt;NA&gt;     0   (0.0%)     0   (0.0%)  86  (30.9%)\n        Total 2,807 (100.0%) 2,803 (100.0%) 278 (100.0%)\n\n\n\n\n\nBy default, the tabyl will print raw to your R console.\nAlternatively, you can pass the tabyl to flextable or similar package to print as a “pretty” image in the RStudio Viewer, which could be exported as .png, .jpeg, .html, etc. Note that if printing in this manner and using adorn_titles(), you must specify placement = \"combined\".\n\nlinelist %&gt;%\n  tabyl(age_cat, gender) %&gt;% \n  adorn_totals(where = \"col\") %&gt;% \n  adorn_percentages(denominator = \"col\") %&gt;% \n  adorn_pct_formatting() %&gt;% \n  adorn_ns(position = \"front\") %&gt;% \n  adorn_title(\n    row_name = \"Age Category\",\n    col_name = \"Gender\",\n    placement = \"combined\") %&gt;% # this is necessary to print as image\n  flextable::flextable() %&gt;%    # convert to pretty image\n  flextable::autofit()          # format to one line per row \n\n\nAge Category/GenderfmNA_Total0-4640 (22.8%)416 (14.8%)39 (14.0%)1,095 (18.6%)5-9641 (22.8%)412 (14.7%)42 (15.1%)1,095 (18.6%)10-14518 (18.5%)383 (13.7%)40 (14.4%)941 (16.0%)15-19359 (12.8%)364 (13.0%)20  (7.2%)743 (12.6%)20-29468 (16.7%)575 (20.5%)30 (10.8%)1,073 (18.2%)30-49179  (6.4%)557 (19.9%)18  (6.5%)754 (12.8%)50-692  (0.1%)91  (3.2%)2  (0.7%)95  (1.6%)70+0  (0.0%)5  (0.2%)1  (0.4%)6  (0.1%)0  (0.0%)0  (0.0%)86 (30.9%)86  (1.5%)\n\n\n\n\n\nYou can use janitor’s adorn_*() functions on other tables, such as those created by summarise() and count() from dplyr, or table() from base R. Simply pipe the table to the desired janitor function. For example:\n\nlinelist %&gt;% \n  count(hospital) %&gt;%   # dplyr function\n  adorn_totals()        # janitor function\n\n                             hospital    n\n                     Central Hospital  454\n                    Military Hospital  896\n                              Missing 1469\n                                Other  885\n                        Port Hospital 1762\n St. Mark's Maternity Hospital (SMMH)  422\n                                Total 5888\n\n\n\n\n\nIf you convert the table to a “pretty” image with a package like flextable, you can save it with functions from that package - like save_as_html(), save_as_word(), save_as_ppt(), and save_as_image() from flextable. Below, the table is saved as a Word document, in which it can be further hand-edited.\n\nlinelist %&gt;%\n  tabyl(age_cat, gender) %&gt;% \n  adorn_totals(where = \"col\") %&gt;% \n  adorn_percentages(denominator = \"col\") %&gt;% \n  adorn_pct_formatting() %&gt;% \n  adorn_ns(position = \"front\") %&gt;% \n  adorn_title(\n    row_name = \"Age Category\",\n    col_name = \"Gender\",\n    placement = \"combined\") %&gt;% \n  flextable::flextable() %&gt;%                     # convert to image\n  flextable::autofit() %&gt;%                       # ensure only one line per row\n  flextable::save_as_docx(path = \"tabyl.docx\")   # save as Word document to filepath\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can apply statistical tests on tabyls, like chisq.test() or fisher.test() from the stats package, as shown below. Note missing values are not allowed so they are excluded from the tabyl with show_na = FALSE.\n\nage_by_outcome &lt;- linelist %&gt;% \n  tabyl(age_cat, outcome, show_na = FALSE) \n\nchisq.test(age_by_outcome)\n\n\n    Pearson's Chi-squared test\n\ndata:  age_by_outcome\nX-squared = 6.4931, df = 7, p-value = 0.4835"
  },
  {
    "objectID": "readings/descriptive-statistics.html#dplyr-package",
    "href": "readings/descriptive-statistics.html#dplyr-package",
    "title": "Descriptive analysis",
    "section": "",
    "text": "dplyr is part of the tidyverse packages and is an very common data management tool. Creating tables with dplyr functions summarise() and count() is a useful approach to calculating summary statistics, summarize by group, or pass tables to ggplot().\nsummarise() creates a new, summary data frame. If the data are ungrouped, it will return a one-row dataframe with the specified summary statistics of the entire data frame. If the data are grouped, the new data frame will have one row per group.\nWithin the summarise() parentheses, you provide the names of each new summary column followed by an equals sign and a statistical function to apply.\n\n\n\n\n\n\nTip\n\n\n\nThe summarise function works with both UK and US spelling (summarise() and summarize()).\n\n\n\n\nThe most simple function to apply within summarise() is n(). Leave the parentheses empty to count the number of rows.\n\nlinelist %&gt;%                 # begin with linelist\n  summarise(n_rows = n())    # return new summary dataframe with column n_rows\n\n  n_rows\n1   5888\n\n\nThis gets more interesting if we have grouped the data beforehand.\n\nlinelist %&gt;% \n  group_by(age_cat) %&gt;%     # group data by unique values in column age_cat\n  summarise(n_rows = n())   # return number of rows *per group*\n\n# A tibble: 9 × 2\n  age_cat n_rows\n  &lt;fct&gt;    &lt;int&gt;\n1 0-4       1095\n2 5-9       1095\n3 10-14      941\n4 15-19      743\n5 20-29     1073\n6 30-49      754\n7 50-69       95\n8 70+          6\n9 &lt;NA&gt;        86\n\n\nThe above command can be shortened by using the count() function instead. count() does the following:\n\nGroups the data by the columns provided to it\n\nSummarises them with n() (creating column n)\n\nUn-groups the data\n\n\nlinelist %&gt;% \n  count(age_cat)\n\n  age_cat    n\n1     0-4 1095\n2     5-9 1095\n3   10-14  941\n4   15-19  743\n5   20-29 1073\n6   30-49  754\n7   50-69   95\n8     70+    6\n9    &lt;NA&gt;   86\n\n\nYou can change the name of the counts column from the default n to something else by specifying it to name =.\nTabulating counts of two or more grouping columns are still returned in “long” format, with the counts in the n column.\n\nlinelist %&gt;% \n  count(age_cat, outcome)\n\n   age_cat outcome   n\n1      0-4   Death 471\n2      0-4 Recover 364\n3      0-4    &lt;NA&gt; 260\n4      5-9   Death 476\n5      5-9 Recover 391\n6      5-9    &lt;NA&gt; 228\n7    10-14   Death 438\n8    10-14 Recover 303\n9    10-14    &lt;NA&gt; 200\n10   15-19   Death 323\n11   15-19 Recover 251\n12   15-19    &lt;NA&gt; 169\n13   20-29   Death 477\n14   20-29 Recover 367\n15   20-29    &lt;NA&gt; 229\n16   30-49   Death 329\n17   30-49 Recover 238\n18   30-49    &lt;NA&gt; 187\n19   50-69   Death  33\n20   50-69 Recover  38\n21   50-69    &lt;NA&gt;  24\n22     70+   Death   3\n23     70+ Recover   3\n24    &lt;NA&gt;   Death  32\n25    &lt;NA&gt; Recover  28\n26    &lt;NA&gt;    &lt;NA&gt;  26\n\n\n\n\n\nIf you are tabling a column of class factor you can ensure that all levels are shown (not just the levels with values in the data) by adding .drop = FALSE into the summarise() or count() command.\nThis technique is useful to standardise your tables/plots. For example if you are creating figures for multiple sub-groups, or repeatedly creating the figure for routine reports. In each of these circumstances, the presence of values in the data may fluctuate, but you can define levels that remain constant.\n\n\n\nProportions can be added by piping the table to mutate() to create a new column. Define the new column as the counts column (n by default) divided by the sum() of the counts column (this will return a proportion).\nNote that in this case, sum() in the mutate() command will return the sum of the whole column n for use as the proportion denominator. If sum() is used in grouped data (e.g. if the mutate() immediately followed a group_by() command), it will return sums by group. As stated just above, count() finishes its actions by ungrouping. Thus, in this scenario we get full column proportions.\nTo easily display percents, you can wrap the proportion in the function percent() from the package scales (note this convert to class character).\n\nage_summary &lt;- linelist %&gt;% \n  count(age_cat) %&gt;%                     # group and count by gender (produces \"n\" column)\n  mutate(                                # create percent of column - note the denominator\n    percent = scales::percent(n / sum(n))) \n\n# print\nage_summary\n\n  age_cat    n percent\n1     0-4 1095  18.60%\n2     5-9 1095  18.60%\n3   10-14  941  15.98%\n4   15-19  743  12.62%\n5   20-29 1073  18.22%\n6   30-49  754  12.81%\n7   50-69   95   1.61%\n8     70+    6   0.10%\n9    &lt;NA&gt;   86   1.46%\n\n\nBelow is a method to calculate proportions within groups. It relies on different levels of data grouping being selectively applied and removed. First, the data are grouped on outcome via group_by(). Then, count() is applied. This function further groups the data by age_cat and returns counts for each outcome-age-cat combination. Importantly - as it finishes its process, count() also ungroups the age_cat grouping, so the only remaining data grouping is the original grouping by outcome. Thus, the final step of calculating proportions (denominator sum(n)) is still grouped by outcome.\n\nage_by_outcome &lt;- linelist %&gt;%                  # begin with linelist\n  group_by(outcome) %&gt;%                         # group by outcome \n  count(age_cat) %&gt;%                            # group and count by age_cat, and then remove age_cat grouping\n  mutate(percent = scales::percent(n / sum(n))) # calculate percent - note the denominator is by outcome group\n\n\n\n\n\n\n\n\n\n\n\nTo display a “long” table output like the above with ggplot() is relatively straight-forward. The data are naturally in “long” format, which is naturally accepted by ggplot().\n\nlinelist %&gt;%                      # begin with linelist\n  count(age_cat, outcome) %&gt;%     # group and tabulate counts by two columns\n  ggplot()+                       # pass new data frame to ggplot\n    geom_col(                     # create bar plot\n      mapping = aes(   \n        x = outcome,              # map outcome to x-axis\n        fill = age_cat,           # map age_cat to the fill\n        y = n))                   # map the counts column `n` to the height\n\n\n\n\n\n\n\nOne major advantage of dplyr and summarise() is the ability to return more advanced statistical summaries like median(), mean(), max(), min(), sd() (standard deviation), and percentiles. You can also use sum() to return the number of rows that meet certain logical criteria. As above, these outputs can be produced for the whole data frame set, or by group.\nThe syntax is the same - within the summarise() parentheses you provide the names of each new summary column followed by an equals sign and a statistical function to apply. Within the statistical function, give the column(s) to be operated on and any relevant arguments (e.g. na.rm = TRUE for most mathematical functions).\nYou can also use sum() to return the number of rows that meet a logical criteria. The expression within is counted if it evaluates to TRUE. For example:\n\nsum(age_years &lt; 18, na.rm=T)\n\nsum(gender == \"male\", na.rm=T)\n\nsum(response %in% c(\"Likely\", \"Very Likely\"))\n\nBelow, linelist data are summarised to describe the days delay from symptom onset to hospital admission (column days_onset_hosp), by hospital.\n\nsummary_table &lt;- linelist %&gt;%                                        # begin with linelist, save out as new object\n  group_by(hospital) %&gt;%                                             # group all calculations by hospital\n  summarise(                                                         # only the below summary columns will be returned\n    cases       = n(),                                                # number of rows per group\n    delay_max   = max(days_onset_hosp, na.rm = T),                    # max delay\n    delay_mean  = round(mean(days_onset_hosp, na.rm=T), digits = 1),  # mean delay, rounded\n    delay_sd    = round(sd(days_onset_hosp, na.rm = T), digits = 1),  # standard deviation of delays, rounded\n    delay_3     = sum(days_onset_hosp &gt;= 3, na.rm = T),               # number of rows with delay of 3 or more days\n    pct_delay_3 = scales::percent(delay_3 / cases)                    # convert previously-defined delay column to percent \n  )\n\nsummary_table  # print\n\n# A tibble: 6 × 7\n  hospital               cases delay_max delay_mean delay_sd delay_3 pct_delay_3\n  &lt;chr&gt;                  &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;      \n1 Central Hospital         454        12        1.9      1.9     108 24%        \n2 Military Hospital        896        15        2.1      2.4     253 28%        \n3 Missing                 1469        22        2.1      2.3     399 27%        \n4 Other                    885        18        2        2.2     234 26%        \n5 Port Hospital           1762        16        2.1      2.2     470 27%        \n6 St. Mark's Maternity …   422        18        2.1      2.3     116 27%        \n\n\nSome tips:\n\nUse sum() with a logic statement to “count” rows that meet certain criteria (==)\n\nNote the use of na.rm = TRUE within mathematical functions like sum(), otherwise NA will be returned if there are any missing values\n\nUse the function percent() from the scales package to easily convert to percents\n\nSet accuracy = to 0.1 or 0.01 to ensure 1 or 2 decimal places respectively\n\n\nUse round() from base R to specify decimals\n\nTo calculate these statistics on the entire dataset, use summarise() without group_by()\n\nYou may create columns for the purposes of later calculations (e.g. denominators) that you eventually drop from your data frame with select().\n\n\n\n\nYou may want to return conditional statistics - e.g. the maximum of rows that meet certain criteria. This can be done by subsetting the column with brackets [ ]. The example below returns the maximum temperature for patients classified having or not having fever. Be aware however - it may be more appropriate to add another column to the group_by() command and pivot_wider() (as demonstrated below).\n\nlinelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    max_temp_fvr = max(temp[fever == \"yes\"], na.rm = T),\n    max_temp_no = max(temp[fever == \"no\"], na.rm = T)\n  )\n\n# A tibble: 6 × 3\n  hospital                             max_temp_fvr max_temp_no\n  &lt;chr&gt;                                       &lt;dbl&gt;       &lt;dbl&gt;\n1 Central Hospital                             40.4        38  \n2 Military Hospital                            40.5        38  \n3 Missing                                      40.6        38  \n4 Other                                        40.8        37.9\n5 Port Hospital                                40.6        38  \n6 St. Mark's Maternity Hospital (SMMH)         40.6        37.9\n\n\n\n\n\nThe function str_glue() from stringr is useful to combine values from several columns into one new column. In this context this is typically used after the summarise() command.\nBelow, the summary_table data frame (created above) is mutated such that columns delay_mean and delay_sd are combined, parentheses formating is added to the new column, and their respective old columns are removed.\nThen, to make the table more presentable, a total row is added with adorn_totals() from janitor (which ignores non-numeric columns). Lastly, we use select() from dplyr to both re-order and rename to nicer column names.\nNow you could pass to flextable and print the table to Word, .png, .jpeg, .html, Powerpoint, RMarkdown, etc.! .\n\nsummary_table %&gt;% \n  mutate(delay = str_glue(\"{delay_mean} ({delay_sd})\")) %&gt;%  # combine and format other values\n  select(-c(delay_mean, delay_sd)) %&gt;%                       # remove two old columns   \n  adorn_totals(where = \"row\") %&gt;%                            # add total row\n  select(                                                    # order and rename cols\n    \"Hospital Name\"   = hospital,\n    \"Cases\"           = cases,\n    \"Max delay\"       = delay_max,\n    \"Mean (sd)\"       = delay,\n    \"Delay 3+ days\"   = delay_3,\n    \"% delay 3+ days\" = pct_delay_3\n    )\n\n                        Hospital Name Cases Max delay Mean (sd) Delay 3+ days\n                     Central Hospital   454        12 1.9 (1.9)           108\n                    Military Hospital   896        15 2.1 (2.4)           253\n                              Missing  1469        22 2.1 (2.3)           399\n                                Other   885        18   2 (2.2)           234\n                        Port Hospital  1762        16 2.1 (2.2)           470\n St. Mark's Maternity Hospital (SMMH)   422        18 2.1 (2.3)           116\n                                Total  5888       101         -          1580\n % delay 3+ days\n             24%\n             28%\n             27%\n             26%\n             27%\n             27%\n               -\n\n\n\n\nPercentiles and quantiles in dplyr deserve a special mention. To return quantiles, use quantile() with the defaults or specify the value(s) you would like with probs =.\n\n# get default percentile values of age (0%, 25%, 50%, 75%, 100%)\nlinelist %&gt;% \n  summarise(age_percentiles = quantile(age_years, na.rm = TRUE))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n  age_percentiles\n1               0\n2               6\n3              13\n4              23\n5              84\n\n# get manually-specified percentile values of age (5%, 50%, 75%, 98%)\nlinelist %&gt;% \n  summarise(\n    age_percentiles = quantile(\n      age_years,\n      probs = c(.05, 0.5, 0.75, 0.98), \n      na.rm=TRUE)\n    )\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n  age_percentiles\n1               1\n2              13\n3              23\n4              48\n\n\nIf you want to return quantiles by group, you may encounter long and less useful outputs if you simply add another column to group_by(). So, try this approach instead - create a column for each quantile level desired.\n\n# get manually-specified percentile values of age (5%, 50%, 75%, 98%)\nlinelist %&gt;% \n  group_by(hospital) %&gt;% \n  summarise(\n    p05 = quantile(age_years, probs = 0.05, na.rm=T),\n    p50 = quantile(age_years, probs = 0.5, na.rm=T),\n    p75 = quantile(age_years, probs = 0.75, na.rm=T),\n    p98 = quantile(age_years, probs = 0.98, na.rm=T)\n    )\n\n# A tibble: 6 × 5\n  hospital                               p05   p50   p75   p98\n  &lt;chr&gt;                                &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Central Hospital                         1    12    21  48  \n2 Military Hospital                        1    13    24  45  \n3 Missing                                  1    13    23  48.2\n4 Other                                    1    13    23  50  \n5 Port Hospital                            1    14    24  49  \n6 St. Mark's Maternity Hospital (SMMH)     2    12    22  50.2\n\n\n\n\n\n\nIf you begin with aggregated data, using n() return the number of rows, not the sum of the aggregated counts. To get sums, use sum() on the data’s counts column.\nFor example, let’s say you are beginning with the data frame of counts below, called linelist_agg - it shows in “long” format the case counts by outcome and gender.\nBelow we create this example data frame of linelist case counts by outcome and gender (missing values removed for clarity).\n\nlinelist_agg &lt;- linelist %&gt;% \n  drop_na(gender, outcome) %&gt;% \n  count(outcome, gender)\n\nlinelist_agg\n\n  outcome gender    n\n1   Death      f 1227\n2   Death      m 1228\n3 Recover      f  953\n4 Recover      m  950\n\n\nTo sum the counts (in column n) by group you can use summarise() but set the new column equal to sum(n, na.rm=T). To add a conditional element to the sum operation, you can use the subset bracket [ ] syntax on the counts column.\n\nlinelist_agg %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(\n    total_cases  = sum(n, na.rm=T),\n    male_cases   = sum(n[gender == \"m\"], na.rm=T),\n    female_cases = sum(n[gender == \"f\"], na.rm=T))\n\n# A tibble: 2 × 4\n  outcome total_cases male_cases female_cases\n  &lt;chr&gt;         &lt;int&gt;      &lt;int&gt;        &lt;int&gt;\n1 Death          2455       1228         1227\n2 Recover        1903        950          953\n\n\n\n\n\nYou can use summarise() across multiple columns using across(). This makes life easier when you want to calculate the same statistics for many columns. Place across() within summarise() and specify the following:\n\n.cols = as either a vector of column names c() or “tidyselect” helper functions (explained below)\n\n.fns = the function to perform (no parentheses) - you can provide multiple within a list()\n\nBelow, mean() is applied to several numeric columns. A vector of columns are named explicitly to .cols = and a single function mean is specified (no parentheses) to .fns =. Any additional arguments for the function (e.g. na.rm=TRUE) are provided after .fns =, separated by a comma.\nIt can be difficult to get the order of parentheses and commas correct when using across(). Remember that within across() you must include the columns, the functions, and any extra arguments needed for the functions.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm),  # columns\n                   .fns = mean,                               # function\n                   na.rm=T))                                  # extra arguments\n\nWarning: There was 1 warning in `summarise()`.\nℹ In argument: `across(...)`.\nℹ In group 1: `outcome = \"Death\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n# A tibble: 3 × 5\n  outcome age_years  temp wt_kg ht_cm\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Death        15.9  38.6  52.6  125.\n2 Recover      16.1  38.6  52.5  125.\n3 &lt;NA&gt;         16.2  38.6  53.0  125.\n\n\nMultiple functions can be run at once. Below the functions mean and sd are provided to .fns = within a list(). You have the opportunity to provide character names (e.g. “mean” and “sd”) which are appended in the new column names.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(.cols = c(age_years, temp, wt_kg, ht_cm), # columns\n                   .fns = list(\"mean\" = mean, \"sd\" = sd),    # multiple functions \n                   na.rm=T))                                 # extra arguments\n\n# A tibble: 3 × 9\n  outcome age_years_mean age_years_sd temp_mean temp_sd wt_kg_mean wt_kg_sd\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n1 Death             15.9         12.3      38.6   0.962       52.6     18.4\n2 Recover           16.1         13.0      38.6   0.997       52.5     18.6\n3 &lt;NA&gt;              16.2         12.8      38.6   0.976       53.0     18.9\n# ℹ 2 more variables: ht_cm_mean &lt;dbl&gt;, ht_cm_sd &lt;dbl&gt;\n\n\nHere are those “tidyselect” helper functions you can provide to .cols = to select columns:\n\neverything() - all other columns not mentioned\n\nlast_col() - the last column\n\nwhere() - applies a function to all columns and selects those which are TRUE\n\nstarts_with() - matches to a specified prefix. Example: starts_with(\"date\")\nends_with() - matches to a specified suffix. Example: ends_with(\"_end\")\n\ncontains() - columns containing a character string. Example: contains(\"time\")\nmatches() - to apply a regular expression (regex). Example: contains(\"[pt]al\")\n\nnum_range() -\nany_of() - matches if column is named. Useful if the name might not exist. Example: any_of(date_onset, date_death, cardiac_arrest)\n\nFor example, to return the mean of every numeric column use where() and provide the function as.numeric() (without parentheses). All this remains within the across() command.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(across(\n    .cols = where(is.numeric),  # all numeric columns in the data frame\n    .fns = mean,\n    na.rm=T))\n\n# A tibble: 3 × 12\n  outcome generation   age age_years   lon   lat wt_kg ht_cm ct_blood  temp\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 Death         16.7  15.9      15.9 -13.2  8.47  52.6  125.     21.3  38.6\n2 Recover       16.4  16.2      16.1 -13.2  8.47  52.5  125.     21.1  38.6\n3 &lt;NA&gt;          16.5  16.3      16.2 -13.2  8.47  53.0  125.     21.2  38.6\n# ℹ 2 more variables: bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\n\n\n\nWhen summarise() operates on grouped data it does not automatically produce “total” statistics. Below, two approaches to adding a total row are presented:\n\n\nIf your table consists only of counts or proportions/percents that can be summed into a total, then you can add sum totals using janitor’s adorn_totals() as described in the section above. Note that this function can only sum the numeric columns - if you want to calculate other total summary statistics see the next approach with dplyr.\nBelow, linelist is grouped by gender and summarised into a table that described the number of cases with known outcome, deaths, and recovered. Piping the table to adorn_totals() adds a total row at the bottom reflecting the sum of each column. The further adorn_*() functions adjust the display as noted in the code.\n\nlinelist %&gt;% \n  group_by(gender) %&gt;%\n  summarise(\n    known_outcome = sum(!is.na(outcome)),           # Number of rows in group where outcome is not missing\n    n_death  = sum(outcome == \"Death\", na.rm=T),    # Number of rows in group where outcome is Death\n    n_recover = sum(outcome == \"Recover\", na.rm=T), # Number of rows in group where outcome is Recovered\n  ) %&gt;% \n  adorn_totals() %&gt;%                                # Adorn total row (sums of each numeric column)\n  adorn_percentages(\"col\") %&gt;%                      # Get column proportions\n  adorn_pct_formatting() %&gt;%                        # Convert proportions to percents\n  adorn_ns(position = \"front\")                      # display % and counts (with counts in front)\n\n gender  known_outcome        n_death      n_recover\n      f 2,180  (47.8%) 1,227  (47.5%)   953  (48.1%)\n      m 2,178  (47.7%) 1,228  (47.6%)   950  (47.9%)\n   &lt;NA&gt;   207   (4.5%)   127   (4.9%)    80   (4.0%)\n  Total 4,565 (100.0%) 2,582 (100.0%) 1,983 (100.0%)\n\n\n\n\n\nIf your table consists of summary statistics such as median(), mean(), etc, the adorn_totals() approach shown above will not be sufficient. Instead, to get summary statistics for the entire dataset you must calculate them with a separate summarise() command and then bind the results to the original grouped summary table. You can make a summary table of outcome by hospital with group_by() and summarise() like this:\n\nby_hospital &lt;- linelist %&gt;% \n  filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%  # Remove cases with missing outcome or hospital\n  group_by(hospital, outcome) %&gt;%                      # Group data\n  summarise(                                           # Create new summary columns of indicators of interest\n    N = n(),                                            # Number of rows per hospital-outcome group     \n    ct_value = median(ct_blood, na.rm=T))               # median CT value per group\n  \nby_hospital # print table\n\n# A tibble: 10 × 4\n# Groups:   hospital [5]\n   hospital                             outcome     N ct_value\n   &lt;chr&gt;                                &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;\n 1 Central Hospital                     Death     193       22\n 2 Central Hospital                     Recover   165       22\n 3 Military Hospital                    Death     399       21\n 4 Military Hospital                    Recover   309       22\n 5 Other                                Death     395       22\n 6 Other                                Recover   290       21\n 7 Port Hospital                        Death     785       22\n 8 Port Hospital                        Recover   579       21\n 9 St. Mark's Maternity Hospital (SMMH) Death     199       22\n10 St. Mark's Maternity Hospital (SMMH) Recover   126       22\n\n\nTo get the totals, run the same summarise() command but only group the data by outcome (not by hospital), like this:\n\ntotals &lt;- linelist %&gt;% \n      filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%\n      group_by(outcome) %&gt;%                            # Grouped only by outcome, not by hospital    \n      summarise(\n        N = n(),                                       # These statistics are now by outcome only     \n        ct_value = median(ct_blood, na.rm=T))\n\ntotals # print table\n\n# A tibble: 2 × 3\n  outcome     N ct_value\n  &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;\n1 Death    1971       22\n2 Recover  1469       22\n\n\nWe can bind these two data frames together. Note that by_hospital has 4 columns whereas totals has 3 columns. By using bind_rows(), the columns are combined by name, and any extra space is filled in with NA (e.g the column hospital values for the two new totals rows). After binding the rows, we convert these empty spaces to “Total” using replace_na().\n\ntable_long &lt;- bind_rows(by_hospital, totals) %&gt;% \n  mutate(hospital = replace_na(hospital, \"Total\"))\n\nHere is the new table with “Total” rows at the bottom.\n\n\n\n\n\n\n\nThis table is in a “long” format, which may be what you want. Optionally, you can pivot this table wider to make it more readable (which we will learn about later). You can also add more columns, and arrange it nicely. This code is below.\n\ntable_long %&gt;% \n  \n  # Pivot wider and format\n  ########################\n  mutate(hospital = replace_na(hospital, \"Total\")) %&gt;% \n  pivot_wider(                                         # Pivot from long to wide\n    values_from = c(ct_value, N),                       # new values are from ct and count columns\n    names_from = outcome) %&gt;%                           # new column names are from outcomes\n  mutate(                                              # Add new columns\n    N_Known = N_Death + N_Recover,                               # number with known outcome\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # percent who recovered (to 1 decimal)\n  select(                                              # Re-order columns\n    hospital, N_Known,                                   # Intro columns\n    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns\n    N_Death, Pct_Death, ct_value_Death)  %&gt;%             # Death columns\n  arrange(N_Known)                                  # Arrange rows from lowest to highest (Total row at bottom)\n\n# A tibble: 6 × 8\n# Groups:   hospital [6]\n  hospital      N_Known N_Recover Pct_Recover ct_value_Recover N_Death Pct_Death\n  &lt;chr&gt;           &lt;int&gt;     &lt;int&gt; &lt;chr&gt;                  &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;    \n1 St. Mark's M…     325       126 38.8%                     22     199 61.2%    \n2 Central Hosp…     358       165 46.1%                     22     193 53.9%    \n3 Other             685       290 42.3%                     21     395 57.7%    \n4 Military Hos…     708       309 43.6%                     22     399 56.4%    \n5 Port Hospital    1364       579 42.4%                     21     785 57.6%    \n6 Total            3440      1469 42.7%                     22    1971 57.3%    \n# ℹ 1 more variable: ct_value_Death &lt;dbl&gt;\n\n\nIn the next chapter we’ll see how to create an attractive visualization of the table, as shown here:\n\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22"
  },
  {
    "objectID": "readings/descriptive-statistics.html#base-r",
    "href": "readings/descriptive-statistics.html#base-r",
    "title": "Descriptive analysis",
    "section": "",
    "text": "You can use the function table() to tabulate and cross-tabulate columns. Unlike the options above, you must specify the dataframe each time you reference a column name, as shown below.\n\n\n\n\n\n\nWarning\n\n\n\nNA (missing) values will not be tabulated unless you include the argument useNA = \"always\" (which could also be set to “no” or “ifany”).\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use the %$% from magrittr to remove the need for repeating data frame calls within base functions. For example the below could be written linelist %$% table(outcome, useNA = \"always\")\n\n\n\ntable(linelist$outcome, useNA = \"always\")\n\n\n  Death Recover    &lt;NA&gt; \n   2582    1983    1323 \n\n\nMultiple columns can be cross-tabulated by listing them one after the other, separated by commas. Optionally, you can assign each column a “name” like Outcome = linelist$outcome.\n\nage_by_outcome &lt;- table(linelist$age_cat, linelist$outcome, useNA = \"always\") # save table as object\nage_by_outcome   # print table\n\n       \n        Death Recover &lt;NA&gt;\n  0-4     471     364  260\n  5-9     476     391  228\n  10-14   438     303  200\n  15-19   323     251  169\n  20-29   477     367  229\n  30-49   329     238  187\n  50-69    33      38   24\n  70+       3       3    0\n  &lt;NA&gt;     32      28   26\n\n\n\n\nTo return proportions, passing the above table to the function prop.table(). Use the margins = argument to specify whether you want the proportions to be of rows (1), of columns (2), or of the whole table (3). For clarity, we pipe the table to the round() function from base R, specifying 2 digits.\n\n# get proportions of table defined above, by rows, rounded\nprop.table(age_by_outcome, 1) %&gt;% round(2)\n\n       \n        Death Recover &lt;NA&gt;\n  0-4    0.43    0.33 0.24\n  5-9    0.43    0.36 0.21\n  10-14  0.47    0.32 0.21\n  15-19  0.43    0.34 0.23\n  20-29  0.44    0.34 0.21\n  30-49  0.44    0.32 0.25\n  50-69  0.35    0.40 0.25\n  70+    0.50    0.50 0.00\n  &lt;NA&gt;   0.37    0.33 0.30\n\n\n\n\n\nTo add row and column totals, pass the table to addmargins(). This works for both counts and proportions.\n\naddmargins(age_by_outcome)\n\n       \n        Death Recover &lt;NA&gt;  Sum\n  0-4     471     364  260 1095\n  5-9     476     391  228 1095\n  10-14   438     303  200  941\n  15-19   323     251  169  743\n  20-29   477     367  229 1073\n  30-49   329     238  187  754\n  50-69    33      38   24   95\n  70+       3       3    0    6\n  &lt;NA&gt;     32      28   26   86\n  Sum    2582    1983 1323 5888"
  },
  {
    "objectID": "readings/basics.html",
    "href": "readings/basics.html",
    "title": "R Basics",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\nlibrary(rio)\nlibrary(here)\nlibrary(apyramid)\nlibrary(dplyr)\nlinelist &lt;- rio::import(\"data/case_linelists/linelist_cleaned.rds\")\n\n\n\n\n\n\nFunctions are at the core of using R. Functions are how you perform tasks and operations. Many functions come installed with R, many more are available for download in packages (explained in the packages section), and you can even write your own custom functions!\nThis basics section on functions explains:\n\nWhat a function is and how they work\n\nWhat function arguments are\n\nHow to get help understanding a function\n\n\n\n\n\n\n\nA quick note on syntax\n\n\n\n\n\nIn this workbook, functions are written in code-text with open parentheses, like this: filter(). As explained in the packages section, functions are downloaded within packages. In this handbook, package names are written in bold, like dplyr. Sometimes in example code you may see the function name linked explicitly to the name of its package with two colons (::) like this: dplyr::filter(). The purpose of this linkage is explained in the packages section.\n\n\n\n\n\n\nA function is like a machine that receives inputs, does some action with those inputs, and produces an output. What the output is depends on the function.\nFunctions typically operate upon some object placed within the function’s parentheses. For example, the function sqrt() calculates the square root of a number:\n\nsqrt(49)\n\n[1] 7\n\n\nThe object provided to a function also can be a column in a dataset (see the Objects section for detail on all the kinds of objects). Because R can store multiple datasets, you will need to specify both the dataset and the column. One way to do this is using the $ notation to link the name of the dataset and the name of the column (dataset$column). In the example below, the function summary() is applied to the numeric column age in the dataset linelist, and the output is a summary of the column’s numeric and missing values.\n\n# Print summary statistics of column 'age' in the dataset 'linelist'\nsummary(linelist$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.07   23.00   84.00      86 \n\n\n\n\n\n\n\n\nNote\n\n\n\nBehind the scenes, a function represents complex additional code that has been wrapped up for the user into one easy command.\n\n\n\n\n\n\nFunctions often ask for several inputs, called arguments, located within the parentheses of the function, usually separated by commas.\n\nSome arguments are required for the function to work correctly, others are optional\n\nOptional arguments have default settings\n\nArguments can take character, numeric, logical (TRUE/FALSE), and other inputs\n\nHere is a fun fictional function, called oven_bake(), as an example of a typical function. It takes an input object (e.g. a dataset, or in this example “dough”) and performs operations on it as specified by additional arguments (minutes = and temperature =). The output can be printed to the console, or saved as an object using the assignment operator &lt;-.\n\n\n\n\n\n\n\n\n\nIn a more realistic example, the age_pyramid() command below produces an age pyramid plot based on defined age groups and a binary split column, such as gender. The function is given three arguments within the parentheses, separated by commas. The values supplied to the arguments establish linelist as the dataframe to use, age_cat5 as the column to count, and gender as the binary column to use for splitting the pyramid by color.\n\n# Create an age pyramid\nage_pyramid(data = linelist, age_group = \"age_cat5\", split_by = \"gender\")\n\n\n\n\nThe above command can be equivalently written as below, in a longer style with a new line for each argument. This style can be easier to read, and easier to write “comments” with # to explain each part (commenting extensively is good practice!). To run this longer command you can highlight the entire command and click “Run”, or just place your cursor in the first line and then press the Ctrl and Enter keys simultaneously.\n\n# Create an age pyramid\nage_pyramid(\n  data = linelist,        # use case linelist\n  age_group = \"age_cat5\", # provide age group column\n  split_by = \"gender\"     # use gender column for two sides of pyramid\n  )\n\n\n\n\nThe first half of an argument assignment (e.g. data =) does not need to be specified if the arguments are written in a specific order (specified in the function’s documentation). The below code produces the exact same pyramid as above, because the function expects the argument order: data frame, age_group variable, split_by variable.\n\n# This command will produce the exact same graphic as above\nage_pyramid(linelist, \"age_cat5\", \"gender\")\n\nA more complex age_pyramid() command might include the optional arguments to:\n\nShow proportions instead of counts (set proportional = TRUE when the default is FALSE)\n\nSpecify the two colors to use (pal = is short for “palette” and is supplied with a vector of two color names. See the objects page for how the function c() makes a vector)\n\n\n\n\n\n\n\nNote\n\n\n\nFor arguments that you specify with both parts of the argument (e.g. proportional = TRUE), their order among all the arguments does not matter.\n\n\n\nage_pyramid(\n  linelist,                    # use case linelist\n  \"age_cat5\",                  # age group column\n  \"gender\",                    # split by gender\n  proportional = TRUE,         # percents instead of counts\n  pal = c(\"orange\", \"purple\")  # colors\n  )\n\n\n\n\n\n\n\n\nPackages contain functions.\nAn R package is a shareable bundle of code and documentation that contains pre-defined functions. Users in the R community develop packages all the time catered to specific problems, it is likely that one can help with your work! You will install and use hundreds of packages in your use of R.\nOn installation, R contains “base” packages and functions that perform common elementary tasks. But many R users create specialized functions, which are verified by the R community and which you can download as a package for your own use. In this handbook, package names are written in bold. One of the more challenging aspects of R is that there are often many functions or packages to choose from to complete a given task.\n\n\nFunctions are contained within packages which can be downloaded (“installed”) to your computer from the internet. Once a package is downloaded, it is stored in your “library”. You can then access the functions it contains during your current R session by “loading” the package.\nThink of R as your personal library: When you download a package, your library gains a new book of functions, but each time you want to use a function in that book, you must borrow (“load”) that book from your library.\nIn summary: to use the functions available in an R package, 2 steps must be implemented:\n\nThe package must be installed (once), and\n\nThe package must be loaded (each R session)\n\n\n\nYour “library” is actually a folder on your computer, containing a folder for each package that has been installed. Find out where R is installed in your computer, and look for a folder called “win-library”. For example: R\\win-library\\4.0 (the 4.0 is the R version - you’ll have a different library for each R version you’ve downloaded).\nYou can print the file path to your library by entering .libPaths() (empty parentheses).\n\n\n\nMost often, R users download packages from CRAN. CRAN (Comprehensive R Archive Network) is an online public warehouse of R packages that have been published by R community members.\n\n\n\nThe base R function for installing a package is install.packages(). The name of the package to install must be provided in the parentheses in quotes. If you want to install multiple packages in one command, they must be listed within a character vector c().\n\n\n\n\n\n\nCommon Mistake\n\n\n\nThis command installs a package, but does not load it for use in the current session.\n\n\n\n# install a single package with base R\ninstall.packages(\"tidyverse\")\n\n# install multiple packages with base R\ninstall.packages(c(\"tidyverse\", \"rio\", \"here\"))\n\nInstallation can also be accomplished point-and-click by going to the RStudio “Packages” pane and clicking “Install” and searching for the desired package name.\nThe base R function to load a package for use (after it has been installed) is library(). It can load only one package at a time (another reason to use p_load()). You can provide the package name with or without quotes.\n\n# load packages for use, with base R\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(here)\n\nTo check whether a package in installed and/or loaded, you can view the Packages pane in RStudio. If the package is installed, it is shown there with version number. If its box is checked, it is loaded for the current session.\n\n\n\n\nFor clarity in this handbook, functions are sometimes preceded by the name of their package using the :: symbol in the following way: package_name::function_name()\nOnce a package is loaded for a session, this explicit style is not necessary. One can just use function_name(). However writing the package name is useful when a function name is common and may exist in multiple packages (e.g. plot()). Writing the package name will also load the package if it is not already loaded.\n\n# This command uses the package \"rio\" and its function \"import()\" to import a dataset\nlinelist &lt;- rio::import(\"linelist.xlsx\", which = \"Sheet1\")\n\n\n\n\nTo read more about a function, you can search for it in the Help tab of the lower-right RStudio. You can also run a command like ?thefunctionname (put the name of the function after a question mark) and the Help page will appear in the Help pane. Finally, try searching online for resources.\n\n\n\nYou can update packages by re-installing them. You can also click the green “Update” button in your RStudio Packages pane to see which packages have new versions Wto install. Be aware that your old code may need to be updated if there is a major revision to how a function works!\n\n\n\nUse remove.packages(). Alternatively, go find the folder which contains your library and manually delete the folder.\n\n\n\nPackages often depend on other packages to work. These are called dependencies. If a dependency fails to install, then the package depending on it may also fail to install.\n\n\n\nIt is not uncommon that two or more packages contain the same function name. For example, the package dplyr has a filter() function, but so does the package stats. The default filter() function depends on the order these packages are first loaded in the R session - the later one will be the default for the command filter().\nYou can check the order in your Environment pane of R Studio - click the drop-down for “Global Environment” and see the order of the packages. Functions from packages lower on that drop-down list will mask functions of the same name in packages that appear higher in the drop-down list. When first loading a package, R will warn you in the console if masking is occurring, but this can be easy to miss.\n\n\n\n\n\n\n\n\n\nHere are ways you can fix masking:\n\nSpecify the package name in the command. For example, use dplyr::filter()\n\nRe-arrange the order in which the packages are loaded and start a new R session\n\n\n\n\nSee this guide to install an older version of a particular package.\n\n\n\n\n\nScripts are a fundamental part of programming. They are documents that hold your commands (e.g. functions to create and modify datasets, print visualizations, etc). You can save a script and run it again later. There are many advantages to storing and running your commands from a script (vs. typing commands one-by-one into the R console “command line”):\n\nPortability - you can share your work with others by sending them your scripts\n\nReproducibility - so that you and others know exactly what you did\n\nVersion control - so you can track changes made by yourself or colleagues\n\nCommenting/annotation - to explain to your colleagues what you have done\n\n\n\nIn a script you can also annotate (“comment”) around your R code. Commenting is helpful to explain to yourself and other readers what you are doing. You can add a comment by typing the hash symbol (#) and writing your comment after it. The commented text will appear in a different color than the R code.\nAny code written after the # will not be run. Therefore, placing a # before code is also a useful way to temporarily block a line of code (“comment out”) if you do not want to delete it). You can comment out/in multiple lines at once by highlighting them and pressing Ctrl+Shift+c (Cmd+Shift+c in Mac).\n\n# A comment can be on a line by itself\n# import data\nlinelist &lt;- import(\"linelist_raw.xlsx\") %&gt;%   # a comment can also come after code\n# filter(age &gt; 50)                          # It can also be used to deactivate / remove a line of code\n  count()\n\n\nComment on what you are doing and on why you are doing it.\n\nBreak your code into logical sections\n\nAccompany your code with a text step-by-step description of what you are doing (e.g. numbered steps)\n\n\n\n\nIt is important to be conscious of your coding style - especially if working on a team. Some nice R styles guide are the tidyverse style guide, Hadley Wickham’s style guide, and Google’s. There are also packages such as styler and lintr which help you conform to a style.\nA few very basic points to make your code readable to others:\n* When naming objects, use only lowercase letters, numbers, and underscores _, e.g. my_data\n* Use frequent spaces, including around operators, e.g. n = 1 and age_new &lt;- age_old + 3\n\n\n\nBelow is an example of a short R script. Remember, the better you succinctly explain your code in comments, the more your colleagues will like you!\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R markdown script is a type of R script in which the script itself becomes an output document (PDF, Word, HTML, Powerpoint, etc.). Quarto is a new version of R markdown. These are incredibly useful and versatile tools often used to create dynamic and automated reports. Even this website is produced with Quarto!\n\n\n\n\n\nEverything in R is an object, and R is an “object-oriented” language. These sections will explain:\n\nHow to create objects (&lt;-)\nTypes of objects (e.g. data frames, vectors..)\n\nHow to access subparts of objects (e.g. variables in a dataset)\n\nClasses of objects (e.g. numeric, logical, integer, double, character, factor)\n\n\n\n\nEverything you store in R - datasets, variables, a list of village names, a total population number, even outputs such as graphs - are objects which are assigned a name and can be referenced in later commands.\nAn object exists when you have assigned it a value (see the assignment section below). When it is assigned a value, the object appears in the Environment (see the upper right pane of RStudio). It can then be operated upon, manipulated, changed, and re-defined.\n\n\n\n\nCreate objects by assigning them a value with the &lt;- operator.\nYou can think of the assignment operator &lt;- as the words “is defined as”. Assignment commands generally follow a standard order:\nobject_name &lt;- value (or process/calculation that produce a value)\nFor example, you may want to record the current epidemiological reporting week as an object for reference in later code. In this example, the object current_week is created when it is assigned the value \"2018-W10\" (the quote marks make this a character value). The object current_week will then appear in the RStudio Environment pane (upper-right) and can be referenced in later commands.\nSee the R commands and their output in the boxes below.\n\ncurrent_week &lt;- \"2018-W10\"   # this command creates the object current_week by assigning it a value\ncurrent_week                 # this command prints the current value of current_week object in the console\n\n[1] \"2018-W10\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the [1] in the R console output is simply indicating that you are viewing the first item of the output\n\n\n\n\n\n\n\n\nCommon Mistake\n\n\n\nAn object’s value can be over-written at any time by running an assignment command to re-define its value. Thus, the order of the commands run is very important.\nFor instance, the following command will re-define the value of current_week:\n\ncurrent_week &lt;- \"2018-W51\"   # assigns a NEW value to the object current_week\ncurrent_week                 # prints the current value of current_week in the console\n\n[1] \"2018-W51\"\n\n\n\n\nEquals signs =\nYou will also see equals signs in R code:\n\nA double equals sign == between two objects or values asks a logical question: “is this equal to that?”.\n\nYou will also see equals signs within functions used to specify values of function arguments (read about these in sections below), for example max(age, na.rm = TRUE).\n\nYou can use a single equals sign = in place of &lt;- to create and define objects, but this is discouraged. You can read about why this is discouraged here.\n\nDatasets\nDatasets are also objects (typically “dataframes”) and must be assigned names when they are imported. In the code below, the object linelist is created and assigned the value of a CSV file imported with the rio package and its import() function.\n\n# linelist is created and assigned the value of the imported CSV file\nlinelist &lt;- import(\"my_linelist.csv\")\n\n\n\n\n\n\n\nNaming Objects\n\n\n\n\nObject names must not contain spaces, but you should use underscore (_) or a period (.) instead of a space.\n\nObject names are case-sensitive (meaning that Dataset_A is different from dataset_A).\nObject names must begin with a letter (cannot begin with a number like 1, 2 or 3).\n\n\n\nOutputs\nOutputs like tables and plots provide an example of how outputs can be saved as objects, or just be printed without being saved. A cross-tabulation of gender and outcome using the base R function table() can be printed directly to the R console (without being saved).\n\n# printed to R console only\ntable(linelist$gender, linelist$outcome)\n\n   \n    Death Recover\n  f  1227     953\n  m  1228     950\n\n\nBut the same table can be saved as a named object. Then, optionally, it can be printed.\n\n# save\ngen_out_table &lt;- table(linelist$gender, linelist$outcome)\n\n# print\ngen_out_table\n\n   \n    Death Recover\n  f  1227     953\n  m  1228     950\n\n\nColumns\nColumns in a dataset are also objects and can be defined, over-written, and created as described below in the section on Columns.\nYou can use the assignment operator from base R to create a new column. Below, the new column bmi (Body Mass Index) is created, and for each row the new value is result of a mathematical operation on the row’s value in the wt_kg and ht_cm columns.\n\n# create new \"bmi\" column using base R syntax\nlinelist$bmi &lt;- linelist$wt_kg / (linelist$ht_cm/100)^2\n\nHowever, in this handbook, we emphasize a different approach to defining columns, which uses the function mutate() from the dplyr package and piping with the pipe operator (%&gt;%). The syntax is easier to read and there are other advantages.\n\n# create new \"bmi\" column using dplyr syntax\nlinelist &lt;- linelist %&gt;% \n  mutate(bmi = wt_kg / (ht_cm/100)^2)\n\n\n\n\n\nObjects can be a single piece of data (e.g. my_number &lt;- 24), or they can consist of structured data.\nThe graphic below is borrowed from this online R tutorial. It shows some common data structures and their names.\n\n\n\n\n\n\n\n\n\nIn epidemiology (and particularly field epidemiology), you will most commonly encounter data frames and vectors:\n\n\n\n\n\n\n\n\nCommon structure\nExplanation\nExample\n\n\n\n\nVectors\nA container for a sequence of singular objects, all of the same class (e.g. numeric, character).\n“Variables” (columns) in data frames are vectors (e.g. the column age_years).\n\n\nData Frames\nVectors (e.g. columns) that are bound together that all have the same number of rows.\nlinelist is a data frame.\n\n\n\nNote that to create a vector that “stands alone” (is not part of a data frame) the function c() is used to combine the different elements. For example, if creating a vector of colors plot’s color scale: vector_of_colors &lt;- c(\"blue\", \"red2\", \"orange\", \"grey\")\n\n\n\n\nAll the objects stored in R have a class which tells R how to handle the object. There are many possible classes, but common ones include:\n\n\n\n\n\n\n\n\nClass\nExplanation\nExamples\n\n\n\n\nCharacter\nThese are text/words/sentences “within quotation marks”. Math cannot be done on these objects.\n“Character objects are in quotation marks”\n\n\nInteger\nNumbers that are whole only (no decimals)\n-5, 14, or 2000\n\n\nNumeric\nThese are numbers and can include decimals. If within quotation marks they will be considered character class.\n23.1 or 14\n\n\nFactor\nThese are vectors that have a specified order or hierarchy of values\nAn variable of economic status with ordered values\n\n\nDate\nOnce R is told that certain data are Dates, these data can be manipulated and displayed in special ways.\n2018-04-12 or 15/3/1954 or Wed 4 Jan 1980\n\n\nLogical\nValues must be one of the two special values TRUE or FALSE (note these are not “TRUE” and “FALSE” in quotation marks)\nTRUE or FALSE\n\n\ndata.frame\nA data frame is how R stores a typical dataset. It consists of vectors (columns) of data bound together, that all have the same number of observations (rows).\nThe example AJS dataset named linelist_raw contains 68 variables with 300 observations (rows) each.\n\n\ntibble\ntibbles are a variation on data frame, the main operational difference being that they print more nicely to the console (display first 10 rows and only columns that fit on the screen)\nAny data frame, list, or matrix can be converted to a tibble with as_tibble()\n\n\nlist\nA list is like vector, but holds other objects that can be other different classes\nA list could hold a single number, and a dataframe, and a vector, and even another list within it!\n\n\n\nYou can test the class of an object by providing its name to the function class(). You can reference a specific column within a dataset using the $ notation to separate the name of the dataset and the name of the column.\n\nclass(linelist)         # class should be a data frame or tibble\n\n[1] \"data.frame\"\n\nclass(linelist$age)     # class should be numeric\n\n[1] \"numeric\"\n\nclass(linelist$gender)  # class should be character\n\n[1] \"character\"\n\n\nSometimes, a column will be converted to a different class automatically by R. Watch out for this! For example, if you have a vector or column of numbers, but a character value is inserted… the entire column will change to class character.\n\nnum_vector &lt;- c(1,2,3,4,5) # define vector as all numbers\nclass(num_vector)          # vector is numeric class\n\n[1] \"numeric\"\n\nnum_vector[3] &lt;- \"three\"   # convert the third element to a character\nclass(num_vector)          # vector is now character class\n\n[1] \"character\"\n\n\nOne common example of this is when manipulating a data frame in order to print a table - if you make a total row and try to paste/glue together percents in the same cell as numbers (e.g. 23 (40%)), the entire numeric column above will convert to character and can no longer be used for mathematical calculations.Sometimes, you will need to convert objects or columns to another class.\n\n\n\nFunction\nAction\n\n\n\n\nas.character()\nConverts to character class\n\n\nas.numeric()\nConverts to numeric class\n\n\nas.integer()\nConverts to integer class\n\n\nas.Date()\nConverts to Date class\n\n\nfactor()\nConverts to factor\n\n\n\nLikewise, there are base R functions to check whether an object IS of a specific class, such as is.numeric(), is.character(), is.double(), is.factor(), is.integer()\nHere is more online material on classes and data structures in R.\n\n\n\n\nA column in a data frame is technically a “vector” (see table above) - a series of values that must all be the same class (either character, numeric, logical, etc).\nA vector can exist independent of a data frame, for example a vector of column names that you want to include as explanatory variables in a model. To create a “stand alone” vector, use the c() function as below:\n\n# define the stand-alone vector of character values\nexplanatory_vars &lt;- c(\"gender\", \"fever\", \"chills\", \"cough\", \"aches\", \"vomit\")\n\n# print the values in this named vector\nexplanatory_vars\n\n[1] \"gender\" \"fever\"  \"chills\" \"cough\"  \"aches\"  \"vomit\" \n\n\nColumns in a data frame are also vectors and can be called, referenced, extracted, or created using the $ symbol. The $ symbol connects the name of the column to the name of its data frame. In this handbook, we try to use the word “column” instead of “variable”.\n\n# Retrieve the length of the vector age_years\nlength(linelist$age) # (age is a column in the linelist data frame)\n\n\n\n\n\nYou may need to view parts of objects, also called “indexing”, which is often done using the square brackets [ ]. Using $ on a dataframe to access a column is also a type of indexing.\n\nmy_vector &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")  # define the vector\nmy_vector[5]                                  # print the 5th element\n\n[1] \"e\"\n\n\nSquare brackets also work to return specific parts of an returned output, such as the output of a summary() function:\n\n# All of the summary\nsummary(linelist$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.07   23.00   84.00      86 \n\n# Just the second element of the summary, with name (using only single brackets)\nsummary(linelist$age)[2]\n\n1st Qu. \n      6 \n\n# Just the second element, without name (using double brackets)\nsummary(linelist$age)[[2]]\n\n[1] 6\n\n# Extract an element by name, without showing the name\nsummary(linelist$age)[[\"Median\"]]\n\n[1] 13\n\n\nBrackets also work on data frames to view specific rows and columns. You can do this using the syntax dataframe[rows, columns]:\n\n# View a specific row (2) from dataset, with all columns (don't forget the comma!)\nlinelist[2,]\n\n# View all rows, but just one column\nlinelist[, \"date_onset\"]\n\n# View values from row 2 and columns 5 through 10\nlinelist[2, 5:10] \n\n# View values from row 2 and columns 5 through 10 and 18\nlinelist[2, c(5:10, 18)] \n\n# View rows 2 through 20, and specific columns\nlinelist[2:20, c(\"date_onset\", \"outcome\", \"age\")]\n\n# View rows and columns based on criteria\n# *** Note the dataframe must still be named in the criteria!\nlinelist[linelist$age &gt; 25 , c(\"date_onset\", \"outcome\", \"age\")]\n\n# Use View() to see the outputs in the RStudio Viewer pane (easier to read) \n# *** Note the capital \"V\" in View() function\nView(linelist[2:20, \"date_onset\"])\n\n# Save as a new object\nnew_table &lt;- linelist[2:20, c(\"date_onset\")] \n\n\n\n\n\n\n\nUsing Tidyverse Instead\n\n\n\n\n\nIn a future session, we will learn how to perform many of these operations using dplyr syntax (functions filter() for rows, and select() for columns) as opposed to the base R syntax shown here.\nTo filter based on “row number”, you can use the dplyr function row_number() with open parentheses as part of a logical filtering statement. Often you will use the %in% operator and a range of numbers as part of that logical statement, as shown below. To see the first N rows, you can also use the special dplyr function head().\n\n# View first 100 rows\nlinelist %&gt;% head(100)\n\n# Show row 5 only\nlinelist %&gt;% filter(row_number() == 5)\n\n# View rows 2 through 20, and three specific columns (note no quotes necessary on column names)\nlinelist %&gt;% filter(row_number() %in% 2:20) %&gt;% select(date_onset, outcome, age)\n\n\n\n\nWhen indexing an object of class list, single brackets always return with class list, even if only a single object is returned. Double brackets, however, can be used to access a single element and return a different class than list.\nBrackets can also be written after one another, as demonstrated below.\nThis visual explanation of lists indexing, with pepper shakers is humorous and helpful.\n\n# define demo list\nmy_list &lt;- list(\n  # First element in the list is a character vector\n  hospitals = c(\"Central\", \"Empire\", \"Santa Anna\"),\n  \n  # second element in the list is a data frame of addresses\n  addresses   = data.frame(\n    street = c(\"145 Medical Way\", \"1048 Brown Ave\", \"999 El Camino\"),\n    city   = c(\"Andover\", \"Hamilton\", \"El Paso\")\n    )\n  )\n\nHere is how the list looks when printed to the console. See how there are two named elements:\n\nhospitals, a character vector\n\naddresses, a data frame of addresses\n\n\nmy_list\n\n$hospitals\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\n$addresses\n           street     city\n1 145 Medical Way  Andover\n2  1048 Brown Ave Hamilton\n3   999 El Camino  El Paso\n\n\nNow we extract, using various methods:\n\nmy_list[1] # this returns the element in class \"list\" - the element name is still displayed\n\n$hospitals\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[1]] # this returns only the (unnamed) character vector\n\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[\"hospitals\"]] # you can also index by name of the list element\n\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[1]][3] # this returns the third element of the \"hospitals\" character vector\n\n[1] \"Santa Anna\"\n\nmy_list[[2]][1] # This returns the first column (\"street\") of the address data frame\n\n           street\n1 145 Medical Way\n2  1048 Brown Ave\n3   999 El Camino\n\n\n\n\n\n\nYou can remove individual objects from your R environment by putting the name in the rm() function (no quote marks):\n\nrm(object_name)\n\nYou can remove all objects (clear your workspace) by running:\n\nrm(list = ls(all = TRUE))\n\n\n\n\n\nSince factors are special vectors, the same rules for selecting values using indices apply.\n\nexpression &lt;- c(\"high\",\"low\",\"low\",\"medium\",\"high\",\"medium\",\"medium\",\"low\",\"low\",\"low\")\n\nThe elements of this expression factor created previously has following categories or levels: low, medium, and high.\nLet’s extract the values of the factor with high expression, and let’s using nesting here:\n\nexpression[expression == \"high\"]    ## This will only return those elements in the factor equal to \"high\"\n\n[1] \"high\" \"high\"\n\n\n\nNesting note:\nThe piece of code above was more efficient with nesting; we used a single step instead of two steps as shown below:\nStep1 (no nesting): idx &lt;- expression == \"high\"\nStep2 (no nesting): expression[idx]\n\n\n\nWe have briefly talked about factors, but this data type only becomes more intuitive once you’ve had a chance to work with it. Let’s take a slight detour and learn about how to relevel categories within a factor.\nTo view the integer assignments under the hood you can use str():\n\nexpression\n\n [1] \"high\"   \"low\"    \"low\"    \"medium\" \"high\"   \"medium\" \"medium\" \"low\"   \n [9] \"low\"    \"low\"   \n\n\nThe categories are referred to as “factor levels”. As we learned earlier, the levels in the expression factor were assigned integers alphabetically, with high=1, low=2, medium=3. However, it makes more sense for us if low=1, medium=2 and high=3, i.e. it makes sense for us to “relevel” the categories in this factor.\nTo relevel the categories, you can add the levels argument to the factor() function, and give it a vector with the categories listed in the required order:\n\nexpression &lt;- factor(expression, levels=c(\"low\", \"medium\", \"high\"))     # you can re-factor a factor \n\nNow we have a releveled factor with low as the lowest or first category, medium as the second and high as the third. This is reflected in the way they are listed in the output of str(), as well as in the numbering of which category is where in the factor.\n\nNote: Releveling becomes necessary when you need a specific category in a factor to be the “base” category, i.e. category that is equal to 1. One example would be if you need the “control” to be the “base” in a given RNA-seq experiment.\n\n\n\n\n\n\n\n\nTwo general approaches to working with objects are:\n\nPipes/tidyverse - pipes send an object from function to function - emphasis is on the action, not the object\n\nDefine intermediate objects - an object is re-defined again and again - emphasis is on the object\n\n\n\n\nSimply explained, the pipe operator (%&gt;%) passes an intermediate output from one function to the next.\nYou can think of it as saying “then”. Many functions can be linked together with %&gt;%.\n\nPiping emphasizes a sequence of actions, not the object the actions are being performed on\n\nPipes are best when a sequence of actions must be performed on one object\n\nPipes come from the package magrittr, which is automatically included in packages dplyr and tidyverse\nPipes can make code more clean and easier to read, more intuitive\n\nRead more on this approach in the tidyverse style guide\nHere is a fake example for comparison, using fictional functions to “bake a cake”. First, the pipe method:\n\n# A fake example of how to bake a cake using piping syntax\n\ncake &lt;- flour %&gt;%       # to define cake, start with flour, and then...\n  add(eggs) %&gt;%   # add eggs\n  add(oil) %&gt;%    # add oil\n  add(water) %&gt;%  # add water\n  mix_together(         # mix together\n    utensil = spoon,\n    minutes = 2) %&gt;%    \n  bake(degrees = 350,   # bake\n       system = \"fahrenheit\",\n       minutes = 35) %&gt;%  \n  let_cool()            # let it cool down\n\nHere is another link describing the utility of pipes.\nPiping is not a base function. To use piping, the magrittr package must be installed and loaded (this is typically done by loading tidyverse or dplyr package which include it). You can read more about piping in the magrittr documentation.\nNote that just like other R commands, pipes can be used to just display the result, or to save/re-save an object, depending on whether the assignment operator &lt;- is involved. See both below:\n\n# Create or overwrite object, defining as aggregate counts by age category (not printed)\nlinelist_summary &lt;- linelist %&gt;% \n  count(age_cat)\n\n\n# Print the table of counts in the console, but don't save it\nlinelist %&gt;% \n  count(age_cat)\n\n  age_cat    n\n1     0-4 1095\n2     5-9 1095\n3   10-14  941\n4   15-19  743\n5   20-29 1073\n6   30-49  754\n7   50-69   95\n8     70+    6\n9    &lt;NA&gt;   86\n\n\n%&lt;&gt;%\nThis is an “assignment pipe” from the magrittr package, which pipes an object forward and also re-defines the object. It must be the first pipe operator in the chain. It is shorthand. The below two commands are equivalent:\n\nlinelist &lt;- linelist %&gt;%\n  filter(age &gt; 50)\n\nlinelist %&lt;&gt;% filter(age &gt; 50)\n\n\n\n\n\nThis approach to changing objects/dataframes may be better if:\n\nYou need to manipulate multiple objects\n\nThere are intermediate steps that are meaningful and deserve separate object names\n\nRisks:\n\nCreating new objects for each step means creating lots of objects. If you use the wrong one you might not realize it!\n\nNaming all the objects can be confusing\n\nErrors may not be easily detectable\n\nEither name each intermediate object, or overwrite the original, or combine all the functions together. All come with their own risks.\nBelow is the same fake “cake” example as above, but using this style:\n\n# a fake example of how to bake a cake using this method (defining intermediate objects)\nbatter_1 &lt;- left_join(flour, eggs)\nbatter_2 &lt;- left_join(batter_1, oil)\nbatter_3 &lt;- left_join(batter_2, water)\n\nbatter_4 &lt;- mix_together(object = batter_3, utensil = spoon, minutes = 2)\n\ncake &lt;- bake(batter_4, degrees = 350, system = \"fahrenheit\", minutes = 35)\n\ncake &lt;- let_cool(cake)\n\nCombine all functions together - this is difficult to read:\n\n# an example of combining/nesting mutliple functions together - difficult to read\ncake &lt;- let_cool(bake(mix_together(batter_3, utensil = spoon, minutes = 2), degrees = 350, system = \"fahrenheit\", minutes = 35))\n\n\n\n\n\n\n\nWhen a command is run, the R Console may show you warning or error messages in red text.\n\nA warning means that R has completed your command, but had to take additional steps or produced unusual output that you should be aware of.\nAn error means that R was not able to complete your command.\n\nLook for clues:\n\nThe error/warning message will often include a line number for the problem.\nIf an object “is unknown” or “not found”, perhaps you spelled it incorrectly, forgot to call a package with library(), or forgot to re-run your script after making changes.\n\nIf all else fails, copy the error message into Google along with some key terms - chances are that someone else has worked through this already!\n\n\n\n\nA few things to remember when writing commands in R, to avoid errors and warnings:\n\nAlways close parentheses - tip: count the number of opening “(” and closing parentheses “)” for each code chunk\nAvoid spaces in column and object names. Use underscore ( _ ) or periods ( . ) instead\nKeep track of and remember to separate a function’s arguments with commas\n\n\n\n\n\nAny script (RMarkdown or otherwise) will give clues when you have made a mistake. For example, if you forgot to write a comma where it is needed, or to close a parentheses, RStudio will raise a flag on that line, on the right side of the script, to warn you."
  },
  {
    "objectID": "readings/basics.html#functions",
    "href": "readings/basics.html#functions",
    "title": "R Basics",
    "section": "",
    "text": "Functions are at the core of using R. Functions are how you perform tasks and operations. Many functions come installed with R, many more are available for download in packages (explained in the packages section), and you can even write your own custom functions!\nThis basics section on functions explains:\n\nWhat a function is and how they work\n\nWhat function arguments are\n\nHow to get help understanding a function\n\n\n\n\n\n\n\nA quick note on syntax\n\n\n\n\n\nIn this workbook, functions are written in code-text with open parentheses, like this: filter(). As explained in the packages section, functions are downloaded within packages. In this handbook, package names are written in bold, like dplyr. Sometimes in example code you may see the function name linked explicitly to the name of its package with two colons (::) like this: dplyr::filter(). The purpose of this linkage is explained in the packages section.\n\n\n\n\n\n\nA function is like a machine that receives inputs, does some action with those inputs, and produces an output. What the output is depends on the function.\nFunctions typically operate upon some object placed within the function’s parentheses. For example, the function sqrt() calculates the square root of a number:\n\nsqrt(49)\n\n[1] 7\n\n\nThe object provided to a function also can be a column in a dataset (see the Objects section for detail on all the kinds of objects). Because R can store multiple datasets, you will need to specify both the dataset and the column. One way to do this is using the $ notation to link the name of the dataset and the name of the column (dataset$column). In the example below, the function summary() is applied to the numeric column age in the dataset linelist, and the output is a summary of the column’s numeric and missing values.\n\n# Print summary statistics of column 'age' in the dataset 'linelist'\nsummary(linelist$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.07   23.00   84.00      86 \n\n\n\n\n\n\n\n\nNote\n\n\n\nBehind the scenes, a function represents complex additional code that has been wrapped up for the user into one easy command.\n\n\n\n\n\n\nFunctions often ask for several inputs, called arguments, located within the parentheses of the function, usually separated by commas.\n\nSome arguments are required for the function to work correctly, others are optional\n\nOptional arguments have default settings\n\nArguments can take character, numeric, logical (TRUE/FALSE), and other inputs\n\nHere is a fun fictional function, called oven_bake(), as an example of a typical function. It takes an input object (e.g. a dataset, or in this example “dough”) and performs operations on it as specified by additional arguments (minutes = and temperature =). The output can be printed to the console, or saved as an object using the assignment operator &lt;-.\n\n\n\n\n\n\n\n\n\nIn a more realistic example, the age_pyramid() command below produces an age pyramid plot based on defined age groups and a binary split column, such as gender. The function is given three arguments within the parentheses, separated by commas. The values supplied to the arguments establish linelist as the dataframe to use, age_cat5 as the column to count, and gender as the binary column to use for splitting the pyramid by color.\n\n# Create an age pyramid\nage_pyramid(data = linelist, age_group = \"age_cat5\", split_by = \"gender\")\n\n\n\n\nThe above command can be equivalently written as below, in a longer style with a new line for each argument. This style can be easier to read, and easier to write “comments” with # to explain each part (commenting extensively is good practice!). To run this longer command you can highlight the entire command and click “Run”, or just place your cursor in the first line and then press the Ctrl and Enter keys simultaneously.\n\n# Create an age pyramid\nage_pyramid(\n  data = linelist,        # use case linelist\n  age_group = \"age_cat5\", # provide age group column\n  split_by = \"gender\"     # use gender column for two sides of pyramid\n  )\n\n\n\n\nThe first half of an argument assignment (e.g. data =) does not need to be specified if the arguments are written in a specific order (specified in the function’s documentation). The below code produces the exact same pyramid as above, because the function expects the argument order: data frame, age_group variable, split_by variable.\n\n# This command will produce the exact same graphic as above\nage_pyramid(linelist, \"age_cat5\", \"gender\")\n\nA more complex age_pyramid() command might include the optional arguments to:\n\nShow proportions instead of counts (set proportional = TRUE when the default is FALSE)\n\nSpecify the two colors to use (pal = is short for “palette” and is supplied with a vector of two color names. See the objects page for how the function c() makes a vector)\n\n\n\n\n\n\n\nNote\n\n\n\nFor arguments that you specify with both parts of the argument (e.g. proportional = TRUE), their order among all the arguments does not matter.\n\n\n\nage_pyramid(\n  linelist,                    # use case linelist\n  \"age_cat5\",                  # age group column\n  \"gender\",                    # split by gender\n  proportional = TRUE,         # percents instead of counts\n  pal = c(\"orange\", \"purple\")  # colors\n  )"
  },
  {
    "objectID": "readings/basics.html#packages",
    "href": "readings/basics.html#packages",
    "title": "R Basics",
    "section": "",
    "text": "Packages contain functions.\nAn R package is a shareable bundle of code and documentation that contains pre-defined functions. Users in the R community develop packages all the time catered to specific problems, it is likely that one can help with your work! You will install and use hundreds of packages in your use of R.\nOn installation, R contains “base” packages and functions that perform common elementary tasks. But many R users create specialized functions, which are verified by the R community and which you can download as a package for your own use. In this handbook, package names are written in bold. One of the more challenging aspects of R is that there are often many functions or packages to choose from to complete a given task.\n\n\nFunctions are contained within packages which can be downloaded (“installed”) to your computer from the internet. Once a package is downloaded, it is stored in your “library”. You can then access the functions it contains during your current R session by “loading” the package.\nThink of R as your personal library: When you download a package, your library gains a new book of functions, but each time you want to use a function in that book, you must borrow (“load”) that book from your library.\nIn summary: to use the functions available in an R package, 2 steps must be implemented:\n\nThe package must be installed (once), and\n\nThe package must be loaded (each R session)\n\n\n\nYour “library” is actually a folder on your computer, containing a folder for each package that has been installed. Find out where R is installed in your computer, and look for a folder called “win-library”. For example: R\\win-library\\4.0 (the 4.0 is the R version - you’ll have a different library for each R version you’ve downloaded).\nYou can print the file path to your library by entering .libPaths() (empty parentheses).\n\n\n\nMost often, R users download packages from CRAN. CRAN (Comprehensive R Archive Network) is an online public warehouse of R packages that have been published by R community members.\n\n\n\nThe base R function for installing a package is install.packages(). The name of the package to install must be provided in the parentheses in quotes. If you want to install multiple packages in one command, they must be listed within a character vector c().\n\n\n\n\n\n\nCommon Mistake\n\n\n\nThis command installs a package, but does not load it for use in the current session.\n\n\n\n# install a single package with base R\ninstall.packages(\"tidyverse\")\n\n# install multiple packages with base R\ninstall.packages(c(\"tidyverse\", \"rio\", \"here\"))\n\nInstallation can also be accomplished point-and-click by going to the RStudio “Packages” pane and clicking “Install” and searching for the desired package name.\nThe base R function to load a package for use (after it has been installed) is library(). It can load only one package at a time (another reason to use p_load()). You can provide the package name with or without quotes.\n\n# load packages for use, with base R\nlibrary(tidyverse)\nlibrary(rio)\nlibrary(here)\n\nTo check whether a package in installed and/or loaded, you can view the Packages pane in RStudio. If the package is installed, it is shown there with version number. If its box is checked, it is loaded for the current session.\n\n\n\n\nFor clarity in this handbook, functions are sometimes preceded by the name of their package using the :: symbol in the following way: package_name::function_name()\nOnce a package is loaded for a session, this explicit style is not necessary. One can just use function_name(). However writing the package name is useful when a function name is common and may exist in multiple packages (e.g. plot()). Writing the package name will also load the package if it is not already loaded.\n\n# This command uses the package \"rio\" and its function \"import()\" to import a dataset\nlinelist &lt;- rio::import(\"linelist.xlsx\", which = \"Sheet1\")\n\n\n\n\nTo read more about a function, you can search for it in the Help tab of the lower-right RStudio. You can also run a command like ?thefunctionname (put the name of the function after a question mark) and the Help page will appear in the Help pane. Finally, try searching online for resources.\n\n\n\nYou can update packages by re-installing them. You can also click the green “Update” button in your RStudio Packages pane to see which packages have new versions Wto install. Be aware that your old code may need to be updated if there is a major revision to how a function works!\n\n\n\nUse remove.packages(). Alternatively, go find the folder which contains your library and manually delete the folder.\n\n\n\nPackages often depend on other packages to work. These are called dependencies. If a dependency fails to install, then the package depending on it may also fail to install.\n\n\n\nIt is not uncommon that two or more packages contain the same function name. For example, the package dplyr has a filter() function, but so does the package stats. The default filter() function depends on the order these packages are first loaded in the R session - the later one will be the default for the command filter().\nYou can check the order in your Environment pane of R Studio - click the drop-down for “Global Environment” and see the order of the packages. Functions from packages lower on that drop-down list will mask functions of the same name in packages that appear higher in the drop-down list. When first loading a package, R will warn you in the console if masking is occurring, but this can be easy to miss.\n\n\n\n\n\n\n\n\n\nHere are ways you can fix masking:\n\nSpecify the package name in the command. For example, use dplyr::filter()\n\nRe-arrange the order in which the packages are loaded and start a new R session\n\n\n\n\nSee this guide to install an older version of a particular package."
  },
  {
    "objectID": "readings/basics.html#scripts",
    "href": "readings/basics.html#scripts",
    "title": "R Basics",
    "section": "",
    "text": "Scripts are a fundamental part of programming. They are documents that hold your commands (e.g. functions to create and modify datasets, print visualizations, etc). You can save a script and run it again later. There are many advantages to storing and running your commands from a script (vs. typing commands one-by-one into the R console “command line”):\n\nPortability - you can share your work with others by sending them your scripts\n\nReproducibility - so that you and others know exactly what you did\n\nVersion control - so you can track changes made by yourself or colleagues\n\nCommenting/annotation - to explain to your colleagues what you have done\n\n\n\nIn a script you can also annotate (“comment”) around your R code. Commenting is helpful to explain to yourself and other readers what you are doing. You can add a comment by typing the hash symbol (#) and writing your comment after it. The commented text will appear in a different color than the R code.\nAny code written after the # will not be run. Therefore, placing a # before code is also a useful way to temporarily block a line of code (“comment out”) if you do not want to delete it). You can comment out/in multiple lines at once by highlighting them and pressing Ctrl+Shift+c (Cmd+Shift+c in Mac).\n\n# A comment can be on a line by itself\n# import data\nlinelist &lt;- import(\"linelist_raw.xlsx\") %&gt;%   # a comment can also come after code\n# filter(age &gt; 50)                          # It can also be used to deactivate / remove a line of code\n  count()\n\n\nComment on what you are doing and on why you are doing it.\n\nBreak your code into logical sections\n\nAccompany your code with a text step-by-step description of what you are doing (e.g. numbered steps)\n\n\n\n\nIt is important to be conscious of your coding style - especially if working on a team. Some nice R styles guide are the tidyverse style guide, Hadley Wickham’s style guide, and Google’s. There are also packages such as styler and lintr which help you conform to a style.\nA few very basic points to make your code readable to others:\n* When naming objects, use only lowercase letters, numbers, and underscores _, e.g. my_data\n* Use frequent spaces, including around operators, e.g. n = 1 and age_new &lt;- age_old + 3\n\n\n\nBelow is an example of a short R script. Remember, the better you succinctly explain your code in comments, the more your colleagues will like you!\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R markdown script is a type of R script in which the script itself becomes an output document (PDF, Word, HTML, Powerpoint, etc.). Quarto is a new version of R markdown. These are incredibly useful and versatile tools often used to create dynamic and automated reports. Even this website is produced with Quarto!"
  },
  {
    "objectID": "readings/basics.html#objects",
    "href": "readings/basics.html#objects",
    "title": "R Basics",
    "section": "",
    "text": "Everything in R is an object, and R is an “object-oriented” language. These sections will explain:\n\nHow to create objects (&lt;-)\nTypes of objects (e.g. data frames, vectors..)\n\nHow to access subparts of objects (e.g. variables in a dataset)\n\nClasses of objects (e.g. numeric, logical, integer, double, character, factor)\n\n\n\n\nEverything you store in R - datasets, variables, a list of village names, a total population number, even outputs such as graphs - are objects which are assigned a name and can be referenced in later commands.\nAn object exists when you have assigned it a value (see the assignment section below). When it is assigned a value, the object appears in the Environment (see the upper right pane of RStudio). It can then be operated upon, manipulated, changed, and re-defined.\n\n\n\n\nCreate objects by assigning them a value with the &lt;- operator.\nYou can think of the assignment operator &lt;- as the words “is defined as”. Assignment commands generally follow a standard order:\nobject_name &lt;- value (or process/calculation that produce a value)\nFor example, you may want to record the current epidemiological reporting week as an object for reference in later code. In this example, the object current_week is created when it is assigned the value \"2018-W10\" (the quote marks make this a character value). The object current_week will then appear in the RStudio Environment pane (upper-right) and can be referenced in later commands.\nSee the R commands and their output in the boxes below.\n\ncurrent_week &lt;- \"2018-W10\"   # this command creates the object current_week by assigning it a value\ncurrent_week                 # this command prints the current value of current_week object in the console\n\n[1] \"2018-W10\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote the [1] in the R console output is simply indicating that you are viewing the first item of the output\n\n\n\n\n\n\n\n\nCommon Mistake\n\n\n\nAn object’s value can be over-written at any time by running an assignment command to re-define its value. Thus, the order of the commands run is very important.\nFor instance, the following command will re-define the value of current_week:\n\ncurrent_week &lt;- \"2018-W51\"   # assigns a NEW value to the object current_week\ncurrent_week                 # prints the current value of current_week in the console\n\n[1] \"2018-W51\"\n\n\n\n\nEquals signs =\nYou will also see equals signs in R code:\n\nA double equals sign == between two objects or values asks a logical question: “is this equal to that?”.\n\nYou will also see equals signs within functions used to specify values of function arguments (read about these in sections below), for example max(age, na.rm = TRUE).\n\nYou can use a single equals sign = in place of &lt;- to create and define objects, but this is discouraged. You can read about why this is discouraged here.\n\nDatasets\nDatasets are also objects (typically “dataframes”) and must be assigned names when they are imported. In the code below, the object linelist is created and assigned the value of a CSV file imported with the rio package and its import() function.\n\n# linelist is created and assigned the value of the imported CSV file\nlinelist &lt;- import(\"my_linelist.csv\")\n\n\n\n\n\n\n\nNaming Objects\n\n\n\n\nObject names must not contain spaces, but you should use underscore (_) or a period (.) instead of a space.\n\nObject names are case-sensitive (meaning that Dataset_A is different from dataset_A).\nObject names must begin with a letter (cannot begin with a number like 1, 2 or 3).\n\n\n\nOutputs\nOutputs like tables and plots provide an example of how outputs can be saved as objects, or just be printed without being saved. A cross-tabulation of gender and outcome using the base R function table() can be printed directly to the R console (without being saved).\n\n# printed to R console only\ntable(linelist$gender, linelist$outcome)\n\n   \n    Death Recover\n  f  1227     953\n  m  1228     950\n\n\nBut the same table can be saved as a named object. Then, optionally, it can be printed.\n\n# save\ngen_out_table &lt;- table(linelist$gender, linelist$outcome)\n\n# print\ngen_out_table\n\n   \n    Death Recover\n  f  1227     953\n  m  1228     950\n\n\nColumns\nColumns in a dataset are also objects and can be defined, over-written, and created as described below in the section on Columns.\nYou can use the assignment operator from base R to create a new column. Below, the new column bmi (Body Mass Index) is created, and for each row the new value is result of a mathematical operation on the row’s value in the wt_kg and ht_cm columns.\n\n# create new \"bmi\" column using base R syntax\nlinelist$bmi &lt;- linelist$wt_kg / (linelist$ht_cm/100)^2\n\nHowever, in this handbook, we emphasize a different approach to defining columns, which uses the function mutate() from the dplyr package and piping with the pipe operator (%&gt;%). The syntax is easier to read and there are other advantages.\n\n# create new \"bmi\" column using dplyr syntax\nlinelist &lt;- linelist %&gt;% \n  mutate(bmi = wt_kg / (ht_cm/100)^2)\n\n\n\n\n\nObjects can be a single piece of data (e.g. my_number &lt;- 24), or they can consist of structured data.\nThe graphic below is borrowed from this online R tutorial. It shows some common data structures and their names.\n\n\n\n\n\n\n\n\n\nIn epidemiology (and particularly field epidemiology), you will most commonly encounter data frames and vectors:\n\n\n\n\n\n\n\n\nCommon structure\nExplanation\nExample\n\n\n\n\nVectors\nA container for a sequence of singular objects, all of the same class (e.g. numeric, character).\n“Variables” (columns) in data frames are vectors (e.g. the column age_years).\n\n\nData Frames\nVectors (e.g. columns) that are bound together that all have the same number of rows.\nlinelist is a data frame.\n\n\n\nNote that to create a vector that “stands alone” (is not part of a data frame) the function c() is used to combine the different elements. For example, if creating a vector of colors plot’s color scale: vector_of_colors &lt;- c(\"blue\", \"red2\", \"orange\", \"grey\")\n\n\n\n\nAll the objects stored in R have a class which tells R how to handle the object. There are many possible classes, but common ones include:\n\n\n\n\n\n\n\n\nClass\nExplanation\nExamples\n\n\n\n\nCharacter\nThese are text/words/sentences “within quotation marks”. Math cannot be done on these objects.\n“Character objects are in quotation marks”\n\n\nInteger\nNumbers that are whole only (no decimals)\n-5, 14, or 2000\n\n\nNumeric\nThese are numbers and can include decimals. If within quotation marks they will be considered character class.\n23.1 or 14\n\n\nFactor\nThese are vectors that have a specified order or hierarchy of values\nAn variable of economic status with ordered values\n\n\nDate\nOnce R is told that certain data are Dates, these data can be manipulated and displayed in special ways.\n2018-04-12 or 15/3/1954 or Wed 4 Jan 1980\n\n\nLogical\nValues must be one of the two special values TRUE or FALSE (note these are not “TRUE” and “FALSE” in quotation marks)\nTRUE or FALSE\n\n\ndata.frame\nA data frame is how R stores a typical dataset. It consists of vectors (columns) of data bound together, that all have the same number of observations (rows).\nThe example AJS dataset named linelist_raw contains 68 variables with 300 observations (rows) each.\n\n\ntibble\ntibbles are a variation on data frame, the main operational difference being that they print more nicely to the console (display first 10 rows and only columns that fit on the screen)\nAny data frame, list, or matrix can be converted to a tibble with as_tibble()\n\n\nlist\nA list is like vector, but holds other objects that can be other different classes\nA list could hold a single number, and a dataframe, and a vector, and even another list within it!\n\n\n\nYou can test the class of an object by providing its name to the function class(). You can reference a specific column within a dataset using the $ notation to separate the name of the dataset and the name of the column.\n\nclass(linelist)         # class should be a data frame or tibble\n\n[1] \"data.frame\"\n\nclass(linelist$age)     # class should be numeric\n\n[1] \"numeric\"\n\nclass(linelist$gender)  # class should be character\n\n[1] \"character\"\n\n\nSometimes, a column will be converted to a different class automatically by R. Watch out for this! For example, if you have a vector or column of numbers, but a character value is inserted… the entire column will change to class character.\n\nnum_vector &lt;- c(1,2,3,4,5) # define vector as all numbers\nclass(num_vector)          # vector is numeric class\n\n[1] \"numeric\"\n\nnum_vector[3] &lt;- \"three\"   # convert the third element to a character\nclass(num_vector)          # vector is now character class\n\n[1] \"character\"\n\n\nOne common example of this is when manipulating a data frame in order to print a table - if you make a total row and try to paste/glue together percents in the same cell as numbers (e.g. 23 (40%)), the entire numeric column above will convert to character and can no longer be used for mathematical calculations.Sometimes, you will need to convert objects or columns to another class.\n\n\n\nFunction\nAction\n\n\n\n\nas.character()\nConverts to character class\n\n\nas.numeric()\nConverts to numeric class\n\n\nas.integer()\nConverts to integer class\n\n\nas.Date()\nConverts to Date class\n\n\nfactor()\nConverts to factor\n\n\n\nLikewise, there are base R functions to check whether an object IS of a specific class, such as is.numeric(), is.character(), is.double(), is.factor(), is.integer()\nHere is more online material on classes and data structures in R.\n\n\n\n\nA column in a data frame is technically a “vector” (see table above) - a series of values that must all be the same class (either character, numeric, logical, etc).\nA vector can exist independent of a data frame, for example a vector of column names that you want to include as explanatory variables in a model. To create a “stand alone” vector, use the c() function as below:\n\n# define the stand-alone vector of character values\nexplanatory_vars &lt;- c(\"gender\", \"fever\", \"chills\", \"cough\", \"aches\", \"vomit\")\n\n# print the values in this named vector\nexplanatory_vars\n\n[1] \"gender\" \"fever\"  \"chills\" \"cough\"  \"aches\"  \"vomit\" \n\n\nColumns in a data frame are also vectors and can be called, referenced, extracted, or created using the $ symbol. The $ symbol connects the name of the column to the name of its data frame. In this handbook, we try to use the word “column” instead of “variable”.\n\n# Retrieve the length of the vector age_years\nlength(linelist$age) # (age is a column in the linelist data frame)\n\n\n\n\n\nYou may need to view parts of objects, also called “indexing”, which is often done using the square brackets [ ]. Using $ on a dataframe to access a column is also a type of indexing.\n\nmy_vector &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")  # define the vector\nmy_vector[5]                                  # print the 5th element\n\n[1] \"e\"\n\n\nSquare brackets also work to return specific parts of an returned output, such as the output of a summary() function:\n\n# All of the summary\nsummary(linelist$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    6.00   13.00   16.07   23.00   84.00      86 \n\n# Just the second element of the summary, with name (using only single brackets)\nsummary(linelist$age)[2]\n\n1st Qu. \n      6 \n\n# Just the second element, without name (using double brackets)\nsummary(linelist$age)[[2]]\n\n[1] 6\n\n# Extract an element by name, without showing the name\nsummary(linelist$age)[[\"Median\"]]\n\n[1] 13\n\n\nBrackets also work on data frames to view specific rows and columns. You can do this using the syntax dataframe[rows, columns]:\n\n# View a specific row (2) from dataset, with all columns (don't forget the comma!)\nlinelist[2,]\n\n# View all rows, but just one column\nlinelist[, \"date_onset\"]\n\n# View values from row 2 and columns 5 through 10\nlinelist[2, 5:10] \n\n# View values from row 2 and columns 5 through 10 and 18\nlinelist[2, c(5:10, 18)] \n\n# View rows 2 through 20, and specific columns\nlinelist[2:20, c(\"date_onset\", \"outcome\", \"age\")]\n\n# View rows and columns based on criteria\n# *** Note the dataframe must still be named in the criteria!\nlinelist[linelist$age &gt; 25 , c(\"date_onset\", \"outcome\", \"age\")]\n\n# Use View() to see the outputs in the RStudio Viewer pane (easier to read) \n# *** Note the capital \"V\" in View() function\nView(linelist[2:20, \"date_onset\"])\n\n# Save as a new object\nnew_table &lt;- linelist[2:20, c(\"date_onset\")] \n\n\n\n\n\n\n\nUsing Tidyverse Instead\n\n\n\n\n\nIn a future session, we will learn how to perform many of these operations using dplyr syntax (functions filter() for rows, and select() for columns) as opposed to the base R syntax shown here.\nTo filter based on “row number”, you can use the dplyr function row_number() with open parentheses as part of a logical filtering statement. Often you will use the %in% operator and a range of numbers as part of that logical statement, as shown below. To see the first N rows, you can also use the special dplyr function head().\n\n# View first 100 rows\nlinelist %&gt;% head(100)\n\n# Show row 5 only\nlinelist %&gt;% filter(row_number() == 5)\n\n# View rows 2 through 20, and three specific columns (note no quotes necessary on column names)\nlinelist %&gt;% filter(row_number() %in% 2:20) %&gt;% select(date_onset, outcome, age)\n\n\n\n\nWhen indexing an object of class list, single brackets always return with class list, even if only a single object is returned. Double brackets, however, can be used to access a single element and return a different class than list.\nBrackets can also be written after one another, as demonstrated below.\nThis visual explanation of lists indexing, with pepper shakers is humorous and helpful.\n\n# define demo list\nmy_list &lt;- list(\n  # First element in the list is a character vector\n  hospitals = c(\"Central\", \"Empire\", \"Santa Anna\"),\n  \n  # second element in the list is a data frame of addresses\n  addresses   = data.frame(\n    street = c(\"145 Medical Way\", \"1048 Brown Ave\", \"999 El Camino\"),\n    city   = c(\"Andover\", \"Hamilton\", \"El Paso\")\n    )\n  )\n\nHere is how the list looks when printed to the console. See how there are two named elements:\n\nhospitals, a character vector\n\naddresses, a data frame of addresses\n\n\nmy_list\n\n$hospitals\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\n$addresses\n           street     city\n1 145 Medical Way  Andover\n2  1048 Brown Ave Hamilton\n3   999 El Camino  El Paso\n\n\nNow we extract, using various methods:\n\nmy_list[1] # this returns the element in class \"list\" - the element name is still displayed\n\n$hospitals\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[1]] # this returns only the (unnamed) character vector\n\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[\"hospitals\"]] # you can also index by name of the list element\n\n[1] \"Central\"    \"Empire\"     \"Santa Anna\"\n\nmy_list[[1]][3] # this returns the third element of the \"hospitals\" character vector\n\n[1] \"Santa Anna\"\n\nmy_list[[2]][1] # This returns the first column (\"street\") of the address data frame\n\n           street\n1 145 Medical Way\n2  1048 Brown Ave\n3   999 El Camino\n\n\n\n\n\n\nYou can remove individual objects from your R environment by putting the name in the rm() function (no quote marks):\n\nrm(object_name)\n\nYou can remove all objects (clear your workspace) by running:\n\nrm(list = ls(all = TRUE))\n\n\n\n\n\nSince factors are special vectors, the same rules for selecting values using indices apply.\n\nexpression &lt;- c(\"high\",\"low\",\"low\",\"medium\",\"high\",\"medium\",\"medium\",\"low\",\"low\",\"low\")\n\nThe elements of this expression factor created previously has following categories or levels: low, medium, and high.\nLet’s extract the values of the factor with high expression, and let’s using nesting here:\n\nexpression[expression == \"high\"]    ## This will only return those elements in the factor equal to \"high\"\n\n[1] \"high\" \"high\"\n\n\n\nNesting note:\nThe piece of code above was more efficient with nesting; we used a single step instead of two steps as shown below:\nStep1 (no nesting): idx &lt;- expression == \"high\"\nStep2 (no nesting): expression[idx]\n\n\n\nWe have briefly talked about factors, but this data type only becomes more intuitive once you’ve had a chance to work with it. Let’s take a slight detour and learn about how to relevel categories within a factor.\nTo view the integer assignments under the hood you can use str():\n\nexpression\n\n [1] \"high\"   \"low\"    \"low\"    \"medium\" \"high\"   \"medium\" \"medium\" \"low\"   \n [9] \"low\"    \"low\"   \n\n\nThe categories are referred to as “factor levels”. As we learned earlier, the levels in the expression factor were assigned integers alphabetically, with high=1, low=2, medium=3. However, it makes more sense for us if low=1, medium=2 and high=3, i.e. it makes sense for us to “relevel” the categories in this factor.\nTo relevel the categories, you can add the levels argument to the factor() function, and give it a vector with the categories listed in the required order:\n\nexpression &lt;- factor(expression, levels=c(\"low\", \"medium\", \"high\"))     # you can re-factor a factor \n\nNow we have a releveled factor with low as the lowest or first category, medium as the second and high as the third. This is reflected in the way they are listed in the output of str(), as well as in the numbering of which category is where in the factor.\n\nNote: Releveling becomes necessary when you need a specific category in a factor to be the “base” category, i.e. category that is equal to 1. One example would be if you need the “control” to be the “base” in a given RNA-seq experiment."
  },
  {
    "objectID": "readings/basics.html#piping",
    "href": "readings/basics.html#piping",
    "title": "R Basics",
    "section": "",
    "text": "Two general approaches to working with objects are:\n\nPipes/tidyverse - pipes send an object from function to function - emphasis is on the action, not the object\n\nDefine intermediate objects - an object is re-defined again and again - emphasis is on the object\n\n\n\n\nSimply explained, the pipe operator (%&gt;%) passes an intermediate output from one function to the next.\nYou can think of it as saying “then”. Many functions can be linked together with %&gt;%.\n\nPiping emphasizes a sequence of actions, not the object the actions are being performed on\n\nPipes are best when a sequence of actions must be performed on one object\n\nPipes come from the package magrittr, which is automatically included in packages dplyr and tidyverse\nPipes can make code more clean and easier to read, more intuitive\n\nRead more on this approach in the tidyverse style guide\nHere is a fake example for comparison, using fictional functions to “bake a cake”. First, the pipe method:\n\n# A fake example of how to bake a cake using piping syntax\n\ncake &lt;- flour %&gt;%       # to define cake, start with flour, and then...\n  add(eggs) %&gt;%   # add eggs\n  add(oil) %&gt;%    # add oil\n  add(water) %&gt;%  # add water\n  mix_together(         # mix together\n    utensil = spoon,\n    minutes = 2) %&gt;%    \n  bake(degrees = 350,   # bake\n       system = \"fahrenheit\",\n       minutes = 35) %&gt;%  \n  let_cool()            # let it cool down\n\nHere is another link describing the utility of pipes.\nPiping is not a base function. To use piping, the magrittr package must be installed and loaded (this is typically done by loading tidyverse or dplyr package which include it). You can read more about piping in the magrittr documentation.\nNote that just like other R commands, pipes can be used to just display the result, or to save/re-save an object, depending on whether the assignment operator &lt;- is involved. See both below:\n\n# Create or overwrite object, defining as aggregate counts by age category (not printed)\nlinelist_summary &lt;- linelist %&gt;% \n  count(age_cat)\n\n\n# Print the table of counts in the console, but don't save it\nlinelist %&gt;% \n  count(age_cat)\n\n  age_cat    n\n1     0-4 1095\n2     5-9 1095\n3   10-14  941\n4   15-19  743\n5   20-29 1073\n6   30-49  754\n7   50-69   95\n8     70+    6\n9    &lt;NA&gt;   86\n\n\n%&lt;&gt;%\nThis is an “assignment pipe” from the magrittr package, which pipes an object forward and also re-defines the object. It must be the first pipe operator in the chain. It is shorthand. The below two commands are equivalent:\n\nlinelist &lt;- linelist %&gt;%\n  filter(age &gt; 50)\n\nlinelist %&lt;&gt;% filter(age &gt; 50)\n\n\n\n\n\nThis approach to changing objects/dataframes may be better if:\n\nYou need to manipulate multiple objects\n\nThere are intermediate steps that are meaningful and deserve separate object names\n\nRisks:\n\nCreating new objects for each step means creating lots of objects. If you use the wrong one you might not realize it!\n\nNaming all the objects can be confusing\n\nErrors may not be easily detectable\n\nEither name each intermediate object, or overwrite the original, or combine all the functions together. All come with their own risks.\nBelow is the same fake “cake” example as above, but using this style:\n\n# a fake example of how to bake a cake using this method (defining intermediate objects)\nbatter_1 &lt;- left_join(flour, eggs)\nbatter_2 &lt;- left_join(batter_1, oil)\nbatter_3 &lt;- left_join(batter_2, water)\n\nbatter_4 &lt;- mix_together(object = batter_3, utensil = spoon, minutes = 2)\n\ncake &lt;- bake(batter_4, degrees = 350, system = \"fahrenheit\", minutes = 35)\n\ncake &lt;- let_cool(cake)\n\nCombine all functions together - this is difficult to read:\n\n# an example of combining/nesting mutliple functions together - difficult to read\ncake &lt;- let_cool(bake(mix_together(batter_3, utensil = spoon, minutes = 2), degrees = 350, system = \"fahrenheit\", minutes = 35))"
  },
  {
    "objectID": "readings/basics.html#errors-vs.-warnings-and-debugging-tips",
    "href": "readings/basics.html#errors-vs.-warnings-and-debugging-tips",
    "title": "R Basics",
    "section": "",
    "text": "When a command is run, the R Console may show you warning or error messages in red text.\n\nA warning means that R has completed your command, but had to take additional steps or produced unusual output that you should be aware of.\nAn error means that R was not able to complete your command.\n\nLook for clues:\n\nThe error/warning message will often include a line number for the problem.\nIf an object “is unknown” or “not found”, perhaps you spelled it incorrectly, forgot to call a package with library(), or forgot to re-run your script after making changes.\n\nIf all else fails, copy the error message into Google along with some key terms - chances are that someone else has worked through this already!\n\n\n\n\nA few things to remember when writing commands in R, to avoid errors and warnings:\n\nAlways close parentheses - tip: count the number of opening “(” and closing parentheses “)” for each code chunk\nAvoid spaces in column and object names. Use underscore ( _ ) or periods ( . ) instead\nKeep track of and remember to separate a function’s arguments with commas\n\n\n\n\n\nAny script (RMarkdown or otherwise) will give clues when you have made a mistake. For example, if you forgot to write a comma where it is needed, or to close a parentheses, RStudio will raise a flag on that line, on the right side of the script, to warn you."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Design and Analysis",
    "section": "",
    "text": "Materials for data analysis module of Research Design and Analysis, 2023."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Research Design and Analysis",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Research Design and Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe zoom link will not be presented on this website since it is public. You should have the Zoom link for sessions and office hours in the calendar invites. Please email Chris if you can’t find the Zoom information.↩︎"
  },
  {
    "objectID": "exercises/basic-practice.html",
    "href": "exercises/basic-practice.html",
    "title": "Basic Practice",
    "section": "",
    "text": "Please try to complete the following problems and email your solutions to Chris at christopher_magnano@hms.harvard.edu. It is okay if you are not able to complete the exercises, you may also email Chris whatever you are able to complete within an hour.\n\nWhat will the class and values in vec1 to vec4 be after the following lines of code? Try to figure it out without running the code, then check your work:\n\n\nvec1 &lt;- c(\"4\", T, F, F)\nvec2 &lt;- c(9, \"banana\", 16, \"apple\")\nvec3 &lt;- c(TRUE + TRUE, TRUE, FALSE, TRUE)\nvec4 &lt;- c(3**2, 19%%2)\n\n\nWrite an R command or set of commands to make the vec1 vector twice as large and contain all of the values of the original vector twice.\n\nFor example, using this command [1,2,3] should become [1,2,3,1,2,3], [\"a\"] should become [\"a\", \"a\"], and [2,3,1,2,1] should become [2,3,1,2,1,2,3,1,2,1].\n\nFor this problem you will need to load in the nhanes example data and, if desired, load the tidyverse package.\n\n\nlibrary(tidyverse)\n\nnhanes &lt;- read.csv(\"https://raw.githubusercontent.com/ccb-hms/hsdm-r-course/main/session-materials/session0/session0Data.csv\")\n\n\nCalculate the mean plasma glucose in the nhanes dataset, ignoring missing values.\nCreate a dataframe or tibble, nhanes_mean_filled, where all missing plasma glucose values in nhanes have been replaced by the mean value calculated in part a. Hint: Check out the replace_na function, you can find its documentation here."
  },
  {
    "objectID": "exercises/basic-practice.html#set-1-complete-by-september-8",
    "href": "exercises/basic-practice.html#set-1-complete-by-september-8",
    "title": "Basic Practice",
    "section": "",
    "text": "Please try to complete the following problems and email your solutions to Chris at christopher_magnano@hms.harvard.edu. It is okay if you are not able to complete the exercises, you may also email Chris whatever you are able to complete within an hour.\n\nWhat will the class and values in vec1 to vec4 be after the following lines of code? Try to figure it out without running the code, then check your work:\n\n\nvec1 &lt;- c(\"4\", T, F, F)\nvec2 &lt;- c(9, \"banana\", 16, \"apple\")\nvec3 &lt;- c(TRUE + TRUE, TRUE, FALSE, TRUE)\nvec4 &lt;- c(3**2, 19%%2)\n\n\nWrite an R command or set of commands to make the vec1 vector twice as large and contain all of the values of the original vector twice.\n\nFor example, using this command [1,2,3] should become [1,2,3,1,2,3], [\"a\"] should become [\"a\", \"a\"], and [2,3,1,2,1] should become [2,3,1,2,1,2,3,1,2,1].\n\nFor this problem you will need to load in the nhanes example data and, if desired, load the tidyverse package.\n\n\nlibrary(tidyverse)\n\nnhanes &lt;- read.csv(\"https://raw.githubusercontent.com/ccb-hms/hsdm-r-course/main/session-materials/session0/session0Data.csv\")\n\n\nCalculate the mean plasma glucose in the nhanes dataset, ignoring missing values.\nCreate a dataframe or tibble, nhanes_mean_filled, where all missing plasma glucose values in nhanes have been replaced by the mean value calculated in part a. Hint: Check out the replace_na function, you can find its documentation here."
  },
  {
    "objectID": "exercises/basic-practice.html#set-2-due-september-22",
    "href": "exercises/basic-practice.html#set-2-due-september-22",
    "title": "Basic Practice",
    "section": "Set 2: Due September 22",
    "text": "Set 2: Due September 22\nStart by loading tidyverse and reading in the nhanes example data from the bootcamp:\n\nlibrary(tidyverse)\n\nnhanes &lt;- read_csv(\"https://raw.githubusercontent.com/ccb-hms/hsdm-r-course/main/session-materials/session0/session0Data.csv\")\n\n\nThe dyplr package, a part of the tidyverse set of packages, contains an arrange function.\n\n\nWhat does the arrange function do?\nWhat is the arrange function’s default behavior towards NA values?\n\nHint: Recall that you can look up information on a function with the command ?function.\n\nSort nhanes by BMI from lowest to highest and store the result in a new variable, nhanes_sorted.\nAdd an additional column to nhanes_sorted called bmi_rank which contains each participant’s BMI rank, meaning that the value for the lowest BMI would be 1, the second lowest BMI would be 2, etc. all the way up to the highest BMI. Don’t worry about ties or preserving the original order of the data.\n\nHint: You might want to use the seq function or colon notation (1:num) to generate a sequence of numbers for the new column."
  },
  {
    "objectID": "docker/install-docker.html",
    "href": "docker/install-docker.html",
    "title": "Installing Docker",
    "section": "",
    "text": "We will be installing Docker Desktop, a graphical application which provides useful additional functionality and ease of access for using Docker containers. You can go to the documentation for Docker Desktop, or go straight to the installation instructions for Mac or Windows. It is recommended for Linux users to instead install Docker Engine (see below).\n\n\nIt is recommended to use the Windows Subsystem for Linux (WSL) backend as opposed to the Hyper-V backend. WSL is official Microsoft software which lets you install a Linux distribution on your Windows machine. Installing WSL will also allow you to easily run other software designed for Linux systems and directly use Bash command-line tools.\nYou can /install WSL by following the instructions here. Feel free to choose any of the possible distributions (we like Ubuntu).\n\n\n\nYou will need to know whether your Mac computer is using an Intel or Apple silicon processor. You can follow the instructions here to find this information.\nIf you are on a newer Mac with an Apple silicon processor, you will most likely also need to change a setting in Docker Desktop:\n\n\n\nSetting for Apple Silicon\n\n\n\nClick the settings button in the right side of the top bar.\nClick on Features in development in the left sidebar.\nCheck the box for Use Rosetta for x86/amd64 emulation on Apple Silicon.\n\n\n\n\nWhile Docker Desktop can be run on Linux, support is limited. This is because the core Docker containerization software (now called Docker Engine) is originally designed for Linux systems, and Docker Desktop enables Windows and Mac users to easily interact with containers as Linux users are already able to with the original software. You can install Docker Engine by following the instructions here."
  },
  {
    "objectID": "docker/install-docker.html#installing-docker",
    "href": "docker/install-docker.html#installing-docker",
    "title": "Installing Docker",
    "section": "",
    "text": "We will be installing Docker Desktop, a graphical application which provides useful additional functionality and ease of access for using Docker containers. You can go to the documentation for Docker Desktop, or go straight to the installation instructions for Mac or Windows. It is recommended for Linux users to instead install Docker Engine (see below).\n\n\nIt is recommended to use the Windows Subsystem for Linux (WSL) backend as opposed to the Hyper-V backend. WSL is official Microsoft software which lets you install a Linux distribution on your Windows machine. Installing WSL will also allow you to easily run other software designed for Linux systems and directly use Bash command-line tools.\nYou can /install WSL by following the instructions here. Feel free to choose any of the possible distributions (we like Ubuntu).\n\n\n\nYou will need to know whether your Mac computer is using an Intel or Apple silicon processor. You can follow the instructions here to find this information.\nIf you are on a newer Mac with an Apple silicon processor, you will most likely also need to change a setting in Docker Desktop:\n\n\n\nSetting for Apple Silicon\n\n\n\nClick the settings button in the right side of the top bar.\nClick on Features in development in the left sidebar.\nCheck the box for Use Rosetta for x86/amd64 emulation on Apple Silicon.\n\n\n\n\nWhile Docker Desktop can be run on Linux, support is limited. This is because the core Docker containerization software (now called Docker Engine) is originally designed for Linux systems, and Docker Desktop enables Windows and Mac users to easily interact with containers as Linux users are already able to with the original software. You can install Docker Engine by following the instructions here."
  },
  {
    "objectID": "docker/docker-background.html",
    "href": "docker/docker-background.html",
    "title": "Docker Background",
    "section": "",
    "text": "docker inspect volume_testing"
  },
  {
    "objectID": "docker/docker-background.html#how-can-i-access-files-in-a-container-or-add-files-to-a-container",
    "href": "docker/docker-background.html#how-can-i-access-files-in-a-container-or-add-files-to-a-container",
    "title": "Docker Background",
    "section": "",
    "text": "docker inspect volume_testing"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "docker/accessing-nhanes.html",
    "href": "docker/accessing-nhanes.html",
    "title": "Accessing NHANES Data",
    "section": "",
    "text": "This page contains 3 ways of accessing NHANES data inside R.\n\nThrough the dockerized container created by the CCB.\nThrough the nhanesA package.\nThrough downloading individual files from the CDC website.\n\n\n\nThese steps will allow you to access the NHANES data via a builtin\nConnect to RStudio in your browser as shown in the Accessing NHANES with Docker page.\n\n\nFirst we need to install the Phonto package, which is the software used to access the NHANES database inside the container.\nIn the R console, run the command:\ndevtools::install_github(\"ccb-hms/phonto\")\nIf successful you should see output similar to:\nDownloading GitHub repo ccb-hms/phonto@HEAD\n✔  checking for file ‘/tmp/RtmpjKcsPx/remotes8977d6e909/ccb-hms-phonto-121d255/DESCRIPTION’ ...\n─  preparing ‘phonto’:\n✔  checking DESCRIPTION meta-information\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘phonto_0.0.0.0069.tar.gz’\n   \n* installing *source* package ‘phonto’ ...\n** using staged installation\n** R\n** data\n*** moving datasets to lazyload DB\n** inst\n** byte-compile and prepare package for lazy loading\nEpiConductor Container Version: v0.0.4\nData Collection Date: 2023-06-28\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (phonto)\n\n\n\n\n\n\nTimeout Errors\n\n\n\n\n\nYou may get a timeout error such as this:\n&gt; devtools::install_github(\"ccb-hms/phonto\")\nError: Failed to install 'phonto' from GitHub:\n  Timeout was reached: [api.github.com] Resolving timed out after 10000 milliseconds\nOr a longer error related to login timing out like this:\n&gt; devtools::install_github(\"ccb-hms/phonto\")\nDownloading GitHub repo ccb-hms/phonto@HEAD\n✔  checking for file ‘/tmp/RtmpjKcsPx/remotes895e1b4f91/ccb-hms-phonto-121d255/DESCRIPTION’ (554ms)\n─  preparing ‘phonto’:\n✔  checking DESCRIPTION meta-information ...\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘phonto_0.0.0.0069.tar.gz’\n   \n* installing *source* package ‘phonto’ ...\n** using staged installation\n** R\n** data\n*** moving datasets to lazyload DB\n** inst\n** byte-compile and prepare package for lazy loading\nError : nanodbc/nanodbc.cpp:1021: 00000: [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired  [Microsoft][ODBC Driver 17 for SQL Server]TCP Provider: Timeout error [258].   [Microsoft][ODBC Driver 17 for SQL Server]Unable to complete login process due to delay in login response  [Microsoft][ODBC Driver 17 for SQL Server]Invalid connection string attribute \nError: unable to load R code in package ‘phonto’\nExecution halted\nERROR: lazy loading failed for package ‘phonto’\n* removing ‘/usr/local/lib/R/library/phonto’\nWarning message:\nIn i.p(...) :\n  installation of package ‘/tmp/RtmpjKcsPx/file8930842adb/phonto_0.0.0.0069.tar.gz’ had non-zero exit status\n  \nIf you get one of these errors, please retry the installation command. It may take 2-3 tries for the docker container to establish a connection with Github.\n\n\n\n\n\n\nLike many R packges, the best way to learn how to use Phonto is to go through the included vignettes.\nYou can access the vignettes inside the Docker container by pulling them from Github.\nRStudio has an integrated Git user interface that makes it very easy to use both Git and GitHub. RStudio has integrated Git support which helps to streamline this process. To get a copy of phonto in RStudio do the following:\n\nClick File → New Project\nSelect Version Control → Git\nFor the URL choose: https://github.com/ccb-hms/phonto.git\nYou can choose the name of the project directory.\nChoose the folder in which you want to store the R project and Git (you can put this either inside your home directory or navigate to an attached volume).\nClick Create Project\nCheck the Files tab to see if you have successfully created the project.\nNavigate to phonto/vignettes or directly open phonto/vignettes/quick_start.Rmd or phonto/vignettes/VariableClassification.Rmd.\n\nWhenever you are working in an RStudio project that has a dedicated Git repository, you can interact with Git through the Git tab (same pane as Environment tab)\n\n\n\nWhile the above vignettes include a variety of examples in how to use phonto to access the NHANES data, let’s take a look at how we accessed the data used in the Beheshti paper.\n\n\nTo start, if we don’t know the variable we’re interested in, we can search for keywords using `nhanesSearch.\nhba1c = nhanesSearch(\"glycohemoglobin\", ignore.case=TRUE, ystart = 2005, ystop=2010, namesonly = FALSE)\n Variable.Name Variable.Description Data.File.Name Data.File.Description Begin.Year EndYear  Component\n1         LBXGH  Glycohemoglobin (%)          GHB_D       Glycohemoglobin       2005    2006 Laboratory\n2         LBXGH  Glycohemoglobin (%)          GHB_E       Glycohemoglobin       2007    2008 Laboratory\n3         LBXGH  Glycohemoglobin (%)          GHB_F       Glycohemoglobin       2009    2010 Laboratory\nAlternatively, we can look up table definitions.\nres = nhanesSearchTableNames(\"DEM\", details=TRUE)\n   TableName     Years\n1       DEMO 1999-2000\n2     DEMO_B 2001-2002\n3     DEMO_C 2003-2004\n4     DEMO_D 2005-2006\n5     DEMO_E 2007-2008\n6     DEMO_F 2009-2010\n7     DEMO_G 2011-2012\n8     DEMO_H 2013-2014\n9     DEMO_I 2015-2016\n10    DEMO_J 2017-2018\nAnd then check column names.\nnhanesColnames(\"DEMO_D\")\n [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIDEXMON\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\" \"RIDAGEEX\" \"RIDRETH1\" \"DMQMILIT\" \"DMDBORN\"  \"DMDCITZN\"\n[13] \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDSCHOL\" \"DMDMARTL\" \"DMDHHSIZ\" \"DMDFMSIZ\" \"INDHHINC\" \"INDFMINC\" \"INDFMPIR\" \"RIDEXPRG\" \"DMDHRGND\"\n[25] \"DMDHRAGE\" \"DMDHRBRN\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\" \"FIAINTRP\" \"MIALANG\" \n[37] \"MIAPROXY\" \"MIAINTRP\" \"AIALANG\"  \"WTINT2YR\" \"WTMEC2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIDEXMON\" \"RIAGENDR\"\n[49] \"RIDRETH1\" \"DMQMILIT\" \"DMDBORN\"  \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDSCHOL\" \"DMDMARTL\" \"INDHHINC\" \"INDFMINC\" \"RIDEXPRG\"\n[61] \"DMDHRGND\" \"DMDHRBRN\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\" \"FIAINTRP\" \"MIALANG\" \n[73] \"MIAPROXY\" \"MIAINTRP\" \"AIALANG\"  \"RIDAGEYR\" \"RIDAGEMN\" \"RIDAGEEX\" \"DMDHHSIZ\" \"DMDFMSIZ\" \"INDFMPIR\" \"DMDHRAGE\" \"WTINT2YR\" \"WTMEC2YR\"\n[85] \"SDMVPSU\"  \"SDMVSTRA\"\n\n\n\nWe can define a query to NHANES by creating a list of column names for each table we’re interested in.\ncols_d = list(DEMO_D= c(\"RIDAGEYR\",\"RIAGENDR\",\"RIDRETH1\", \n                       \"DMDBORN\", \"INDFMPIR\", \"SDMVPSU\", \n                       \"SDMVSTRA\", \"WTINT2YR\", \"WTMEC2YR\"), \n              OHX_D = c(\"OHXDECAY\", \"OHXREST\"),\n              GLU_D = c(\"LBXGLU\", \"WTSAF2YR\"), GHB_D = \"LBXGH\",\n              BMX_D= \"BMXBMI\"\n)\ncols_e = list(DEMO_E= c(\"RIDAGEYR\",\"RIAGENDR\",\"RIDRETH1\",\n                       \"DMDBORN2\", \"INDFMPIR\", \"SDMVPSU\", \n                       \"SDMVSTRA\", \"WTINT2YR\", \"WTMEC2YR\"), \n              OHX_E = c(\"OHXDECAY\", \"OHXREST\"),\n              GLU_E = c(\"LBXGLU\", \"WTSAF2YR\"), \n              GHB_E = \"LBXGH\",\n              BMX_E = \"BMXBMI\"\n)\n\ncols_f = list(DEMO_F= c(\"RIDAGEYR\",\"RIAGENDR\",\"RIDRETH1\",\n                       \"DMDBORN2\", \"INDFMPIR\", \"SDMVPSU\", \n                       \"SDMVSTRA\", \"WTINT2YR\", \"WTMEC2YR\"), \n              OHXDEN_F = c(\"OHXDECAY\", \"OHXREST\"),\n              GLU_F = c(\"LBXGLU\", \"WTSAF2YR\"), \n              GHB_F = \"LBXGH\",\n              BMX_F = \"BMXBMI\"\n)\n\n\n\nWe can get metadata on each column by calling dataDescription. Here we combine years, but note that only unique variable names and variable descriptions are returned, i.e., if the list contains the same questionnaire/variables across different survey years, and if all metadata is consistent, then only one row for this variable will be return.\nall_cols &lt;- c(cols_d, cols_e, cols_f)\nmetadata &lt;- dataDescription(all_cols)\ntail(metadata)\n   VariableName                            SASLabel                                                          EnglishText                                     Target\n19     DMDBORN2           Country of Birth - Recode                              In what country {were you/was SP} born? Both males and females 0 YEARS - 150 YEARS\n20     INDFMPIR   Ratio of family income to poverty                        A ratio of family income to poverty threshold Both males and females 0 YEARS - 150 YEARS\n21      SDMVPSU          Masked Variance Pseudo-PSU     Masked Variance Unit Pseudo-PSU variable for variance estimation Both males and females 0 YEARS - 150 YEARS\n22     SDMVSTRA      Masked Variance Pseudo-Stratum Masked Variance Unit Pseudo-Stratum variable for variance estimation Both males and females 0 YEARS - 150 YEARS\n23     WTINT2YR Full Sample 2 Year Interview Weight                                          Interviewed Sample Persons. Both males and females 0 YEARS - 150 YEARS\n24     WTMEC2YR  Full Sample 2 Year MEC Exam Weight                    Both Interviewed and MEC Examined Sample Persons. Both males and females 0 YEARS - 150 YEARS\n\n\n\nWe can use jointQuery to get data from NHANES. This will return all columns in the query already translated and combined into a single dataframe for us.\nbase_df_d &lt;- jointQuery(cols_d)\nbase_df_e &lt;- jointQuery(cols_e)\nbase_df_f &lt;- jointQuery(cols_f)\n\nhead(base_df_d)\n   SEQN RIDAGEYR RIAGENDR           RIDRETH1                        DMDBORN INDFMPIR SDMVPSU SDMVSTRA  WTINT2YR  WTMEC2YR OHXDECAY OHXREST LBXGLU WTSAF2YR LBXGH BMXBMI Begin.Year EndYear\n1 31127        0     Male Non-Hispanic White \"Born in 50 US States or Washi     0.75       2       44  6434.950  6571.396     &lt;NA&gt;    &lt;NA&gt;     NA       NA    NA     NA       2005    2006\n2 31128       11   Female Non-Hispanic Black \"Born in 50 US States or Washi     0.77       1       52  9081.701  8987.042      Yes      No     NA       NA    NA  17.45       2005    2006\n3 31129       15     Male Non-Hispanic Black \"Born in 50 US States or Washi     2.71       1       51  5316.895  5586.719       No      No     NA       NA   5.2  26.53       2005    2006\n4 31130       85   Female Non-Hispanic White \"Born in 50 US States or Washi     1.99       2       46 29960.840 34030.995      Yes     Yes     NA     0.00    NA     NA       2005    2006\n5 31131       44   Female Non-Hispanic Black \"Born in 50 US States or Washi     4.65       1       48 26457.708 26770.585       No     Yes     90 67556.81   6.0  30.90       2005    2006\n6 31132       70     Male Non-Hispanic White \"Born in 50 US States or Washi     5.00       2       52 32961.510 35315.539       No     Yes    157 80193.96   7.1  24.74       2005    2006\n\n\n\n\n\nWith nhanesA, we can easily download entire tables from NHANES. However, there are some extra processing steps we’ll have to perform compared to using the dockerized database. You can learn more about using nhanesA here.\n\nlibrary(nhanesA)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT)\n# Get data with nahnesA\nDEMO_H = nhanes('DEMO_H')\nDEMO_I = nhanes('DEMO_I')\nDPQ_H = nhanes('DPQ_H')\nDPQ_I = nhanes('DPQ_I')\n\n# Append Files\nDEMO &lt;- bind_rows(DEMO_H, DEMO_I)\nDPQ &lt;- bind_rows(DPQ_H, DPQ_I)\n\ndatatable(head(DEMO))\n\n\n\n\n\ndatatable(head(DPQ))\n\n\n\n\n\n\nThere are a few differences between this data and the processed data we’ve been using. First, we have to join the DEMO and DPQ tables. We’ll learn more about joining or merging data in the last week of class.\nSecond, the values in the raw NHANES tables are numeric encodings for each variable. With nhanesA we can lookup the code using nhanesCodebook and convert numeric codes using nhanesTranslate.\n\nnhanesCodebook('DEMO_H', 'RIAGENDR')\n\n$`Variable Name:`\n[1] \"RIAGENDR\"\n\n$`SAS Label:`\n[1] \"Gender\"\n\n$`English Text:`\n[1] \"Gender of the participant.\"\n\n$`Target:`\n[1] \"Both males and females 0 YEARS -\\r 150 YEARS\"\n\n$RIAGENDR\n# A tibble: 3 × 5\n  `Code or Value` `Value Description` Count Cumulative `Skip to Item`\n  &lt;chr&gt;           &lt;chr&gt;               &lt;int&gt;      &lt;int&gt; &lt;lgl&gt;         \n1 1               Male                 5003       5003 NA            \n2 2               Female               5172      10175 NA            \n3 .               Missing                 0      10175 NA            \n\n\n\nnhanesTranslate(DEMO_H)\n\nColumn name is required\n\n\nNULL\n\n\n\n\n\nIf all else fails, individual files can be downloaded from the CDC website and read into R using the foreign package. This example is taken from an example analysis put out by the CDC here.\n\n#' Prevalence of Depression Among Adults Aged 20 and Over: United States, 2013-2016 \n#' Brody DJ, Pratt LA, Hughes JP. Prevalence of Depression Among Adults Aged 20 and Over: United          \n#' States, 2013-2016. NCHS Data Brief. No 303. Hyattsville, MD: National Center for Health Statistics. 2018.                     \n\n#' # Data preparation\n# Download & Read SAS Transport Files\n# Demographic (DEMO)\ndownload.file(\"https://wwwn.cdc.gov/nchs/nhanes/2013-2014/DEMO_H.XPT\", tf &lt;- tempfile(), mode=\"wb\")\nDEMO_H &lt;- foreign::read.xport(tf)[,c(\"SEQN\",\"RIAGENDR\",\"RIDAGEYR\",\"SDMVSTRA\",\"SDMVPSU\",\"WTMEC2YR\")]\ndownload.file(\"https://wwwn.cdc.gov/nchs/nhanes/2015-2016/DEMO_I.XPT\", tf &lt;- tempfile(), mode=\"wb\")\nDEMO_I &lt;- foreign::read.xport(tf)[,c(\"SEQN\",\"RIAGENDR\",\"RIDAGEYR\",\"SDMVSTRA\",\"SDMVPSU\",\"WTMEC2YR\")]\n\n# Mental Health - Depression Screener (DPQ) \ndownload.file(\"http://wwwn.cdc.gov/nchs/nhanes/2013-2014/DPQ_H.XPT\", tf &lt;- tempfile(), mode=\"wb\")\nDPQ_H &lt;- foreign::read.xport(tf)\ndownload.file(\"http://wwwn.cdc.gov/nchs/nhanes/2015-2016/DPQ_I.XPT\", tf &lt;- tempfile(), mode=\"wb\")\nDPQ_I &lt;- foreign::read.xport(tf)"
  },
  {
    "objectID": "docker/accessing-nhanes.html#through-the-docker-container-reccomended",
    "href": "docker/accessing-nhanes.html#through-the-docker-container-reccomended",
    "title": "Accessing NHANES Data",
    "section": "",
    "text": "These steps will allow you to access the NHANES data via a builtin\nConnect to RStudio in your browser as shown in the Accessing NHANES with Docker page.\n\n\nFirst we need to install the Phonto package, which is the software used to access the NHANES database inside the container.\nIn the R console, run the command:\ndevtools::install_github(\"ccb-hms/phonto\")\nIf successful you should see output similar to:\nDownloading GitHub repo ccb-hms/phonto@HEAD\n✔  checking for file ‘/tmp/RtmpjKcsPx/remotes8977d6e909/ccb-hms-phonto-121d255/DESCRIPTION’ ...\n─  preparing ‘phonto’:\n✔  checking DESCRIPTION meta-information\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘phonto_0.0.0.0069.tar.gz’\n   \n* installing *source* package ‘phonto’ ...\n** using staged installation\n** R\n** data\n*** moving datasets to lazyload DB\n** inst\n** byte-compile and prepare package for lazy loading\nEpiConductor Container Version: v0.0.4\nData Collection Date: 2023-06-28\n** help\n*** installing help indices\n** building package indices\n** installing vignettes\n** testing if installed package can be loaded from temporary location\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (phonto)\n\n\n\n\n\n\nTimeout Errors\n\n\n\n\n\nYou may get a timeout error such as this:\n&gt; devtools::install_github(\"ccb-hms/phonto\")\nError: Failed to install 'phonto' from GitHub:\n  Timeout was reached: [api.github.com] Resolving timed out after 10000 milliseconds\nOr a longer error related to login timing out like this:\n&gt; devtools::install_github(\"ccb-hms/phonto\")\nDownloading GitHub repo ccb-hms/phonto@HEAD\n✔  checking for file ‘/tmp/RtmpjKcsPx/remotes895e1b4f91/ccb-hms-phonto-121d255/DESCRIPTION’ (554ms)\n─  preparing ‘phonto’:\n✔  checking DESCRIPTION meta-information ...\n─  checking for LF line-endings in source and make files and shell scripts\n─  checking for empty or unneeded directories\n─  building ‘phonto_0.0.0.0069.tar.gz’\n   \n* installing *source* package ‘phonto’ ...\n** using staged installation\n** R\n** data\n*** moving datasets to lazyload DB\n** inst\n** byte-compile and prepare package for lazy loading\nError : nanodbc/nanodbc.cpp:1021: 00000: [Microsoft][ODBC Driver 17 for SQL Server]Login timeout expired  [Microsoft][ODBC Driver 17 for SQL Server]TCP Provider: Timeout error [258].   [Microsoft][ODBC Driver 17 for SQL Server]Unable to complete login process due to delay in login response  [Microsoft][ODBC Driver 17 for SQL Server]Invalid connection string attribute \nError: unable to load R code in package ‘phonto’\nExecution halted\nERROR: lazy loading failed for package ‘phonto’\n* removing ‘/usr/local/lib/R/library/phonto’\nWarning message:\nIn i.p(...) :\n  installation of package ‘/tmp/RtmpjKcsPx/file8930842adb/phonto_0.0.0.0069.tar.gz’ had non-zero exit status\n  \nIf you get one of these errors, please retry the installation command. It may take 2-3 tries for the docker container to establish a connection with Github.\n\n\n\n\n\n\nLike many R packges, the best way to learn how to use Phonto is to go through the included vignettes.\nYou can access the vignettes inside the Docker container by pulling them from Github.\nRStudio has an integrated Git user interface that makes it very easy to use both Git and GitHub. RStudio has integrated Git support which helps to streamline this process. To get a copy of phonto in RStudio do the following:\n\nClick File → New Project\nSelect Version Control → Git\nFor the URL choose: https://github.com/ccb-hms/phonto.git\nYou can choose the name of the project directory.\nChoose the folder in which you want to store the R project and Git (you can put this either inside your home directory or navigate to an attached volume).\nClick Create Project\nCheck the Files tab to see if you have successfully created the project.\nNavigate to phonto/vignettes or directly open phonto/vignettes/quick_start.Rmd or phonto/vignettes/VariableClassification.Rmd.\n\nWhenever you are working in an RStudio project that has a dedicated Git repository, you can interact with Git through the Git tab (same pane as Environment tab)\n\n\n\nWhile the above vignettes include a variety of examples in how to use phonto to access the NHANES data, let’s take a look at how we accessed the data used in the Beheshti paper.\n\n\nTo start, if we don’t know the variable we’re interested in, we can search for keywords using `nhanesSearch.\nhba1c = nhanesSearch(\"glycohemoglobin\", ignore.case=TRUE, ystart = 2005, ystop=2010, namesonly = FALSE)\n Variable.Name Variable.Description Data.File.Name Data.File.Description Begin.Year EndYear  Component\n1         LBXGH  Glycohemoglobin (%)          GHB_D       Glycohemoglobin       2005    2006 Laboratory\n2         LBXGH  Glycohemoglobin (%)          GHB_E       Glycohemoglobin       2007    2008 Laboratory\n3         LBXGH  Glycohemoglobin (%)          GHB_F       Glycohemoglobin       2009    2010 Laboratory\nAlternatively, we can look up table definitions.\nres = nhanesSearchTableNames(\"DEM\", details=TRUE)\n   TableName     Years\n1       DEMO 1999-2000\n2     DEMO_B 2001-2002\n3     DEMO_C 2003-2004\n4     DEMO_D 2005-2006\n5     DEMO_E 2007-2008\n6     DEMO_F 2009-2010\n7     DEMO_G 2011-2012\n8     DEMO_H 2013-2014\n9     DEMO_I 2015-2016\n10    DEMO_J 2017-2018\nAnd then check column names.\nnhanesColnames(\"DEMO_D\")\n [1] \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIDEXMON\" \"RIAGENDR\" \"RIDAGEYR\" \"RIDAGEMN\" \"RIDAGEEX\" \"RIDRETH1\" \"DMQMILIT\" \"DMDBORN\"  \"DMDCITZN\"\n[13] \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDSCHOL\" \"DMDMARTL\" \"DMDHHSIZ\" \"DMDFMSIZ\" \"INDHHINC\" \"INDFMINC\" \"INDFMPIR\" \"RIDEXPRG\" \"DMDHRGND\"\n[25] \"DMDHRAGE\" \"DMDHRBRN\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\" \"FIAINTRP\" \"MIALANG\" \n[37] \"MIAPROXY\" \"MIAINTRP\" \"AIALANG\"  \"WTINT2YR\" \"WTMEC2YR\" \"SDMVPSU\"  \"SDMVSTRA\" \"SEQN\"     \"SDDSRVYR\" \"RIDSTATR\" \"RIDEXMON\" \"RIAGENDR\"\n[49] \"RIDRETH1\" \"DMQMILIT\" \"DMDBORN\"  \"DMDCITZN\" \"DMDYRSUS\" \"DMDEDUC3\" \"DMDEDUC2\" \"DMDSCHOL\" \"DMDMARTL\" \"INDHHINC\" \"INDFMINC\" \"RIDEXPRG\"\n[61] \"DMDHRGND\" \"DMDHRBRN\" \"DMDHREDU\" \"DMDHRMAR\" \"DMDHSEDU\" \"SIALANG\"  \"SIAPROXY\" \"SIAINTRP\" \"FIALANG\"  \"FIAPROXY\" \"FIAINTRP\" \"MIALANG\" \n[73] \"MIAPROXY\" \"MIAINTRP\" \"AIALANG\"  \"RIDAGEYR\" \"RIDAGEMN\" \"RIDAGEEX\" \"DMDHHSIZ\" \"DMDFMSIZ\" \"INDFMPIR\" \"DMDHRAGE\" \"WTINT2YR\" \"WTMEC2YR\"\n[85] \"SDMVPSU\"  \"SDMVSTRA\"\n\n\n\nWe can define a query to NHANES by creating a list of column names for each table we’re interested in.\ncols_d = list(DEMO_D= c(\"RIDAGEYR\",\"RIAGENDR\",\"RIDRETH1\", \n                       \"DMDBORN\", \"INDFMPIR\", \"SDMVPSU\", \n                       \"SDMVSTRA\", \"WTINT2YR\", \"WTMEC2YR\"), \n              OHX_D = c(\"OHXDECAY\", \"OHXREST\"),\n              GLU_D = c(\"LBXGLU\", \"WTSAF2YR\"), GHB_D = \"LBXGH\",\n              BMX_D= \"BMXBMI\"\n)\ncols_e = list(DEMO_E= c(\"RIDAGEYR\",\"RIAGENDR\",\"RIDRETH1\",\n                       \"DMDBORN2\", \"INDFMPIR\", \"SDMVPSU\", \n                       \"SDMVSTRA\", \"WTINT2YR\", \"WTMEC2YR\"), \n              OHX_E = c(\"OHXDECAY\", \"OHXREST\"),\n              GLU_E = c(\"LBXGLU\", \"WTSAF2YR\"), \n              GHB_E = \"LBXGH\",\n              BMX_E = \"BMXBMI\"\n)\n\ncols_f = list(DEMO_F= c(\"RIDAGEYR\",\"RIAGENDR\",\"RIDRETH1\",\n                       \"DMDBORN2\", \"INDFMPIR\", \"SDMVPSU\", \n                       \"SDMVSTRA\", \"WTINT2YR\", \"WTMEC2YR\"), \n              OHXDEN_F = c(\"OHXDECAY\", \"OHXREST\"),\n              GLU_F = c(\"LBXGLU\", \"WTSAF2YR\"), \n              GHB_F = \"LBXGH\",\n              BMX_F = \"BMXBMI\"\n)\n\n\n\nWe can get metadata on each column by calling dataDescription. Here we combine years, but note that only unique variable names and variable descriptions are returned, i.e., if the list contains the same questionnaire/variables across different survey years, and if all metadata is consistent, then only one row for this variable will be return.\nall_cols &lt;- c(cols_d, cols_e, cols_f)\nmetadata &lt;- dataDescription(all_cols)\ntail(metadata)\n   VariableName                            SASLabel                                                          EnglishText                                     Target\n19     DMDBORN2           Country of Birth - Recode                              In what country {were you/was SP} born? Both males and females 0 YEARS - 150 YEARS\n20     INDFMPIR   Ratio of family income to poverty                        A ratio of family income to poverty threshold Both males and females 0 YEARS - 150 YEARS\n21      SDMVPSU          Masked Variance Pseudo-PSU     Masked Variance Unit Pseudo-PSU variable for variance estimation Both males and females 0 YEARS - 150 YEARS\n22     SDMVSTRA      Masked Variance Pseudo-Stratum Masked Variance Unit Pseudo-Stratum variable for variance estimation Both males and females 0 YEARS - 150 YEARS\n23     WTINT2YR Full Sample 2 Year Interview Weight                                          Interviewed Sample Persons. Both males and females 0 YEARS - 150 YEARS\n24     WTMEC2YR  Full Sample 2 Year MEC Exam Weight                    Both Interviewed and MEC Examined Sample Persons. Both males and females 0 YEARS - 150 YEARS\n\n\n\nWe can use jointQuery to get data from NHANES. This will return all columns in the query already translated and combined into a single dataframe for us.\nbase_df_d &lt;- jointQuery(cols_d)\nbase_df_e &lt;- jointQuery(cols_e)\nbase_df_f &lt;- jointQuery(cols_f)\n\nhead(base_df_d)\n   SEQN RIDAGEYR RIAGENDR           RIDRETH1                        DMDBORN INDFMPIR SDMVPSU SDMVSTRA  WTINT2YR  WTMEC2YR OHXDECAY OHXREST LBXGLU WTSAF2YR LBXGH BMXBMI Begin.Year EndYear\n1 31127        0     Male Non-Hispanic White \"Born in 50 US States or Washi     0.75       2       44  6434.950  6571.396     &lt;NA&gt;    &lt;NA&gt;     NA       NA    NA     NA       2005    2006\n2 31128       11   Female Non-Hispanic Black \"Born in 50 US States or Washi     0.77       1       52  9081.701  8987.042      Yes      No     NA       NA    NA  17.45       2005    2006\n3 31129       15     Male Non-Hispanic Black \"Born in 50 US States or Washi     2.71       1       51  5316.895  5586.719       No      No     NA       NA   5.2  26.53       2005    2006\n4 31130       85   Female Non-Hispanic White \"Born in 50 US States or Washi     1.99       2       46 29960.840 34030.995      Yes     Yes     NA     0.00    NA     NA       2005    2006\n5 31131       44   Female Non-Hispanic Black \"Born in 50 US States or Washi     4.65       1       48 26457.708 26770.585       No     Yes     90 67556.81   6.0  30.90       2005    2006\n6 31132       70     Male Non-Hispanic White \"Born in 50 US States or Washi     5.00       2       52 32961.510 35315.539       No     Yes    157 80193.96   7.1  24.74       2005    2006"
  },
  {
    "objectID": "docker/accessing-nhanes.html#through-nhanesa",
    "href": "docker/accessing-nhanes.html#through-nhanesa",
    "title": "Accessing NHANES Data",
    "section": "",
    "text": "With nhanesA, we can easily download entire tables from NHANES. However, there are some extra processing steps we’ll have to perform compared to using the dockerized database. You can learn more about using nhanesA here.\n\nlibrary(nhanesA)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT)\n# Get data with nahnesA\nDEMO_H = nhanes('DEMO_H')\nDEMO_I = nhanes('DEMO_I')\nDPQ_H = nhanes('DPQ_H')\nDPQ_I = nhanes('DPQ_I')\n\n# Append Files\nDEMO &lt;- bind_rows(DEMO_H, DEMO_I)\nDPQ &lt;- bind_rows(DPQ_H, DPQ_I)\n\ndatatable(head(DEMO))\n\n\n\n\n\ndatatable(head(DPQ))\n\n\n\n\n\n\nThere are a few differences between this data and the processed data we’ve been using. First, we have to join the DEMO and DPQ tables. We’ll learn more about joining or merging data in the last week of class.\nSecond, the values in the raw NHANES tables are numeric encodings for each variable. With nhanesA we can lookup the code using nhanesCodebook and convert numeric codes using nhanesTranslate.\n\nnhanesCodebook('DEMO_H', 'RIAGENDR')\n\n$`Variable Name:`\n[1] \"RIAGENDR\"\n\n$`SAS Label:`\n[1] \"Gender\"\n\n$`English Text:`\n[1] \"Gender of the participant.\"\n\n$`Target:`\n[1] \"Both males and females 0 YEARS -\\r 150 YEARS\"\n\n$RIAGENDR\n# A tibble: 3 × 5\n  `Code or Value` `Value Description` Count Cumulative `Skip to Item`\n  &lt;chr&gt;           &lt;chr&gt;               &lt;int&gt;      &lt;int&gt; &lt;lgl&gt;         \n1 1               Male                 5003       5003 NA            \n2 2               Female               5172      10175 NA            \n3 .               Missing                 0      10175 NA            \n\n\n\nnhanesTranslate(DEMO_H)\n\nColumn name is required\n\n\nNULL"
  },
  {
    "objectID": "docker/accessing-nhanes.html#downloading-individual-files",
    "href": "docker/accessing-nhanes.html#downloading-individual-files",
    "title": "Accessing NHANES Data",
    "section": "",
    "text": "If all else fails, individual files can be downloaded from the CDC website and read into R using the foreign package. This example is taken from an example analysis put out by the CDC here.\n\n#' Prevalence of Depression Among Adults Aged 20 and Over: United States, 2013-2016 \n#' Brody DJ, Pratt LA, Hughes JP. Prevalence of Depression Among Adults Aged 20 and Over: United          \n#' States, 2013-2016. NCHS Data Brief. No 303. Hyattsville, MD: National Center for Health Statistics. 2018.                     \n\n#' # Data preparation\n# Download & Read SAS Transport Files\n# Demographic (DEMO)\ndownload.file(\"https://wwwn.cdc.gov/nchs/nhanes/2013-2014/DEMO_H.XPT\", tf &lt;- tempfile(), mode=\"wb\")\nDEMO_H &lt;- foreign::read.xport(tf)[,c(\"SEQN\",\"RIAGENDR\",\"RIDAGEYR\",\"SDMVSTRA\",\"SDMVPSU\",\"WTMEC2YR\")]\ndownload.file(\"https://wwwn.cdc.gov/nchs/nhanes/2015-2016/DEMO_I.XPT\", tf &lt;- tempfile(), mode=\"wb\")\nDEMO_I &lt;- foreign::read.xport(tf)[,c(\"SEQN\",\"RIAGENDR\",\"RIDAGEYR\",\"SDMVSTRA\",\"SDMVPSU\",\"WTMEC2YR\")]\n\n# Mental Health - Depression Screener (DPQ) \ndownload.file(\"http://wwwn.cdc.gov/nchs/nhanes/2013-2014/DPQ_H.XPT\", tf &lt;- tempfile(), mode=\"wb\")\nDPQ_H &lt;- foreign::read.xport(tf)\ndownload.file(\"http://wwwn.cdc.gov/nchs/nhanes/2015-2016/DPQ_I.XPT\", tf &lt;- tempfile(), mode=\"wb\")\nDPQ_I &lt;- foreign::read.xport(tf)"
  },
  {
    "objectID": "docker/getting-container.html",
    "href": "docker/getting-container.html",
    "title": "Downloading and Running NHANES Docker Container",
    "section": "",
    "text": "Make a free DockerHub account at this link.\nLogin to your DockerHub account in Docker Desktop:\n\n 3. Search for hmsccb/nhanes-workbench in the search bar, to the left of where you previously clicked Sign In in the top blue bar. You should get one image as the result.\n\n\n\nSearch result\n\n\n\nMake sure the Tag dropdown is set to version-0.0.4 and click Pull.\n\n\n\n\nOpen a terminal (if in Mac or Linux) or the command prompt (in Windows). Type the command\ndocker pull hmsccb/nhanes-workbench:version-0.0.4\ninto the terminal.\nYou may need to specify a platform, you can try\ndocker pull --platform linux/arm64 hmsccb/nhanes-workbench:version-0.0.4\nor\ndocker pull --platform linux/x86_64/v8 hmsccb/nhanes-workbench:version-0.0.4\nif pulling without a platform is returning an error.\nAfter you’ve downloaded the container, you should see it when you look at the Images tab in Docker Desktop.\n ## Setting up a volume\n\n\n\n\nUnfortunately, right now the container cannot be run from Docker Desktop due to Docker Desktop not allowing us to setup multiple ports. However, we can simply run the container from the command line.\nOn Mac or Linux, copy and paste the following command into a terminal:\ndocker \\\n    run \\\n        --rm \\\n        --name nhanes-workbench \\\n        -d \\\n        -v LOCAL_DIRECTORY:/HostData \\\n        -p 8787:8787 \\\n        -p 2200:22 \\\n        -p 1433:1433 \\\n        -e 'CONTAINER_USER_USERNAME=USER' \\\n        -e 'CONTAINER_USER_PASSWORD=PASSWORD' \\\n        -e 'ACCEPT_EULA=Y' \\\n        -e 'SA_PASSWORD=yourStrong(!)Password' \\\n        hmsccb/nhanes-workbench:version-0.0.4\nSimilar to pulling the container, if you get an error you may need to specify a platform by adding --platform linux/arm64 or --platform linux/x86_64/v8 to the run command.\nIf you’re using the WSL backend on Windows (recommended), you can open a linux terminal by hitting the windows key and typing in WSL:\n\n\n\nRunning WSL\n\n\nPaste the run command above into your terminal to run the container.\nIf you’re using the Hyper-V backend on Windows, copy and paste this command into the command prompt (this is the same as the Mac/Linux command but doesn’t use the \\ character to make it multiline):\ndocker run  --rm --name nhanes-workbench -d  -v LOCAL_DIRECTORY:/HostData -p 8787:8787 -p 2200:22 -p 1433:1433  -e 'CONTAINER_USER_USERNAME=USER'  -e 'CONTAINER_USER_PASSWORD=PASSWORD' -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=yourStrong(!)Password' hmsccb/nhanes-workbench:version-0.0.4\nYou can learn more about what each part of this command does below.\n\n\n\n\nLOCAL_DIRECTORY is a directory on the host that you would like mounted at /HostData in the container. Can be omitted.\nCONTAINER_USER_USERNAME is the name of the user in the container that will be created at runtime. You can connect via ssh or RStudio Server with this user name.\nCONTAINER_USER_PASSWORD is the password of the user in the container that will be created at runtime. You can connect via ssh or RStudio Server with this password.\nACCEPT_EULA is required for SQL Server to successfully start\nSA_PASSWORD is the password for the SQL Server sa account. See here for complexity requirements.\n\n\n\nThese options control port forwarding from the container to the host:\n        -p 8787:8787 \\\n        -p 2200:22 \\\n        -p 1433:1433 \\\nPort 8787 is used for access to the RStudio Server HTTP server. Port 2200 on the host provides access to ssh server in the container. Port 1433 provides access to SQL Server running in the container.\n\n\n\n\nInside Docker Desktop, in the Containers tab you should now see the nhanes-workbench container.\n\n\n\nThe running container\n\n\n\n\n\nIf you click on the nhanes-workbench entry, you can see and interact with the running container in a number of tabs.\nLogs\nThis tab shows all of the output of the running container. If you are having problems with the container, the logs are the place to try to figure out what is going wrong.\nInspect\nThis tab gives you the general container information. You can check here to see where mounted files in the container live under Mounts or double check what you set your password to under Environment.\nTerminal\nThis tab gives you a command-line interface into the container while it is running. Here you can manipulate files, run command-line tools, or anything else you’d need to do at the terminal inside the container.\nFiles\nThis tab lets you easily explore all of the files in the container.\n\n\n\nInspecting the container’s files\n\n\nYou can select files or folders by right/command clicking on them, then select Save to save a copy of that file somewhere on your computer outside of the container.\n\n\n\nSaving a file to outside docker\n\n\n\n\n\n\n\n\nOnce the Docker container is running, you can connect to it by pointing your web browser to port 8787. To do this, simply type http://localhost:8787 into your browser’s address bar. You’ll then need to login using USER and PASSWORD or whatever you set CONTAINER_USER_USERNAME and CONTAINER_USER_PASSWORD to when creating the container. You will then be in an RStudio instance inside the container.\n\n\n\nRunning RStudio in your browser\n\n\n\n\n\nDatabase connectivity is enabled on TCP port 1433 on the host. Any standard tools that work with SQL Server (Azure Data Studio, SSMS, ODBC, JDBC) can be aimed at this port on the host to work with the DBs in the container.\nIf you are already familiar with relational databases, this can be a great option for accessing the NHANES data.\nFor instance, in Azure Data Studio you can connect to localhost using the username SA and password yourStrong(!)Password or whatever SA_PASSWORD was set to when you created the container. Note that you’ll have to set the authentication to SQL Login.\n\n\n\nConnecting to the SQL server\n\n\n\n\n\nYou can can ssh into the container, e.g.:\nssh USER@HOST_ADDRESS -p 2200 -o GlobalKnownHostsFile=/dev/null -o UserKnownHostsFile=/dev/null\nIf you are running SSH on the same host where the container is running:\nssh USER@localhost -p 2200 -o GlobalKnownHostsFile=/dev/null -o UserKnownHostsFile=/dev/null\n\n\n\nYou can find more details and some example scripts for running the SQL server on its Github page."
  },
  {
    "objectID": "docker/getting-container.html#downloading-the-container",
    "href": "docker/getting-container.html#downloading-the-container",
    "title": "Downloading and Running NHANES Docker Container",
    "section": "",
    "text": "Make a free DockerHub account at this link.\nLogin to your DockerHub account in Docker Desktop:\n\n 3. Search for hmsccb/nhanes-workbench in the search bar, to the left of where you previously clicked Sign In in the top blue bar. You should get one image as the result.\n\n\n\nSearch result\n\n\n\nMake sure the Tag dropdown is set to version-0.0.4 and click Pull.\n\n\n\n\nOpen a terminal (if in Mac or Linux) or the command prompt (in Windows). Type the command\ndocker pull hmsccb/nhanes-workbench:version-0.0.4\ninto the terminal.\nYou may need to specify a platform, you can try\ndocker pull --platform linux/arm64 hmsccb/nhanes-workbench:version-0.0.4\nor\ndocker pull --platform linux/x86_64/v8 hmsccb/nhanes-workbench:version-0.0.4\nif pulling without a platform is returning an error.\nAfter you’ve downloaded the container, you should see it when you look at the Images tab in Docker Desktop.\n ## Setting up a volume"
  },
  {
    "objectID": "docker/getting-container.html#running-the-container",
    "href": "docker/getting-container.html#running-the-container",
    "title": "Downloading and Running NHANES Docker Container",
    "section": "",
    "text": "Unfortunately, right now the container cannot be run from Docker Desktop due to Docker Desktop not allowing us to setup multiple ports. However, we can simply run the container from the command line.\nOn Mac or Linux, copy and paste the following command into a terminal:\ndocker \\\n    run \\\n        --rm \\\n        --name nhanes-workbench \\\n        -d \\\n        -v LOCAL_DIRECTORY:/HostData \\\n        -p 8787:8787 \\\n        -p 2200:22 \\\n        -p 1433:1433 \\\n        -e 'CONTAINER_USER_USERNAME=USER' \\\n        -e 'CONTAINER_USER_PASSWORD=PASSWORD' \\\n        -e 'ACCEPT_EULA=Y' \\\n        -e 'SA_PASSWORD=yourStrong(!)Password' \\\n        hmsccb/nhanes-workbench:version-0.0.4\nSimilar to pulling the container, if you get an error you may need to specify a platform by adding --platform linux/arm64 or --platform linux/x86_64/v8 to the run command.\nIf you’re using the WSL backend on Windows (recommended), you can open a linux terminal by hitting the windows key and typing in WSL:\n\n\n\nRunning WSL\n\n\nPaste the run command above into your terminal to run the container.\nIf you’re using the Hyper-V backend on Windows, copy and paste this command into the command prompt (this is the same as the Mac/Linux command but doesn’t use the \\ character to make it multiline):\ndocker run  --rm --name nhanes-workbench -d  -v LOCAL_DIRECTORY:/HostData -p 8787:8787 -p 2200:22 -p 1433:1433  -e 'CONTAINER_USER_USERNAME=USER'  -e 'CONTAINER_USER_PASSWORD=PASSWORD' -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=yourStrong(!)Password' hmsccb/nhanes-workbench:version-0.0.4\nYou can learn more about what each part of this command does below.\n\n\n\n\nLOCAL_DIRECTORY is a directory on the host that you would like mounted at /HostData in the container. Can be omitted.\nCONTAINER_USER_USERNAME is the name of the user in the container that will be created at runtime. You can connect via ssh or RStudio Server with this user name.\nCONTAINER_USER_PASSWORD is the password of the user in the container that will be created at runtime. You can connect via ssh or RStudio Server with this password.\nACCEPT_EULA is required for SQL Server to successfully start\nSA_PASSWORD is the password for the SQL Server sa account. See here for complexity requirements.\n\n\n\nThese options control port forwarding from the container to the host:\n        -p 8787:8787 \\\n        -p 2200:22 \\\n        -p 1433:1433 \\\nPort 8787 is used for access to the RStudio Server HTTP server. Port 2200 on the host provides access to ssh server in the container. Port 1433 provides access to SQL Server running in the container.\n\n\n\n\nInside Docker Desktop, in the Containers tab you should now see the nhanes-workbench container.\n\n\n\nThe running container\n\n\n\n\n\nIf you click on the nhanes-workbench entry, you can see and interact with the running container in a number of tabs.\nLogs\nThis tab shows all of the output of the running container. If you are having problems with the container, the logs are the place to try to figure out what is going wrong.\nInspect\nThis tab gives you the general container information. You can check here to see where mounted files in the container live under Mounts or double check what you set your password to under Environment.\nTerminal\nThis tab gives you a command-line interface into the container while it is running. Here you can manipulate files, run command-line tools, or anything else you’d need to do at the terminal inside the container.\nFiles\nThis tab lets you easily explore all of the files in the container.\n\n\n\nInspecting the container’s files\n\n\nYou can select files or folders by right/command clicking on them, then select Save to save a copy of that file somewhere on your computer outside of the container.\n\n\n\nSaving a file to outside docker"
  },
  {
    "objectID": "docker/getting-container.html#connecting-to-the-container",
    "href": "docker/getting-container.html#connecting-to-the-container",
    "title": "Downloading and Running NHANES Docker Container",
    "section": "",
    "text": "Once the Docker container is running, you can connect to it by pointing your web browser to port 8787. To do this, simply type http://localhost:8787 into your browser’s address bar. You’ll then need to login using USER and PASSWORD or whatever you set CONTAINER_USER_USERNAME and CONTAINER_USER_PASSWORD to when creating the container. You will then be in an RStudio instance inside the container.\n\n\n\nRunning RStudio in your browser\n\n\n\n\n\nDatabase connectivity is enabled on TCP port 1433 on the host. Any standard tools that work with SQL Server (Azure Data Studio, SSMS, ODBC, JDBC) can be aimed at this port on the host to work with the DBs in the container.\nIf you are already familiar with relational databases, this can be a great option for accessing the NHANES data.\nFor instance, in Azure Data Studio you can connect to localhost using the username SA and password yourStrong(!)Password or whatever SA_PASSWORD was set to when you created the container. Note that you’ll have to set the authentication to SQL Login.\n\n\n\nConnecting to the SQL server\n\n\n\n\n\nYou can can ssh into the container, e.g.:\nssh USER@HOST_ADDRESS -p 2200 -o GlobalKnownHostsFile=/dev/null -o UserKnownHostsFile=/dev/null\nIf you are running SSH on the same host where the container is running:\nssh USER@localhost -p 2200 -o GlobalKnownHostsFile=/dev/null -o UserKnownHostsFile=/dev/null\n\n\n\nYou can find more details and some example scripts for running the SQL server on its Github page."
  },
  {
    "objectID": "exercises/analysis-practice.html",
    "href": "exercises/analysis-practice.html",
    "title": "Analysis Practice",
    "section": "",
    "text": "What is at least one challenge you expect to encounter when analyzing your project’s data? Examples of challenges include:\n\n\nA high percent of missing data.\nGroups of interest are of uneven size.\nA specialized type of data which is difficult to deal with (such as geographic or time-series data).\nUnreliable data (values may be incorrect).\nData has to be combined from many different sources.\nHow the data was collected or recorded changed over time.\n\nTip: If you are unsure what challenges your project’s data might face, take a look at other papers which use the same or similar data. Do the authors mention any special data cleaning or analysis steps?\n\nHow do you plan to deal with this challenge? Are there specialized R packages which can help with this challenge? If you are unsure, how do you plan to learn how to deal with this challenge?\nA small, simulated dataset is useful when creating an analysis. Since we know the exact properties of the simulated dataset, we can make sure any calculations or corrections we perform in our analysis are correct by first trying them on the simulated dataset.\n\nCreate a small dataset (50 or fewer rows) which is similar to your project’s data and if possible contains the challenge you talk about above. You can create the dataset by hand, as a subset of real data, or by random generation.\nAttach your dataset and any code you used to generate your dataset to your solution.\nYou can create vectors of different types in R:\n\n# Make a vector of 10 0's and 0 empty strings \"\"\nnum_vec &lt;- numeric(10)\nchar_vec &lt;- character(10)\n\nnum_vec\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\nchar_vec\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nAnd then assign values and create a dataframe:\n\nnum_vec[1:3] &lt;- 6\nnum_vec[4:10] &lt;- 8.5\nnum_vec\n\n [1] 6.0 6.0 6.0 8.5 8.5 8.5 8.5 8.5 8.5 8.5\n\nchar_vec[1:5] &lt;- \"Male\"\nchar_vec[5:10] &lt;- \"Female\"\nchar_vec\n\n [1] \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Female\" \"Female\" \"Female\" \"Female\"\n [9] \"Female\" \"Female\"\n\ndf &lt;- data.frame(char_vec, num_vec)\n\nYou can generate random numbers using functions like rnorm:\n\n# Generate 50 random numbers centered around 1000 with a standard deviation of 300\nnorm_nums &lt;- rnorm(50, 1000, 300)\nnorm_nums\n\n [1] 1116.6379  563.9742 1231.9937  895.2387  853.6645 1247.6103 1410.7942\n [8]  493.6071 1517.8996 1331.8473  546.5383  923.6370 1023.6361 1277.3763\n[15]  312.3785  936.9149 1514.6225 1135.7611  714.0299 1362.7430 1024.7950\n[22] 1359.3195  511.7774 1129.9905  711.4480  827.8140 1006.3039 1028.9449\n[29] 1222.6779  651.3089  399.4065 1019.8172  744.3571  661.3804  854.0215\n[36] 1078.0156 1873.4845 1312.8426 1265.6508 1229.0833 1105.4688  910.2854\n[43]  487.4748 1141.4996  586.9807  723.0083 1639.8555 1330.9395 1176.6209\n[50]  684.7990\n\n\nAnother useful function for generating example data is sample. It selects random items from a vector.\n\n# Select a random item 8 times from norm_nums\nsample(norm_nums, 8, replace = TRUE)\n\n[1] 1135.7611 1514.6225 1330.9395 1019.8172 1176.6209  923.6370  910.2854\n[8] 1247.6103\n\n# Select 6 different random whole numbers between 1 and 100\nsample(1:100, 6, replace=FALSE)\n\n[1]  7 22 51 65 62  4\n\n# Randomly replace 25% of norm_nums with NA to simulate missing data\nn &lt;- length(norm_nums)\nidx &lt;- sample(1:n, round(n * .25), replace = FALSE)\nnorm_nums[idx] &lt;- NA\nnorm_nums\n\n [1] 1116.6379        NA 1231.9937  895.2387  853.6645 1247.6103        NA\n [8]        NA 1517.8996 1331.8473  546.5383  923.6370        NA 1277.3763\n[15]  312.3785        NA 1514.6225 1135.7611  714.0299 1362.7430 1024.7950\n[22] 1359.3195        NA        NA        NA  827.8140 1006.3039 1028.9449\n[29] 1222.6779  651.3089        NA 1019.8172  744.3571  661.3804  854.0215\n[36] 1078.0156 1873.4845 1312.8426 1265.6508 1229.0833 1105.4688  910.2854\n[43]  487.4748 1141.4996        NA        NA        NA 1330.9395 1176.6209\n[50]  684.7990"
  },
  {
    "objectID": "exercises/analysis-practice.html#data-analysis-exercise-1-due-october-25",
    "href": "exercises/analysis-practice.html#data-analysis-exercise-1-due-october-25",
    "title": "Analysis Practice",
    "section": "",
    "text": "What is at least one challenge you expect to encounter when analyzing your project’s data? Examples of challenges include:\n\n\nA high percent of missing data.\nGroups of interest are of uneven size.\nA specialized type of data which is difficult to deal with (such as geographic or time-series data).\nUnreliable data (values may be incorrect).\nData has to be combined from many different sources.\nHow the data was collected or recorded changed over time.\n\nTip: If you are unsure what challenges your project’s data might face, take a look at other papers which use the same or similar data. Do the authors mention any special data cleaning or analysis steps?\n\nHow do you plan to deal with this challenge? Are there specialized R packages which can help with this challenge? If you are unsure, how do you plan to learn how to deal with this challenge?\nA small, simulated dataset is useful when creating an analysis. Since we know the exact properties of the simulated dataset, we can make sure any calculations or corrections we perform in our analysis are correct by first trying them on the simulated dataset.\n\nCreate a small dataset (50 or fewer rows) which is similar to your project’s data and if possible contains the challenge you talk about above. You can create the dataset by hand, as a subset of real data, or by random generation.\nAttach your dataset and any code you used to generate your dataset to your solution.\nYou can create vectors of different types in R:\n\n# Make a vector of 10 0's and 0 empty strings \"\"\nnum_vec &lt;- numeric(10)\nchar_vec &lt;- character(10)\n\nnum_vec\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\nchar_vec\n\n [1] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\"\n\n\nAnd then assign values and create a dataframe:\n\nnum_vec[1:3] &lt;- 6\nnum_vec[4:10] &lt;- 8.5\nnum_vec\n\n [1] 6.0 6.0 6.0 8.5 8.5 8.5 8.5 8.5 8.5 8.5\n\nchar_vec[1:5] &lt;- \"Male\"\nchar_vec[5:10] &lt;- \"Female\"\nchar_vec\n\n [1] \"Male\"   \"Male\"   \"Male\"   \"Male\"   \"Female\" \"Female\" \"Female\" \"Female\"\n [9] \"Female\" \"Female\"\n\ndf &lt;- data.frame(char_vec, num_vec)\n\nYou can generate random numbers using functions like rnorm:\n\n# Generate 50 random numbers centered around 1000 with a standard deviation of 300\nnorm_nums &lt;- rnorm(50, 1000, 300)\nnorm_nums\n\n [1] 1116.6379  563.9742 1231.9937  895.2387  853.6645 1247.6103 1410.7942\n [8]  493.6071 1517.8996 1331.8473  546.5383  923.6370 1023.6361 1277.3763\n[15]  312.3785  936.9149 1514.6225 1135.7611  714.0299 1362.7430 1024.7950\n[22] 1359.3195  511.7774 1129.9905  711.4480  827.8140 1006.3039 1028.9449\n[29] 1222.6779  651.3089  399.4065 1019.8172  744.3571  661.3804  854.0215\n[36] 1078.0156 1873.4845 1312.8426 1265.6508 1229.0833 1105.4688  910.2854\n[43]  487.4748 1141.4996  586.9807  723.0083 1639.8555 1330.9395 1176.6209\n[50]  684.7990\n\n\nAnother useful function for generating example data is sample. It selects random items from a vector.\n\n# Select a random item 8 times from norm_nums\nsample(norm_nums, 8, replace = TRUE)\n\n[1] 1135.7611 1514.6225 1330.9395 1019.8172 1176.6209  923.6370  910.2854\n[8] 1247.6103\n\n# Select 6 different random whole numbers between 1 and 100\nsample(1:100, 6, replace=FALSE)\n\n[1]  7 22 51 65 62  4\n\n# Randomly replace 25% of norm_nums with NA to simulate missing data\nn &lt;- length(norm_nums)\nidx &lt;- sample(1:n, round(n * .25), replace = FALSE)\nnorm_nums[idx] &lt;- NA\nnorm_nums\n\n [1] 1116.6379        NA 1231.9937  895.2387  853.6645 1247.6103        NA\n [8]        NA 1517.8996 1331.8473  546.5383  923.6370        NA 1277.3763\n[15]  312.3785        NA 1514.6225 1135.7611  714.0299 1362.7430 1024.7950\n[22] 1359.3195        NA        NA        NA  827.8140 1006.3039 1028.9449\n[29] 1222.6779  651.3089        NA 1019.8172  744.3571  661.3804  854.0215\n[36] 1078.0156 1873.4845 1312.8426 1265.6508 1229.0833 1105.4688  910.2854\n[43]  487.4748 1141.4996        NA        NA        NA 1330.9395 1176.6209\n[50]  684.7990"
  },
  {
    "objectID": "exercises/analysis-practice.html#data-analysis-exercise-2-due-november-8",
    "href": "exercises/analysis-practice.html#data-analysis-exercise-2-due-november-8",
    "title": "Analysis Practice",
    "section": "Data analysis exercise 2: Due November 8",
    "text": "Data analysis exercise 2: Due November 8\n\nWhat is a data visualization or table you plan to create as a part of your analysis?\nUsing your simulated dataset dataset, create an example of the visualization. Try to get it as close to ‘publication ready’ as possible in terms of legends, axis labels, colors, text size, etc.\nExport your figure and attach it to your solution."
  },
  {
    "objectID": "exercises/analysis-practice.html#data-analysis-exercise-3-due-november-15",
    "href": "exercises/analysis-practice.html#data-analysis-exercise-3-due-november-15",
    "title": "Analysis Practice",
    "section": "Data analysis exercise 3: Due November 15",
    "text": "Data analysis exercise 3: Due November 15\n\nName 2 ways you could deal with either missing or duplicate data.\nWill the data you’re using for your project contain duplicate and/or missing data?\nWrite R code on your sample data to demonstrate one of the solutions you mentioned. If the data for your project will contain neither, use the NHANES adolescent diabetes dataset from in-class."
  },
  {
    "objectID": "exercises/analysis-practice.html#data-analysis-exercise-4-due-november-29",
    "href": "exercises/analysis-practice.html#data-analysis-exercise-4-due-november-29",
    "title": "Analysis Practice",
    "section": "Data analysis exercise 4: Due November 29",
    "text": "Data analysis exercise 4: Due November 29\n\nChoose a relationship between 2 or more variables in your project’s data you wish to explore the relationship between.\nWhat test or model would you use to explore that relationship?\nWrite R code using to run your chosen model or test on your simulated dataset.\nCreate the results figure, statement, or table you plan to use in your final report around this result, but using the result from your simulated dataset."
  },
  {
    "objectID": "exercises/exercises.html",
    "href": "exercises/exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "This set of pages practice exercises split into 2 main categories.\n\n\nThese short exercises cover basic R coding and data manipulation. One similarity of learning coding and learning a foreign language is that learned skills can quickly fade if not used. The purpose of these exercises are to help you develop and maintain skills in basic data manipulation in R.\nThese exercises are to be completed between the end of the bootcamp (August 30) and the beginning of the data analysis module (October 11). There will be 3-4 in total.\n\n\n\nThese short exercises will ask you to complete tasks related to your project. They will allow you to practice the skills we learn in-class and seeing how they apply to the particular data you plan to use in your project.\nIf you have not yet determined or been able to access your data, or a particular exercise wouldn’t make sense with your data, backup data from NHANES will be provided.\n\n\n\nThe exercises should take 1 - 1.5 hours. As this is the first the we are running the course in this format, it is likely that the length of some of the problem sets will be need additional tuning. Therefore, if you find a week’s exercises taking significantly more than an hour, please reach out instructors. We want to make sure we are not overly burdening your already busy semester."
  },
  {
    "objectID": "exercises/exercises.html#basic-practice-august-30---october-11",
    "href": "exercises/exercises.html#basic-practice-august-30---october-11",
    "title": "Exercises",
    "section": "",
    "text": "These short exercises cover basic R coding and data manipulation. One similarity of learning coding and learning a foreign language is that learned skills can quickly fade if not used. The purpose of these exercises are to help you develop and maintain skills in basic data manipulation in R.\nThese exercises are to be completed between the end of the bootcamp (August 30) and the beginning of the data analysis module (October 11). There will be 3-4 in total."
  },
  {
    "objectID": "exercises/exercises.html#project-practice",
    "href": "exercises/exercises.html#project-practice",
    "title": "Exercises",
    "section": "",
    "text": "These short exercises will ask you to complete tasks related to your project. They will allow you to practice the skills we learn in-class and seeing how they apply to the particular data you plan to use in your project.\nIf you have not yet determined or been able to access your data, or a particular exercise wouldn’t make sense with your data, backup data from NHANES will be provided."
  },
  {
    "objectID": "exercises/exercises.html#exercise-length",
    "href": "exercises/exercises.html#exercise-length",
    "title": "Exercises",
    "section": "",
    "text": "The exercises should take 1 - 1.5 hours. As this is the first the we are running the course in this format, it is likely that the length of some of the problem sets will be need additional tuning. Therefore, if you find a week’s exercises taking significantly more than an hour, please reach out instructors. We want to make sure we are not overly burdening your already busy semester."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Licenses",
    "section": "",
    "text": "All Software Carpentry, Data Carpentry, and Library Carpentry instructional material is made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license.\nYou are free:\n\nto Share—copy and redistribute the material in any medium or format\nto Adapt—remix, transform, and build upon the material\n\nfor any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following terms:\n\nAttribution—You must give appropriate credit (mentioning that your work is derived from work that is Copyright © Software Carpentry and, where practical, linking to http://software-carpentry.org/), provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nNo additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. With the understanding that:\nNotices:\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material."
  },
  {
    "objectID": "LICENSE.html#instructional-material",
    "href": "LICENSE.html#instructional-material",
    "title": "Licenses",
    "section": "",
    "text": "All Software Carpentry, Data Carpentry, and Library Carpentry instructional material is made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license.\nYou are free:\n\nto Share—copy and redistribute the material in any medium or format\nto Adapt—remix, transform, and build upon the material\n\nfor any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following terms:\n\nAttribution—You must give appropriate credit (mentioning that your work is derived from work that is Copyright © Software Carpentry and, where practical, linking to http://software-carpentry.org/), provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nNo additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. With the understanding that:\nNotices:\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material."
  },
  {
    "objectID": "LICENSE.html#software",
    "href": "LICENSE.html#software",
    "title": "Licenses",
    "section": "Software",
    "text": "Software\nExcept where otherwise noted, the example programs and other software provided by Software Carpentry and Data Carpentry are made available under the OSI-approved MIT license.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "LICENSE.html#trademark",
    "href": "LICENSE.html#trademark",
    "title": "Licenses",
    "section": "Trademark",
    "text": "Trademark\n“Software Carpentry” and “Data Carpentry” and their respective logos are registered trademarks of Community Initiatives."
  },
  {
    "objectID": "readings/data-basics.html",
    "href": "readings/data-basics.html",
    "title": "Core Data Functions",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\npacman::p_load(\n  rio,        # importing data  \n  here,       # relative file pathways  \n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  matchmaker, # dictionary-based cleaning\n  epikit,     # age_categories() function\n  tidyverse   # data management and visualization\n)\nlinelist &lt;- rio::import(\"data/case_linelists/linelist_cleaned.rds\")\n\n\n\n\nThis week we are going to learn how to use R to manipulate data. This will include learning about core functions for manipulating and summarizing data, as well as using conditional statements to create subsets.\n\n\nAn operators is a symbol or set of symbols representing some mathematical or logical operation. They are essentially equivalent to functions. R has a number of built-in operators, and libraries may add additional operators (such as the %&gt;% operator used in Tidyverse packages). Some examples of operators are:\n\nDefinitional operators\n\nRelational operators (less than, equal too..)\n\nLogical operators (and, or…)\n\nHandling missing values\n\nMathematical operators and functions (+/-, &gt;, sum(), median(), …)\n\nThe %in% operator\n\nR also has some built-in constants, which have the same meaning in programming as in mathematics and statistics. Examples of constants in R include:\n\npi which in base R equals 3.141593\nInf and -Inf for positive and negative infinity\nNaN for not a number (such as the result of 0/0)\nNA for missing data\n\n\n\n\n\n&lt;-\nThe basic assignment operator in R is &lt;-. Such that object_name &lt;- value.\nThis assignment operator can also be written as =. We advise use of &lt;- for general R use.\nWe also advise surrounding such operators with spaces, for readability.\n\n\n\n\nRelational operators compare values and are often used when defining new variables and subsets of datasets. Here are the common relational operators in R:\n\n\n\n\n\n\n\n\n\nMeaning\nOperator\nExample\nExample Result\n\n\n\n\nEqual to\n==\n\"A\" == \"a\"\nFALSE (because R is case sensitive) Note that == (double equals) is different from = (single equals), which acts like the assignment operator &lt;-\n\n\nNot equal to\n!=\n2 != 0\nTRUE\n\n\nGreater than\n&gt;\n4 &gt; 2\nTRUE\n\n\nLess than\n&lt;\n4 &lt; 2\nFALSE\n\n\nGreater than or equal to\n&gt;=\n6 &gt;= 4\nTRUE\n\n\nLess than or equal to\n&lt;=\n6 &lt;= 4\nFALSE\n\n\nValue is missing\nis.na()\nis.na(7)\nFALSE\n\n\nValue is not missing\n!is.na()\n!is.na(7)\nTRUE\n\n\n\nLogical operators, such as AND and OR, are often used to connect relational operators and create more complicated criteria. Complex statements might require parentheses ( ) for grouping and order of application.\n\n\n\n\n\n\n\nMeaning\nOperator\n\n\n\n\nAND\n&\n\n\nOR\n| (vertical bar)\n\n\nParentheses\n( ) Used to group criteria together and clarify order of operations\n\n\n\nFor example, below, we have a linelist with two variables we want to use to create our case definition, hep_e_rdt, a test result and other_cases_in_hh, which will tell us if there are other cases in the household. The command below uses the function case_when() to create the new variable case_def such that:\n\nlinelist_cleaned &lt;- linelist %&gt;%\n  mutate(case_def = case_when(\n    is.na(rdt_result) & is.na(other_case_in_home)            ~ NA_character_,\n    rdt_result == \"Positive\"                                 ~ \"Confirmed\",\n    rdt_result != \"Positive\" & other_cases_in_home == \"Yes\"  ~ \"Probable\",\n    TRUE                                                     ~ \"Suspected\"\n  ))\n\n\n\n\n\n\n\n\nCriteria in example above\nResulting value in new variable “case_def”\n\n\n\n\nIf the value for variables rdt_result and other_cases_in_home are missing\nNA (missing)\n\n\nIf the value in rdt_result is “Positive”\n“Confirmed”\n\n\nIf the value in rdt_result is NOT “Positive” AND the value in other_cases_in_home is “Yes”\n“Probable”\n\n\nIf one of the above criteria are not met\n“Suspected”\n\n\n\nNote that R is case-sensitive, so “Positive” is different than “positive”…\n\n\n\n\nIn R, missing values are represented by the special value NA (a “reserved” value) (capital letters N and A - not in quotation marks). To test whether a value is NA, use the special function is.na(), which returns TRUE or FALSE.\n\nrdt_result &lt;- c(\"Positive\", \"Suspected\", \"Positive\", NA)   # two positive cases, one suspected, and one unknown\nis.na(rdt_result)  # Tests whether the value of rdt_result is NA\n\n[1] FALSE FALSE FALSE  TRUE\n\n\nWe will be learning more about how to deal with missing data in future weeks.\n\n\n\n\nAll the operators and functions in this page are automatically available using base R.\n\n\nThese are often used to perform addition, division, to create new columns, etc. Below are common mathematical operators in R. Whether you put spaces around the operators is not important.\n\n\n\nPurpose\nExample in R\n\n\n\n\naddition\n2 + 3\n\n\nsubtraction\n2 - 3\n\n\nmultiplication\n2 * 3\n\n\ndivision\n30 / 5\n\n\nexponent\n2^3\n\n\norder of operations\n( )\n\n\n\n\n\n\n\n\n\nPurpose\nFunction\n\n\n\n\nrounding\nround(x, digits = n)\n\n\nrounding\njanitor::round_half_up(x, digits = n)\n\n\nceiling (round up)\nceiling(x)\n\n\nfloor (round down)\nfloor(x)\n\n\nabsolute value\nabs(x)\n\n\nsquare root\nsqrt(x)\n\n\nexponent\nexponent(x)\n\n\nnatural logarithm\nlog(x)\n\n\nlog base 10\nlog10(x)\n\n\nlog base 2\nlog2(x)\n\n\n\nNote: for round() the digits = specifies the number of decimal placed. Use signif() to round to a number of significant figures.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe functions below will by default include missing values in calculations. Missing values will result in an output of NA, unless the argument na.rm = TRUE is specified. This can be written shorthand as na.rm = T.\n\n\n\n\n\nObjective\nFunction\n\n\n\n\nmean (average)\nmean(x, na.rm=T)\n\n\nmedian\nmedian(x, na.rm=T)\n\n\nstandard deviation\nsd(x, na.rm=T)\n\n\nquantiles*\nquantile(x, probs)\n\n\nsum\nsum(x, na.rm=T)\n\n\nminimum value\nmin(x, na.rm=T)\n\n\nmaximum value\nmax(x, na.rm=T)\n\n\nrange of numeric values\nrange(x, na.rm=T)\n\n\nsummary**\nsummary(x)\n\n\n\nNotes:\n\n*quantile(): x is the numeric vector to examine, and probs = is a numeric vector with probabilities within 0 and 1.0, e.g c(0.5, 0.8, 0.85)\n**summary(): gives a summary on a numeric vector including mean, median, and common percentiles\n\n\n\n\n\n\n\nWarning\n\n\n\nIf providing a vector of numbers to one of the above functions, be sure to wrap the numbers within c() .}\n\n# If supplying raw numbers to a function, wrap them in c()\nmean(1, 6, 12, 10, 5, 0)    # !!! INCORRECT !!!  \n\n[1] 1\n\nmean(c(1, 6, 12, 10, 5, 0)) # CORRECT\n\n[1] 5.666667\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObjective\nFunction\nExample\n\n\n\n\ncreate a sequence\nseq(from, to, by)\nseq(1, 10, 2)\n\n\nrepeat x, n times\nrep(x, ntimes)\nrep(1:3, 2) or rep(c(\"a\", \"b\", \"c\"), 3)\n\n\nsubdivide a numeric vector\ncut(x, n)\ncut(linelist$age, 5)\n\n\ntake a random sample\nsample(x, size)\nsample(linelist$id, size = 5, replace = TRUE)\n\n\n\n\n\n\n\nA very useful operator for matching values, and for quickly assessing if a value is within a vector or dataframe.\n\nmy_vector &lt;- c(\"a\", \"b\", \"c\", \"d\")\n\n\n\"a\" %in% my_vector\n\n[1] TRUE\n\n\"h\" %in% my_vector\n\n[1] FALSE\n\n\nTo ask if a value is not %in% a vector, put an exclamation mark (!) in front of the logic statement:\n\n# to negate, put an exclamation in front\n!\"a\" %in% my_vector\n\n[1] FALSE\n\n!\"h\" %in% my_vector\n\n[1] TRUE\n\n\n%in% is very useful when using the dplyr function case_when(). You can define a vector previously, and then reference it later. For example:\n\naffirmative &lt;- c(\"1\", \"Yes\", \"YES\", \"yes\", \"y\", \"Y\", \"oui\", \"Oui\", \"Si\")\n\nlinelist &lt;- linelist %&gt;% \n  mutate(child_hospitaled = case_when(\n    hospitalized %in% affirmative & age &lt; 18 ~ \"Hospitalized Child\",\n    TRUE                                      ~ \"Not\"))\n\n\n\n\nWe will be emphasizing use of the functions from the tidyverse family of R packages. The functions we will be learning about are listed below.\nMany of these functions belong to the dplyr R package, which provides “verb” functions to solve data manipulation challenges (the name is a reference to a “data frame-plier. dplyr is part of the tidyverse family of R packages (which also includes ggplot2, tidyr, stringr, tibble, purrr, magrittr, and forcats among others).\n\n\n\n\n\n\n\n\nFunction\nUtility\nPackage\n\n\n\n\n%&gt;%\n“pipe” (pass) data from one function to the next\nmagrittr\n\n\nmutate()\ncreate, transform, and re-define columns\ndplyr\n\n\nselect()\nkeep, remove, select, or re-name columns\ndplyr\n\n\nrename()\nrename columns\ndplyr\n\n\nclean_names()\nstandardize the syntax of column names\njanitor\n\n\nas.character(), as.numeric(), as.Date(), etc.\nconvert the class of a column\nbase R\n\n\nacross()\ntransform multiple columns at one time\ndplyr\n\n\ntidyselect functions\nuse logic to select columns\ntidyselect\n\n\nfilter()\nkeep certain rows\ndplyr\n\n\ndistinct()\nde-duplicate rows\ndplyr\n\n\nrowwise()\noperations by/within each row\ndplyr\n\n\nadd_row()\nadd rows manually\ntibble\n\n\narrange()\nsort rows\ndplyr\n\n\nrecode()\nre-code values in a column\ndplyr\n\n\ncase_when()\nre-code values in a column using more complex logical criteria\ndplyr\n\n\nreplace_na(), na_if(), coalesce()\nspecial functions for re-coding\ntidyr\n\n\nage_categories() and cut()\ncreate categorical groups from a numeric column\nepikit and base R\n\n\nmatch_df()\nre-code/clean values using a data dictionary\nmatchmaker\n\n\nwhich()\napply logical criteria; return indices\nbase R"
  },
  {
    "objectID": "readings/data-basics.html#operators",
    "href": "readings/data-basics.html#operators",
    "title": "Core Data Functions",
    "section": "",
    "text": "An operators is a symbol or set of symbols representing some mathematical or logical operation. They are essentially equivalent to functions. R has a number of built-in operators, and libraries may add additional operators (such as the %&gt;% operator used in Tidyverse packages). Some examples of operators are:\n\nDefinitional operators\n\nRelational operators (less than, equal too..)\n\nLogical operators (and, or…)\n\nHandling missing values\n\nMathematical operators and functions (+/-, &gt;, sum(), median(), …)\n\nThe %in% operator\n\nR also has some built-in constants, which have the same meaning in programming as in mathematics and statistics. Examples of constants in R include:\n\npi which in base R equals 3.141593\nInf and -Inf for positive and negative infinity\nNaN for not a number (such as the result of 0/0)\nNA for missing data"
  },
  {
    "objectID": "readings/data-basics.html#assignment-operators",
    "href": "readings/data-basics.html#assignment-operators",
    "title": "Core Data Functions",
    "section": "",
    "text": "&lt;-\nThe basic assignment operator in R is &lt;-. Such that object_name &lt;- value.\nThis assignment operator can also be written as =. We advise use of &lt;- for general R use.\nWe also advise surrounding such operators with spaces, for readability."
  },
  {
    "objectID": "readings/data-basics.html#relational-and-logical-operators",
    "href": "readings/data-basics.html#relational-and-logical-operators",
    "title": "Core Data Functions",
    "section": "",
    "text": "Relational operators compare values and are often used when defining new variables and subsets of datasets. Here are the common relational operators in R:\n\n\n\n\n\n\n\n\n\nMeaning\nOperator\nExample\nExample Result\n\n\n\n\nEqual to\n==\n\"A\" == \"a\"\nFALSE (because R is case sensitive) Note that == (double equals) is different from = (single equals), which acts like the assignment operator &lt;-\n\n\nNot equal to\n!=\n2 != 0\nTRUE\n\n\nGreater than\n&gt;\n4 &gt; 2\nTRUE\n\n\nLess than\n&lt;\n4 &lt; 2\nFALSE\n\n\nGreater than or equal to\n&gt;=\n6 &gt;= 4\nTRUE\n\n\nLess than or equal to\n&lt;=\n6 &lt;= 4\nFALSE\n\n\nValue is missing\nis.na()\nis.na(7)\nFALSE\n\n\nValue is not missing\n!is.na()\n!is.na(7)\nTRUE\n\n\n\nLogical operators, such as AND and OR, are often used to connect relational operators and create more complicated criteria. Complex statements might require parentheses ( ) for grouping and order of application.\n\n\n\n\n\n\n\nMeaning\nOperator\n\n\n\n\nAND\n&\n\n\nOR\n| (vertical bar)\n\n\nParentheses\n( ) Used to group criteria together and clarify order of operations\n\n\n\nFor example, below, we have a linelist with two variables we want to use to create our case definition, hep_e_rdt, a test result and other_cases_in_hh, which will tell us if there are other cases in the household. The command below uses the function case_when() to create the new variable case_def such that:\n\nlinelist_cleaned &lt;- linelist %&gt;%\n  mutate(case_def = case_when(\n    is.na(rdt_result) & is.na(other_case_in_home)            ~ NA_character_,\n    rdt_result == \"Positive\"                                 ~ \"Confirmed\",\n    rdt_result != \"Positive\" & other_cases_in_home == \"Yes\"  ~ \"Probable\",\n    TRUE                                                     ~ \"Suspected\"\n  ))\n\n\n\n\n\n\n\n\nCriteria in example above\nResulting value in new variable “case_def”\n\n\n\n\nIf the value for variables rdt_result and other_cases_in_home are missing\nNA (missing)\n\n\nIf the value in rdt_result is “Positive”\n“Confirmed”\n\n\nIf the value in rdt_result is NOT “Positive” AND the value in other_cases_in_home is “Yes”\n“Probable”\n\n\nIf one of the above criteria are not met\n“Suspected”\n\n\n\nNote that R is case-sensitive, so “Positive” is different than “positive”…"
  },
  {
    "objectID": "readings/data-basics.html#missing-values",
    "href": "readings/data-basics.html#missing-values",
    "title": "Core Data Functions",
    "section": "",
    "text": "In R, missing values are represented by the special value NA (a “reserved” value) (capital letters N and A - not in quotation marks). To test whether a value is NA, use the special function is.na(), which returns TRUE or FALSE.\n\nrdt_result &lt;- c(\"Positive\", \"Suspected\", \"Positive\", NA)   # two positive cases, one suspected, and one unknown\nis.na(rdt_result)  # Tests whether the value of rdt_result is NA\n\n[1] FALSE FALSE FALSE  TRUE\n\n\nWe will be learning more about how to deal with missing data in future weeks."
  },
  {
    "objectID": "readings/data-basics.html#mathematics-and-statistics",
    "href": "readings/data-basics.html#mathematics-and-statistics",
    "title": "Core Data Functions",
    "section": "",
    "text": "All the operators and functions in this page are automatically available using base R.\n\n\nThese are often used to perform addition, division, to create new columns, etc. Below are common mathematical operators in R. Whether you put spaces around the operators is not important.\n\n\n\nPurpose\nExample in R\n\n\n\n\naddition\n2 + 3\n\n\nsubtraction\n2 - 3\n\n\nmultiplication\n2 * 3\n\n\ndivision\n30 / 5\n\n\nexponent\n2^3\n\n\norder of operations\n( )\n\n\n\n\n\n\n\n\n\nPurpose\nFunction\n\n\n\n\nrounding\nround(x, digits = n)\n\n\nrounding\njanitor::round_half_up(x, digits = n)\n\n\nceiling (round up)\nceiling(x)\n\n\nfloor (round down)\nfloor(x)\n\n\nabsolute value\nabs(x)\n\n\nsquare root\nsqrt(x)\n\n\nexponent\nexponent(x)\n\n\nnatural logarithm\nlog(x)\n\n\nlog base 10\nlog10(x)\n\n\nlog base 2\nlog2(x)\n\n\n\nNote: for round() the digits = specifies the number of decimal placed. Use signif() to round to a number of significant figures.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe functions below will by default include missing values in calculations. Missing values will result in an output of NA, unless the argument na.rm = TRUE is specified. This can be written shorthand as na.rm = T.\n\n\n\n\n\nObjective\nFunction\n\n\n\n\nmean (average)\nmean(x, na.rm=T)\n\n\nmedian\nmedian(x, na.rm=T)\n\n\nstandard deviation\nsd(x, na.rm=T)\n\n\nquantiles*\nquantile(x, probs)\n\n\nsum\nsum(x, na.rm=T)\n\n\nminimum value\nmin(x, na.rm=T)\n\n\nmaximum value\nmax(x, na.rm=T)\n\n\nrange of numeric values\nrange(x, na.rm=T)\n\n\nsummary**\nsummary(x)\n\n\n\nNotes:\n\n*quantile(): x is the numeric vector to examine, and probs = is a numeric vector with probabilities within 0 and 1.0, e.g c(0.5, 0.8, 0.85)\n**summary(): gives a summary on a numeric vector including mean, median, and common percentiles\n\n\n\n\n\n\n\nWarning\n\n\n\nIf providing a vector of numbers to one of the above functions, be sure to wrap the numbers within c() .}\n\n# If supplying raw numbers to a function, wrap them in c()\nmean(1, 6, 12, 10, 5, 0)    # !!! INCORRECT !!!  \n\n[1] 1\n\nmean(c(1, 6, 12, 10, 5, 0)) # CORRECT\n\n[1] 5.666667"
  },
  {
    "objectID": "readings/data-basics.html#other-useful-functions",
    "href": "readings/data-basics.html#other-useful-functions",
    "title": "Core Data Functions",
    "section": "",
    "text": "Objective\nFunction\nExample\n\n\n\n\ncreate a sequence\nseq(from, to, by)\nseq(1, 10, 2)\n\n\nrepeat x, n times\nrep(x, ntimes)\nrep(1:3, 2) or rep(c(\"a\", \"b\", \"c\"), 3)\n\n\nsubdivide a numeric vector\ncut(x, n)\ncut(linelist$age, 5)\n\n\ntake a random sample\nsample(x, size)\nsample(linelist$id, size = 5, replace = TRUE)"
  },
  {
    "objectID": "readings/data-basics.html#in",
    "href": "readings/data-basics.html#in",
    "title": "Core Data Functions",
    "section": "",
    "text": "A very useful operator for matching values, and for quickly assessing if a value is within a vector or dataframe.\n\nmy_vector &lt;- c(\"a\", \"b\", \"c\", \"d\")\n\n\n\"a\" %in% my_vector\n\n[1] TRUE\n\n\"h\" %in% my_vector\n\n[1] FALSE\n\n\nTo ask if a value is not %in% a vector, put an exclamation mark (!) in front of the logic statement:\n\n# to negate, put an exclamation in front\n!\"a\" %in% my_vector\n\n[1] FALSE\n\n!\"h\" %in% my_vector\n\n[1] TRUE\n\n\n%in% is very useful when using the dplyr function case_when(). You can define a vector previously, and then reference it later. For example:\n\naffirmative &lt;- c(\"1\", \"Yes\", \"YES\", \"yes\", \"y\", \"Y\", \"oui\", \"Oui\", \"Si\")\n\nlinelist &lt;- linelist %&gt;% \n  mutate(child_hospitaled = case_when(\n    hospitalized %in% affirmative & age &lt; 18 ~ \"Hospitalized Child\",\n    TRUE                                      ~ \"Not\"))"
  },
  {
    "objectID": "readings/data-basics.html#tidyverse-functions",
    "href": "readings/data-basics.html#tidyverse-functions",
    "title": "Core Data Functions",
    "section": "",
    "text": "We will be emphasizing use of the functions from the tidyverse family of R packages. The functions we will be learning about are listed below.\nMany of these functions belong to the dplyr R package, which provides “verb” functions to solve data manipulation challenges (the name is a reference to a “data frame-plier. dplyr is part of the tidyverse family of R packages (which also includes ggplot2, tidyr, stringr, tibble, purrr, magrittr, and forcats among others).\n\n\n\n\n\n\n\n\nFunction\nUtility\nPackage\n\n\n\n\n%&gt;%\n“pipe” (pass) data from one function to the next\nmagrittr\n\n\nmutate()\ncreate, transform, and re-define columns\ndplyr\n\n\nselect()\nkeep, remove, select, or re-name columns\ndplyr\n\n\nrename()\nrename columns\ndplyr\n\n\nclean_names()\nstandardize the syntax of column names\njanitor\n\n\nas.character(), as.numeric(), as.Date(), etc.\nconvert the class of a column\nbase R\n\n\nacross()\ntransform multiple columns at one time\ndplyr\n\n\ntidyselect functions\nuse logic to select columns\ntidyselect\n\n\nfilter()\nkeep certain rows\ndplyr\n\n\ndistinct()\nde-duplicate rows\ndplyr\n\n\nrowwise()\noperations by/within each row\ndplyr\n\n\nadd_row()\nadd rows manually\ntibble\n\n\narrange()\nsort rows\ndplyr\n\n\nrecode()\nre-code values in a column\ndplyr\n\n\ncase_when()\nre-code values in a column using more complex logical criteria\ndplyr\n\n\nreplace_na(), na_if(), coalesce()\nspecial functions for re-coding\ntidyr\n\n\nage_categories() and cut()\ncreate categorical groups from a numeric column\nepikit and base R\n\n\nmatch_df()\nre-code/clean values using a data dictionary\nmatchmaker\n\n\nwhich()\napply logical criteria; return indices\nbase R"
  },
  {
    "objectID": "readings/formatting-tables.html",
    "href": "readings/formatting-tables.html",
    "title": "Tables for presentation",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\n\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nIn this chapter we’ll learn how to convert summary data frames into presentation-ready tables with the flextable package. These tables can be inserted into powerpoint slides, HTML pages, PDF or Word documents, etc.\nUnderstand that before using flextable, you must create the summary table as a data frame as we saw in the previous chapter. The resulting data frame can then be passed to flextable for display formatting.\nThere are many other R packages that can be used to craft tables for presentation - we chose to highlight flextable in this chapter.\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\npacman::p_load(\n  rio,            # import/export\n  here,           # file pathways\n  flextable,      # make HTML tables \n  officer,        # helper functions for tables\n  tidyverse)      # data management, summary, and visualization\n\n\n\n\nTo begin, we import the cleaned linelist of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file).\n\n# import the linelist\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\n\n\n\nBefore beginning to use flextable you will need to create your table as a data frame as we have seen in the previous chapter:\n\ntable &lt;- linelist %&gt;% \n  \n  # Get summary values per hospital-outcome group\n  ###############################################\n  group_by(hospital, outcome) %&gt;%                      # Group data\n  summarise(                                           # Create new summary columns of indicators of interest\n    N = n(),                                            # Number of rows per hospital-outcome group     \n    ct_value = median(ct_blood, na.rm=T)) %&gt;%           # median CT value per group\n  \n  # add totals\n  ############\n  bind_rows(                                           # Bind the previous table with this mini-table of totals\n    linelist %&gt;% \n      filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%\n      group_by(outcome) %&gt;%                            # Grouped only by outcome, not by hospital    \n      summarise(\n        N = n(),                                       # Number of rows for whole dataset     \n        ct_value = median(ct_blood, na.rm=T))) %&gt;%     # Median CT for whole dataset\n  \n  # Pivot wider and format\n  ########################\n  mutate(hospital = replace_na(hospital, \"Total\")) %&gt;% \n  pivot_wider(                                         # Pivot from long to wide\n    values_from = c(ct_value, N),                       # new values are from ct and count columns\n    names_from = outcome) %&gt;%                           # new column names are from outcomes\n  mutate(                                              # Add new columns\n    N_Known = N_Death + N_Recover,                               # number with known outcome\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # percent who recovered (to 1 decimal)\n  select(                                              # Re-order columns\n    hospital, N_Known,                                   # Intro columns\n    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns\n    N_Death, Pct_Death, ct_value_Death)  %&gt;%             # Death columns\n  arrange(N_Known)                                    # Arrange rows from lowest to highest (Total row at bottom)\n\ntable  # print\n\n# A tibble: 7 × 8\n# Groups:   hospital [7]\n  hospital      N_Known N_Recover Pct_Recover ct_value_Recover N_Death Pct_Death\n  &lt;chr&gt;           &lt;int&gt;     &lt;int&gt; &lt;chr&gt;                  &lt;dbl&gt;   &lt;int&gt; &lt;chr&gt;    \n1 St. Mark's M…     325       126 38.8%                     22     199 61.2%    \n2 Central Hosp…     358       165 46.1%                     22     193 53.9%    \n3 Other             685       290 42.3%                     21     395 57.7%    \n4 Military Hos…     708       309 43.6%                     22     399 56.4%    \n5 Missing          1125       514 45.7%                     21     611 54.3%    \n6 Port Hospital    1364       579 42.4%                     21     785 57.6%    \n7 Total            3440      1469 42.7%                     22    1971 57.3%    \n# ℹ 1 more variable: ct_value_Death &lt;dbl&gt;\n\n\n\n\n\n\n\n\n\n\n\nTo create and manage flextable objects, we first pass the data frame through the flextable() function. We save the result as my_table.\n\nmy_table &lt;- flextable(table) \nmy_table\n\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nAfter doing this, we can progressively pipe the my_table object through more flextable formatting functions.\nIn this page for sake of clarity we will save the table at intermediate steps as my_table, adding flextable functions bit-by-bit. If you want to see all the code from beginning to end written in one chunk, visit the All code together section below.\nThe general syntax of each line of flextable code is as follows:\n\nfunction(table, i = X, j = X, part = \"X\"), where:\n\nThe ‘function’ can be one of many different functions, such as width() to determine column widths, bg() to set background colours, align() to set whether text is centre/right/left aligned, and so on.\ntable = is the name of the data frame, although does not need to be stated if the data frame is piped into the function.\npart = refers to which part of the table the function is being applied to. E.g. “header”, “body” or “all”.\ni = specifies the row to apply the function to, where ‘X’ is the row number. If multiple rows, e.g. the first to third rows, one can specify: i = c(1:3). Note if ‘body’ is selected, the first row starts from underneath the header section.\nj = specifies the column to apply the function to, where ‘x’ is the column number or name. If multiple columns, e.g. the fifth and sixth, one can specify: j = c(5,6).\n\n\nYou can find the complete list of flextable formatting function here or review the documentation by entering ?flextable.\n\n\n\nWe can use the autofit() function, which nicely stretches out the table so that each cell only has one row of text. The function qflextable() is a convenient shorthand for flextable() and autofit().\n\nmy_table %&gt;% autofit()\n\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nHowever, this might not always be appropriate, especially if there are very long values within cells, meaning the table might not fit on the page.\nInstead, we can specify widths with the width() function. It can take some playing around to know what width value to put. In the example below, we specify different widths for column 1, column 2, and columns 4 to 8.\n\nmy_table &lt;- my_table %&gt;% \n  width(j=1, width = 2.7) %&gt;% \n  width(j=2, width = 1.5) %&gt;% \n  width(j=c(4,5,7,8), width = 1)\n\nmy_table\n\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nWe want more clearer headers for easier interpretation of the table contents.\nFor this table, we will want to add a second header layer so that columns covering the same subgroups can be grouped together. We do this with the add_header_row() function with top = TRUE. We provide the new name of each column to values =, leaving empty values \"\" for columns we know we will merge together later.\nWe also rename the header names in the now-second header in a separate set_header_labels() command.\nFinally, to “combine” certain column headers in the top header we use merge_at() to merge the column headers in the top header row.\n\nmy_table &lt;- my_table %&gt;% \n  \n  add_header_row(\n    top = TRUE,                # New header goes on top of existing header row\n    values = c(\"Hospital\",     # Header values for each column below\n               \"Total cases with known outcome\", \n               \"Recovered\",    # This will be the top-level header for this and two next columns\n               \"\",\n               \"\",\n               \"Died\",         # This will be the top-level header for this and two next columns\n               \"\",             # Leave blank, as it will be merged with \"Died\"\n               \"\")) %&gt;% \n    \n  set_header_labels(         # Rename the columns in original header row\n      hospital = \"\", \n      N_Known = \"\",                  \n      N_Recover = \"Total\",\n      Pct_Recover = \"% of cases\",\n      ct_value_Recover = \"Median CT values\",\n      N_Death = \"Total\",\n      Pct_Death = \"% of cases\",\n      ct_value_Death = \"Median CT values\")  %&gt;% \n  \n  merge_at(i = 1, j = 3:5, part = \"header\") %&gt;% # Horizontally merge columns 3 to 5 in new header row\n  merge_at(i = 1, j = 6:8, part = \"header\")     # Horizontally merge columns 6 to 8 in new header row\n\nmy_table  # print\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nYou can adjust the borders, internal lines, etc. with various flextable functions. It is often easier to start by removing all existing borders with border_remove().\nThen, you can apply default border themes by passing the table to theme_box(), theme_booktabs(), or theme_alafoli().\nYou can add vertical and horizontal lines with a variety of functions. hline() and vline() add lines to a specified row or column, respectively. Within each, you must specify the part = as either “all”, “body”, or “header”. For vertical lines, specify the column to j =, and for horizontal lines the row to i =. Other functions like vline_right(), vline_left(), hline_top(), and hline_bottom() add lines to the outsides only.\nIn all of these functions, the actual line style itself must be specified to border = and must be the output of a separate command using the fp_border() function from the officer package. This function helps you define the width and color of the line. You can define this above the table commands, as shown below.\n\n# define style for border line\nborder_style = officer::fp_border(color=\"black\", width=1)\n\n# add border lines to table\nmy_table &lt;- my_table %&gt;% \n\n  # Remove all existing borders\n  border_remove() %&gt;%  \n  \n  # add horizontal lines via a pre-determined theme setting\n  theme_booktabs() %&gt;% \n  \n  # add vertical lines to separate Recovered and Died sections\n  vline(part = \"all\", j = 2, border = border_style) %&gt;%   # at column 2 \n  vline(part = \"all\", j = 5, border = border_style)       # at column 5\n\nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nWe centre-align all columns aside from the left-most column with the hospital names, using the align() function from flextable.\n\nmy_table &lt;- my_table %&gt;% \n   flextable::align(align = \"center\", j = c(2:8), part = \"all\") \nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nAdditionally, we can increase the header font size and change then to bold. We can also change the total row to bold.\n\nmy_table &lt;-  my_table %&gt;%  \n  fontsize(i = 1, size = 12, part = \"header\") %&gt;%   # adjust font size of header\n  bold(i = 1, bold = TRUE, part = \"header\") %&gt;%     # adjust bold face of header\n  bold(i = 7, bold = TRUE, part = \"body\")           # adjust bold face of total row (row 7 of body)\n\nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nWe can ensure that the proportion columns display only one decimal place using the function colformat_num(). Note this could also have been done at data management stage with the round() function.\n\nmy_table &lt;- colformat_num(my_table, j = c(4,7), digits = 1)\nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nJust as we merge cells horizontally in the header row, we can also merge cells vertically using merge_at() and specifying the rows (i) and column (j). Here we merge the “Hospital” and “Total cases with known outcome” values vertically to give them more space.\n\nmy_table &lt;- my_table %&gt;% \n  merge_at(i = 1:2, j = 1, part = \"header\") %&gt;% \n  merge_at(i = 1:2, j = 2, part = \"header\")\n\nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nTo distinguish the content of the table from the headers, we may want to add additional formatting. e.g. changing the background color. In this example we change the table body to gray.\n\nmy_table &lt;- my_table %&gt;% \n    bg(part = \"body\", bg = \"gray95\")  \n\nmy_table \n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\n\n\nWe can highlight all values in a column that meet a certain rule, e.g. where more than 55% of cases died. Simply put the criteria to the i = or j = argument, preceded by a tilde ~. Reference the column in the data frame, not the display heading values.\n\nmy_table %&gt;% \n  bg(j = 7, i = ~ Pct_Death &gt;= 55, part = \"body\", bg = \"red\") \n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nOr, we can highlight the entire row meeting a certain criterion, such as a hospital of interest. To do this we just remove the column (j) specification so the criteria apply to all columns.\n\nmy_table %&gt;% \n  bg(., i= ~ hospital == \"Military Hospital\", part = \"body\", bg = \"#91c293\") \n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nBelow we show all the code from the above sections together.\n\nborder_style = officer::fp_border(color=\"black\", width=1)\n\npacman::p_load(\n  rio,            # import/export\n  here,           # file pathways\n  flextable,      # make HTML tables \n  officer,        # helper functions for tables\n  tidyverse)      # data management, summary, and visualization\n\ntable &lt;- linelist %&gt;% \n\n  # Get summary values per hospital-outcome group\n  ###############################################\n  group_by(hospital, outcome) %&gt;%                      # Group data\n  summarise(                                           # Create new summary columns of indicators of interest\n    N = n(),                                            # Number of rows per hospital-outcome group     \n    ct_value = median(ct_blood, na.rm=T)) %&gt;%           # median CT value per group\n  \n  # add totals\n  ############\n  bind_rows(                                           # Bind the previous table with this mini-table of totals\n    linelist %&gt;% \n      filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%\n      group_by(outcome) %&gt;%                            # Grouped only by outcome, not by hospital    \n      summarise(\n        N = n(),                                       # Number of rows for whole dataset     \n        ct_value = median(ct_blood, na.rm=T))) %&gt;%     # Median CT for whole dataset\n  \n  # Pivot wider and format\n  ########################\n  mutate(hospital = replace_na(hospital, \"Total\")) %&gt;% \n  pivot_wider(                                         # Pivot from long to wide\n    values_from = c(ct_value, N),                       # new values are from ct and count columns\n    names_from = outcome) %&gt;%                           # new column names are from outcomes\n  mutate(                                              # Add new columns\n    N_Known = N_Death + N_Recover,                               # number with known outcome\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # percent who recovered (to 1 decimal)\n  select(                                              # Re-order columns\n    hospital, N_Known,                                   # Intro columns\n    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns\n    N_Death, Pct_Death, ct_value_Death)  %&gt;%             # Death columns\n  arrange(N_Known) %&gt;%                                 # Arrange rows from lowest to highest (Total row at bottom)\n\n  # formatting\n  ############\n  flextable() %&gt;%              # table is piped in from above\n  add_header_row(\n    top = TRUE,                # New header goes on top of existing header row\n    values = c(\"Hospital\",     # Header values for each column below\n               \"Total cases with known outcome\", \n               \"Recovered\",    # This will be the top-level header for this and two next columns\n               \"\",\n               \"\",\n               \"Died\",         # This will be the top-level header for this and two next columns\n               \"\",             # Leave blank, as it will be merged with \"Died\"\n               \"\")) %&gt;% \n    set_header_labels(         # Rename the columns in original header row\n      hospital = \"\", \n      N_Known = \"\",                  \n      N_Recover = \"Total\",\n      Pct_Recover = \"% of cases\",\n      ct_value_Recover = \"Median CT values\",\n      N_Death = \"Total\",\n      Pct_Death = \"% of cases\",\n      ct_value_Death = \"Median CT values\")  %&gt;% \n  merge_at(i = 1, j = 3:5, part = \"header\") %&gt;% # Horizontally merge columns 3 to 5 in new header row\n  merge_at(i = 1, j = 6:8, part = \"header\") %&gt;%  \n  border_remove() %&gt;%  \n  theme_booktabs() %&gt;% \n  vline(part = \"all\", j = 2, border = border_style) %&gt;%   # at column 2 \n  vline(part = \"all\", j = 5, border = border_style) %&gt;%   # at column 5\n  merge_at(i = 1:2, j = 1, part = \"header\") %&gt;% \n  merge_at(i = 1:2, j = 2, part = \"header\") %&gt;% \n  width(j=1, width = 2.7) %&gt;% \n  width(j=2, width = 1.5) %&gt;% \n  width(j=c(4,5,7,8), width = 1) %&gt;% \n  flextable::align(., align = \"center\", j = c(2:8), part = \"all\") %&gt;% \n  bg(., part = \"body\", bg = \"gray95\")  %&gt;% \n  bg(., j=c(1:8), i= ~ hospital == \"Military Hospital\", part = \"body\", bg = \"#91c293\") %&gt;% \n  colformat_num(., j = c(4,7), digits = 1) %&gt;%\n  bold(i = 1, bold = TRUE, part = \"header\") %&gt;% \n  bold(i = 7, bold = TRUE, part = \"body\")\n\n`summarise()` has grouped output by 'hospital'. You can override using the\n`.groups` argument.\n\ntable\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\n\nThere are different ways the table can be integrated into your output.\n\n\nYou can export the tables to Word, PowerPoint or HTML or as an image (PNG) files. To do this, use one of the following functions:\n\nsave_as_docx()\n\nsave_as_pptx()\n\nsave_as_image()\n\nsave_as_html()\n\nFor instance below we save our table as a word document. Note the syntax of the first argument - you can just provide the name of your flextable object e.g. my_table, or you can give is a “name” as shown below (the name is “my table”). If name, this will appear as the title of the table in Word. We also demonstrate code to save as PNG image.\n\n# Edit the 'my table' as needed for the title of table.  \nsave_as_docx(\"my table\" = my_table, path = \"file.docx\")\n\nsave_as_image(my_table, path = \"file.png\")\n\nNote the packages webshot or webshot2 are required to save a flextable as an image. Images may come out with transparent backgrounds.\nIf you want to view a ‘live’ version of the flextable output in the intended document format, use print() and specify one of the below to preview =. The document will “pop-up” open on your computer in the specified software program, but will not be saved. This can be useful to check if the table fits in one page/slide or so you can quickly copy it into another document, you can use the print method with the argument preview set to “pptx” or “docx”.\n\nprint(my_table, preview = \"docx\") # Word document example\nprint(my_table, preview = \"pptx\") # Powerpoint example"
  },
  {
    "objectID": "readings/formatting-tables.html#basic-flextable",
    "href": "readings/formatting-tables.html#basic-flextable",
    "title": "Tables for presentation",
    "section": "",
    "text": "To create and manage flextable objects, we first pass the data frame through the flextable() function. We save the result as my_table.\n\nmy_table &lt;- flextable(table) \nmy_table\n\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nAfter doing this, we can progressively pipe the my_table object through more flextable formatting functions.\nIn this page for sake of clarity we will save the table at intermediate steps as my_table, adding flextable functions bit-by-bit. If you want to see all the code from beginning to end written in one chunk, visit the All code together section below.\nThe general syntax of each line of flextable code is as follows:\n\nfunction(table, i = X, j = X, part = \"X\"), where:\n\nThe ‘function’ can be one of many different functions, such as width() to determine column widths, bg() to set background colours, align() to set whether text is centre/right/left aligned, and so on.\ntable = is the name of the data frame, although does not need to be stated if the data frame is piped into the function.\npart = refers to which part of the table the function is being applied to. E.g. “header”, “body” or “all”.\ni = specifies the row to apply the function to, where ‘X’ is the row number. If multiple rows, e.g. the first to third rows, one can specify: i = c(1:3). Note if ‘body’ is selected, the first row starts from underneath the header section.\nj = specifies the column to apply the function to, where ‘x’ is the column number or name. If multiple columns, e.g. the fifth and sixth, one can specify: j = c(5,6).\n\n\nYou can find the complete list of flextable formatting function here or review the documentation by entering ?flextable.\n\n\n\nWe can use the autofit() function, which nicely stretches out the table so that each cell only has one row of text. The function qflextable() is a convenient shorthand for flextable() and autofit().\n\nmy_table %&gt;% autofit()\n\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nHowever, this might not always be appropriate, especially if there are very long values within cells, meaning the table might not fit on the page.\nInstead, we can specify widths with the width() function. It can take some playing around to know what width value to put. In the example below, we specify different widths for column 1, column 2, and columns 4 to 8.\n\nmy_table &lt;- my_table %&gt;% \n  width(j=1, width = 2.7) %&gt;% \n  width(j=2, width = 1.5) %&gt;% \n  width(j=c(4,5,7,8), width = 1)\n\nmy_table\n\n\nhospitalN_KnownN_RecoverPct_Recoverct_value_RecoverN_DeathPct_Deathct_value_DeathSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nWe want more clearer headers for easier interpretation of the table contents.\nFor this table, we will want to add a second header layer so that columns covering the same subgroups can be grouped together. We do this with the add_header_row() function with top = TRUE. We provide the new name of each column to values =, leaving empty values \"\" for columns we know we will merge together later.\nWe also rename the header names in the now-second header in a separate set_header_labels() command.\nFinally, to “combine” certain column headers in the top header we use merge_at() to merge the column headers in the top header row.\n\nmy_table &lt;- my_table %&gt;% \n  \n  add_header_row(\n    top = TRUE,                # New header goes on top of existing header row\n    values = c(\"Hospital\",     # Header values for each column below\n               \"Total cases with known outcome\", \n               \"Recovered\",    # This will be the top-level header for this and two next columns\n               \"\",\n               \"\",\n               \"Died\",         # This will be the top-level header for this and two next columns\n               \"\",             # Leave blank, as it will be merged with \"Died\"\n               \"\")) %&gt;% \n    \n  set_header_labels(         # Rename the columns in original header row\n      hospital = \"\", \n      N_Known = \"\",                  \n      N_Recover = \"Total\",\n      Pct_Recover = \"% of cases\",\n      ct_value_Recover = \"Median CT values\",\n      N_Death = \"Total\",\n      Pct_Death = \"% of cases\",\n      ct_value_Death = \"Median CT values\")  %&gt;% \n  \n  merge_at(i = 1, j = 3:5, part = \"header\") %&gt;% # Horizontally merge columns 3 to 5 in new header row\n  merge_at(i = 1, j = 6:8, part = \"header\")     # Horizontally merge columns 6 to 8 in new header row\n\nmy_table  # print\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nYou can adjust the borders, internal lines, etc. with various flextable functions. It is often easier to start by removing all existing borders with border_remove().\nThen, you can apply default border themes by passing the table to theme_box(), theme_booktabs(), or theme_alafoli().\nYou can add vertical and horizontal lines with a variety of functions. hline() and vline() add lines to a specified row or column, respectively. Within each, you must specify the part = as either “all”, “body”, or “header”. For vertical lines, specify the column to j =, and for horizontal lines the row to i =. Other functions like vline_right(), vline_left(), hline_top(), and hline_bottom() add lines to the outsides only.\nIn all of these functions, the actual line style itself must be specified to border = and must be the output of a separate command using the fp_border() function from the officer package. This function helps you define the width and color of the line. You can define this above the table commands, as shown below.\n\n# define style for border line\nborder_style = officer::fp_border(color=\"black\", width=1)\n\n# add border lines to table\nmy_table &lt;- my_table %&gt;% \n\n  # Remove all existing borders\n  border_remove() %&gt;%  \n  \n  # add horizontal lines via a pre-determined theme setting\n  theme_booktabs() %&gt;% \n  \n  # add vertical lines to separate Recovered and Died sections\n  vline(part = \"all\", j = 2, border = border_style) %&gt;%   # at column 2 \n  vline(part = \"all\", j = 5, border = border_style)       # at column 5\n\nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nWe centre-align all columns aside from the left-most column with the hospital names, using the align() function from flextable.\n\nmy_table &lt;- my_table %&gt;% \n   flextable::align(align = \"center\", j = c(2:8), part = \"all\") \nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nAdditionally, we can increase the header font size and change then to bold. We can also change the total row to bold.\n\nmy_table &lt;-  my_table %&gt;%  \n  fontsize(i = 1, size = 12, part = \"header\") %&gt;%   # adjust font size of header\n  bold(i = 1, bold = TRUE, part = \"header\") %&gt;%     # adjust bold face of header\n  bold(i = 7, bold = TRUE, part = \"body\")           # adjust bold face of total row (row 7 of body)\n\nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nWe can ensure that the proportion columns display only one decimal place using the function colformat_num(). Note this could also have been done at data management stage with the round() function.\n\nmy_table &lt;- colformat_num(my_table, j = c(4,7), digits = 1)\nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nJust as we merge cells horizontally in the header row, we can also merge cells vertically using merge_at() and specifying the rows (i) and column (j). Here we merge the “Hospital” and “Total cases with known outcome” values vertically to give them more space.\n\nmy_table &lt;- my_table %&gt;% \n  merge_at(i = 1:2, j = 1, part = \"header\") %&gt;% \n  merge_at(i = 1:2, j = 2, part = \"header\")\n\nmy_table\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\n\n\n\nTo distinguish the content of the table from the headers, we may want to add additional formatting. e.g. changing the background color. In this example we change the table body to gray.\n\nmy_table &lt;- my_table %&gt;% \n    bg(part = \"body\", bg = \"gray95\")  \n\nmy_table \n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22"
  },
  {
    "objectID": "readings/formatting-tables.html#conditional-formatting",
    "href": "readings/formatting-tables.html#conditional-formatting",
    "title": "Tables for presentation",
    "section": "",
    "text": "We can highlight all values in a column that meet a certain rule, e.g. where more than 55% of cases died. Simply put the criteria to the i = or j = argument, preceded by a tilde ~. Reference the column in the data frame, not the display heading values.\n\nmy_table %&gt;% \n  bg(j = 7, i = ~ Pct_Death &gt;= 55, part = \"body\", bg = \"red\") \n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22\n\n\nOr, we can highlight the entire row meeting a certain criterion, such as a hospital of interest. To do this we just remove the column (j) specification so the criteria apply to all columns.\n\nmy_table %&gt;% \n  bg(., i= ~ hospital == \"Military Hospital\", part = \"body\", bg = \"#91c293\") \n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22"
  },
  {
    "objectID": "readings/formatting-tables.html#tbl_pres_all",
    "href": "readings/formatting-tables.html#tbl_pres_all",
    "title": "Tables for presentation",
    "section": "",
    "text": "Below we show all the code from the above sections together.\n\nborder_style = officer::fp_border(color=\"black\", width=1)\n\npacman::p_load(\n  rio,            # import/export\n  here,           # file pathways\n  flextable,      # make HTML tables \n  officer,        # helper functions for tables\n  tidyverse)      # data management, summary, and visualization\n\ntable &lt;- linelist %&gt;% \n\n  # Get summary values per hospital-outcome group\n  ###############################################\n  group_by(hospital, outcome) %&gt;%                      # Group data\n  summarise(                                           # Create new summary columns of indicators of interest\n    N = n(),                                            # Number of rows per hospital-outcome group     \n    ct_value = median(ct_blood, na.rm=T)) %&gt;%           # median CT value per group\n  \n  # add totals\n  ############\n  bind_rows(                                           # Bind the previous table with this mini-table of totals\n    linelist %&gt;% \n      filter(!is.na(outcome) & hospital != \"Missing\") %&gt;%\n      group_by(outcome) %&gt;%                            # Grouped only by outcome, not by hospital    \n      summarise(\n        N = n(),                                       # Number of rows for whole dataset     \n        ct_value = median(ct_blood, na.rm=T))) %&gt;%     # Median CT for whole dataset\n  \n  # Pivot wider and format\n  ########################\n  mutate(hospital = replace_na(hospital, \"Total\")) %&gt;% \n  pivot_wider(                                         # Pivot from long to wide\n    values_from = c(ct_value, N),                       # new values are from ct and count columns\n    names_from = outcome) %&gt;%                           # new column names are from outcomes\n  mutate(                                              # Add new columns\n    N_Known = N_Death + N_Recover,                               # number with known outcome\n    Pct_Death = scales::percent(N_Death / N_Known, 0.1),         # percent cases who died (to 1 decimal)\n    Pct_Recover = scales::percent(N_Recover / N_Known, 0.1)) %&gt;% # percent who recovered (to 1 decimal)\n  select(                                              # Re-order columns\n    hospital, N_Known,                                   # Intro columns\n    N_Recover, Pct_Recover, ct_value_Recover,            # Recovered columns\n    N_Death, Pct_Death, ct_value_Death)  %&gt;%             # Death columns\n  arrange(N_Known) %&gt;%                                 # Arrange rows from lowest to highest (Total row at bottom)\n\n  # formatting\n  ############\n  flextable() %&gt;%              # table is piped in from above\n  add_header_row(\n    top = TRUE,                # New header goes on top of existing header row\n    values = c(\"Hospital\",     # Header values for each column below\n               \"Total cases with known outcome\", \n               \"Recovered\",    # This will be the top-level header for this and two next columns\n               \"\",\n               \"\",\n               \"Died\",         # This will be the top-level header for this and two next columns\n               \"\",             # Leave blank, as it will be merged with \"Died\"\n               \"\")) %&gt;% \n    set_header_labels(         # Rename the columns in original header row\n      hospital = \"\", \n      N_Known = \"\",                  \n      N_Recover = \"Total\",\n      Pct_Recover = \"% of cases\",\n      ct_value_Recover = \"Median CT values\",\n      N_Death = \"Total\",\n      Pct_Death = \"% of cases\",\n      ct_value_Death = \"Median CT values\")  %&gt;% \n  merge_at(i = 1, j = 3:5, part = \"header\") %&gt;% # Horizontally merge columns 3 to 5 in new header row\n  merge_at(i = 1, j = 6:8, part = \"header\") %&gt;%  \n  border_remove() %&gt;%  \n  theme_booktabs() %&gt;% \n  vline(part = \"all\", j = 2, border = border_style) %&gt;%   # at column 2 \n  vline(part = \"all\", j = 5, border = border_style) %&gt;%   # at column 5\n  merge_at(i = 1:2, j = 1, part = \"header\") %&gt;% \n  merge_at(i = 1:2, j = 2, part = \"header\") %&gt;% \n  width(j=1, width = 2.7) %&gt;% \n  width(j=2, width = 1.5) %&gt;% \n  width(j=c(4,5,7,8), width = 1) %&gt;% \n  flextable::align(., align = \"center\", j = c(2:8), part = \"all\") %&gt;% \n  bg(., part = \"body\", bg = \"gray95\")  %&gt;% \n  bg(., j=c(1:8), i= ~ hospital == \"Military Hospital\", part = \"body\", bg = \"#91c293\") %&gt;% \n  colformat_num(., j = c(4,7), digits = 1) %&gt;%\n  bold(i = 1, bold = TRUE, part = \"header\") %&gt;% \n  bold(i = 7, bold = TRUE, part = \"body\")\n\n`summarise()` has grouped output by 'hospital'. You can override using the\n`.groups` argument.\n\ntable\n\n\nHospitalTotal cases with known outcomeRecoveredDiedTotal% of casesMedian CT valuesTotal% of casesMedian CT valuesSt. Mark's Maternity Hospital (SMMH)32512638.8%2219961.2%22Central Hospital35816546.1%2219353.9%22Other68529042.3%2139557.7%22Military Hospital70830943.6%2239956.4%21Missing1,12551445.7%2161154.3%21Port Hospital1,36457942.4%2178557.6%22Total3,4401,46942.7%221,97157.3%22"
  },
  {
    "objectID": "readings/formatting-tables.html#saving-your-table",
    "href": "readings/formatting-tables.html#saving-your-table",
    "title": "Tables for presentation",
    "section": "",
    "text": "There are different ways the table can be integrated into your output.\n\n\nYou can export the tables to Word, PowerPoint or HTML or as an image (PNG) files. To do this, use one of the following functions:\n\nsave_as_docx()\n\nsave_as_pptx()\n\nsave_as_image()\n\nsave_as_html()\n\nFor instance below we save our table as a word document. Note the syntax of the first argument - you can just provide the name of your flextable object e.g. my_table, or you can give is a “name” as shown below (the name is “my table”). If name, this will appear as the title of the table in Word. We also demonstrate code to save as PNG image.\n\n# Edit the 'my table' as needed for the title of table.  \nsave_as_docx(\"my table\" = my_table, path = \"file.docx\")\n\nsave_as_image(my_table, path = \"file.png\")\n\nNote the packages webshot or webshot2 are required to save a flextable as an image. Images may come out with transparent backgrounds.\nIf you want to view a ‘live’ version of the flextable output in the intended document format, use print() and specify one of the below to preview =. The document will “pop-up” open on your computer in the specified software program, but will not be saved. This can be useful to check if the table fits in one page/slide or so you can quickly copy it into another document, you can use the print method with the argument preview set to “pptx” or “docx”.\n\nprint(my_table, preview = \"docx\") # Word document example\nprint(my_table, preview = \"pptx\") # Powerpoint example"
  },
  {
    "objectID": "readings/ggplot-customize.html",
    "href": "readings/ggplot-customize.html",
    "title": "Themes and Specialized Plots",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of these materials from chapters 30 and 31.\n\n\n\n\n\n\n\n\n\n\n\nggplot2 is the most popular data visualisation R package. Its ggplot() function is at the core of this package, and this whole approach is colloquially known as “ggplot” with the resulting figures sometimes affectionately called “ggplots”. The “gg” in these names reflects the “grammar of graphics” used to construct the figures. ggplot2 benefits from a wide variety of supplementary R packages that further enhance its functionality.\nThe data visualization with ggplot cheatsheet from the RStudio website is a great reference to have on-hand when creating pltos. If you want inspiration for ways to creatively visualise your data, we suggest reviewing websites like the R graph gallery and Data-to-viz.\n\n::: {.callout-note appearance=“minimal” collapse=“true”} ## Data Preparation\n\n\nWe’ll import and clean the data identically to the previous chapter.\n\nlinelist &lt;- rio::import(\"data/case_linelists/linelist_cleaned.rds\")\n\n# make display version of columns with more friendly names\nlinelist &lt;- linelist %&gt;%\n  mutate(\n    gender_disp = case_when(gender == \"m\" ~ \"Male\",        # m to Male \n                            gender == \"f\" ~ \"Female\",      # f to Female,\n                            is.na(gender) ~ \"Unknown\"),    # NA to Unknown\n    \n    outcome_disp = replace_na(outcome, \"Unknown\")          # replace NA outcome with \"unknown\"\n  )\n\nsymptoms_data &lt;- linelist %&gt;% \n  select(c(case_id, fever, chills, cough, aches, vomit))\n\nsymptoms_data_long &lt;- symptoms_data %&gt;%    # begin with \"mini\" linelist called symptoms_data\n  \n  pivot_longer(\n    cols = -case_id,                       # pivot all columns except case_id (all the symptoms columns)\n    names_to = \"symptom_name\",             # assign name for new column that holds the symptoms\n    values_to = \"symptom_is_present\") %&gt;%  # assign name for new column that holds the values (yes/no)\n  \n  mutate(symptom_is_present = replace_na(symptom_is_present, \"unknown\")) # convert NA to \"unknown\"\n\n\n\n\nOne of the best parts of ggplot2 is the amount of control you have over the plot - you can define anything! As mentioned above, the design of the plot that is not related to the data shapes/geometries are adjusted within the theme() function. For example, the plot background color, presence/absence of gridlines, and the font/size/color/alignment of text (titles, subtitles, captions, axis text…). These adjustments can be done in one of two ways:\n\nAdd a complete theme theme_() function to make sweeping adjustments - these include theme_classic(), theme_minimal(), theme_dark(), theme_light() theme_grey(), theme_bw() among others\n\nAdjust each tiny aspect of the plot individually within theme()\n\n\n\nAs they are quite straight-forward, we will demonstrate the complete theme functions below and will not describe them further here. Note that any micro-adjustments with theme() should be made after use of a complete theme.\nWrite them with empty parentheses.\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme classic\")+\n  theme_classic()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme bw\")+\n  theme_bw()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme minimal\")+\n  theme_minimal()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme gray\")+\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe theme() function can take a large number of arguments, each of which edits a very specific aspect of the plot. There is no way we could cover all of the arguments, but we will describe the general pattern for them and show you how to find the argument name that you need. The basic syntax is this:\n\nWithin theme() write the argument name for the plot element you want to edit, like plot.title =\n\nProvide an element_() function to the argument\n\n\n\nMost often, use element_text(), but others include element_rect() for canvas background colors, or element_blank() to remove plot elements\n\n\n\nWithin the element_() function, write argument assignments to make the fine adjustments you desire\n\nSo, that description was quite abstract, so here are some examples.\nThe below plot looks quite silly, but it serves to show you a variety of the ways you can adjust your plot.\n\nWe begin with the plot age_by_wt defined just above and add theme_classic()\n\nFor finer adjustments we add theme() and include one argument for each plot element to adjust\n\nIt can be nice to organize the arguments in logical sections. To describe just some of those used below:\n\nlegend.position = is unique in that it accepts simple values like “bottom”, “top”, “left”, and “right”. But generally, text-related arguments require that you place the details within element_text().\n\nTitle size with element_text(size = 30)\n\nThe caption horizontal alignment with element_text(hjust = 0) (from right to left)\n\nThe subtitle is italicized with element_text(face = \"italic\")\n\n\nage_by_wt + \n  theme_classic()+                                 # pre-defined theme adjustments\n  theme(\n    legend.position = \"bottom\",                    # move legend to bottom\n    \n    plot.title = element_text(size = 30),          # size of title to 30\n    plot.caption = element_text(hjust = 0),        # left-align caption\n    plot.subtitle = element_text(face = \"italic\"), # italicize subtitle\n    \n    axis.text.x = element_text(color = \"red\", size = 15, angle = 90), # adjusts only x-axis text\n    axis.text.y = element_text(size = 15),         # adjusts only y-axis text\n    \n    axis.title = element_text(size = 20)           # adjusts both axes titles\n    )     \n\n\n\n\nHere are some especially common theme() arguments. You will recognize some patterns, such as appending .x or .y to apply the change only to one axis.\n\n\n\n\n\n\n\ntheme() argument\nWhat it adjusts\n\n\n\n\nplot.title = element_text()\nThe title\n\n\nplot.subtitle = element_text()\nThe subtitle\n\n\nplot.caption = element_text()\nThe caption (family, face, color, size, angle, vjust, hjust…)\n\n\naxis.title = element_text()\nAxis titles (both x and y) (size, face, angle, color…)\n\n\naxis.title.x = element_text()\nAxis title x-axis only (use .y for y-axis only)\n\n\naxis.text = element_text()\nAxis text (both x and y)\n\n\naxis.text.x = element_text()\nAxis text x-axis only (use .y for y-axis only)\n\n\naxis.ticks = element_blank()\nRemove axis ticks\n\n\naxis.line = element_line()\nAxis lines (colour, size, linetype: solid dashed dotted etc)\n\n\nstrip.text = element_text()\nFacet strip text (colour, face, size, angle…)\n\n\nstrip.background = element_rect()\nfacet strip (fill, colour, size…)\n\n\n\nBut there are so many theme arguments! How could I remember them all? Do not worry - it is impossible to remember them all. Luckily there are a few tools to help you:\nThe tidyverse documentation on modifying theme, which has a complete list.\n\n\n\n\n\n\nTip\n\n\n\nRun theme_get() from ggplot2 to print a list of all 90+ theme() arguments to the console.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you ever want to remove an element of a plot, you can also do it through theme(). Just pass element_blank() to an argument to have it disappear completely. For legends, set legend.position = \"none\".\n\n\n\n\n\n\nIn ggplot2, when aesthetics of plotted data (e.g. size, color, shape, fill, plot axis) are mapped to columns in the data, the exact display can be adjusted with the corresponding “scale” command. In this section we explain some common scale adjustments.\n\n\nOne thing that can initially be difficult to understand with ggplot2 is control of color schemes. Note that this section discusses the color of plot objects (geoms/shapes) such as points, bars, lines, tiles, etc.\nTo control “color” of plot objects you will be adjusting either the color = aesthetic (the exterior color) or the fill = aesthetic (the interior color). One exception to this pattern is geom_point(), where you really only get to control color =, which controls the color of the point (interior and exterior).\nWhen setting colour or fill you can use colour names recognized by R like \"red\" (see complete list or enter ?colors), or a specific hex colour such as \"#ff0505\".\n\n# histogram - \nggplot(data = linelist, mapping = aes(x = age))+       # set data and axes\n  geom_histogram(              # display histogram\n    binwidth = 7,                # width of bins\n    color = \"red\",               # bin line color\n    fill = \"lightblue\")          # bin interior color (fill) \n\n\n\n\nAesthetics such as fill = and color = can be defined either outside of a mapping = aes() statement or inside of one. If outside the aes(), the assigned value should be static (e.g. color = \"blue\") and will apply for all data plotted by the geom. If inside, the aesthetic should be mapped to a column, like color = hospital, and the expression will vary by the value for that row in the data. A few examples:\n\n# Static color for points and for line\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(color = \"purple\")+\n  geom_vline(xintercept = 50, color = \"orange\")+\n  labs(title = \"Static color for points and line\")\n\n# Color mapped to continuous column\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(mapping = aes(color = temp))+         \n  labs(title = \"Color mapped to continuous column\")\n\n# Color mapped to discrete column\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(mapping = aes(color = gender))+         \n  labs(title = \"Color mapped to discrete column\")\n\n# bar plot, fill to discrete column, color to static value\nggplot(data = linelist, mapping = aes(x = hospital))+     \n  geom_bar(mapping = aes(fill = gender), color = \"yellow\")+         \n  labs(title = \"Fill mapped to discrete column, static color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce you map a column to a plot aesthetic (e.g. x =, y =, fill =, color =…), your plot will gain a scale/legend. See above how the scale can be continuous, discrete, date, etc. values depending on the class of the assigned column. If you have multiple aesthetics mapped to columns, your plot will have multiple scales.\nYou can control the scales with the appropriate scales_() function. The scale functions of ggplot() have 3 parts that are written like this: scale_AESTHETIC_METHOD().\n\nThe first part, scale_(), is fixed.\n\nThe second part, the AESTHETIC, should be the aesthetic that you want to adjust the scale for (_fill_, _shape_, _color_, _size_, _alpha_…) - the options here also include _x_ and _y_.\n\nThe third part, the METHOD, will be either _discrete(), continuous(), _date(), _gradient(), or _manual() depending on the class of the column and how you want to control it. There are others, but these are the most-often used.\n\nBe sure that you use the correct function for the scale! Otherwise your scale command will not appear to change anything. If you have multiple scales, you may use multiple scale functions to adjust them! For example:\n\n\n\nEach kind of scale has its own arguments, though there is some overlap. Query the function like ?scale_color_discrete in the R console to see the function argument documentation.\nFor continuous scales, use breaks = to provide a sequence of values with seq() (take to =, from =, and by = as shown in the example below. Set expand = c(0,0) to eliminate padding space around the axes (this can be used on any _x_ or _y_ scale.\nFor discrete scales, you can adjust the order of level appearance with breaks =, and how the values display with the labels = argument. Provide a character vector to each of those (see example below). You can also drop NA easily by setting na.translate = FALSE.\n\n\n\nOne of the most useful tricks is using “manual” scaling functions to explicitly assign colors as you desire. These are functions with the syntax scale_xxx_manual() (e.g. scale_colour_manual() or scale_fill_manual()). Each of the below arguments are demonstrated in the code example below.\n\nAssign colors to data values with the values = argument\n\nSpecify a color for NA with na.value =\n\nChange how the values are written in the legend with the labels = argument\n\nChange the legend title with name =\n\nBelow, we create a bar plot and show how it appears by default, and then with three scales adjusted - the continuous y-axis scale, the discrete x-axis scale, and manual adjustment of the fill (interior bar color).\n\n# BASELINE - no scale adjustment\nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  labs(title = \"Baseline - no scale adjustments\")\n\n\n\n# SCALES ADJUSTED\nggplot(data = linelist)+\n  \n  geom_bar(mapping = aes(x = outcome, fill = gender), color = \"black\")+\n  \n  theme_minimal()+                   # simplify background\n  \n  scale_y_continuous(                # continuous scale for y-axis (counts)\n    expand = c(0,0),                 # no padding\n    breaks = seq(from = 0,\n                 to = 3000,\n                 by = 500))+\n  \n  scale_x_discrete(                   # discrete scale for x-axis (gender)\n    expand = c(0,0),                  # no padding\n    drop = FALSE,                     # show all factor levels (even if not in data)\n    na.translate = FALSE,             # remove NA outcomes from plot\n    labels = c(\"Died\", \"Recovered\"))+ # Change display of values\n    \n  \n  scale_fill_manual(                  # Manually specify fill (bar interior color)\n    values = c(\"m\" = \"violetred\",     # reference values in data to assign colors\n               \"f\" = \"aquamarine\"),\n    labels = c(\"m\" = \"Male\",          # re-label the legend (use \"=\" assignment to avoid mistakes)\n              \"f\" = \"Female\",\n              \"Missing\"),\n    name = \"Gender\",                  # title of legend\n    na.value = \"grey\"                 # assign a color for missing values\n  )+\n  labs(title = \"Adjustment of scales\") # Adjust the title of the fill legend\n\n\n\n\n\n\n\nWhen data are mapping to the plot axes, these too can be adjusted with scales commands. A common example is adjusting the display of an axis (e.g. y-axis) that is mapped to a column with continuous data.\nWe may want to adjust the breaks or display of the values in the ggplot using scale_y_continuous(). As noted above, use the argument breaks = to provide a sequence of values that will serve as “breaks” along the scale. These are the values at which numbers will display. To this argument, you can provide a c() vector containing the desired break values, or you can provide a regular sequence of numbers using the base R function seq(). This seq() function accepts to =, from =, and by =.\n\n# BASELINE - no scale adjustment\nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  labs(title = \"Baseline - no scale adjustments\")\n\n# \nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  scale_y_continuous(\n    breaks = seq(\n      from = 0,\n      to = 3000,\n      by = 100)\n  )+\n  labs(title = \"Adjusted y-axis breaks\")\n\n\n\n\n\n\n\n\n\nIf your original data values are proportions, you can easily display them as percents with “%” by providing labels = scales::percent in your scales command, as shown below.\nWhile an alternative would be to convert the values to character and add a “%” character to the end, this approach will cause complications because your data will no longer be continuous numeric values.\n\n# Original y-axis proportions\n#############################\nlinelist %&gt;%                                   # start with linelist\n  group_by(hospital) %&gt;%                       # group data by hospital\n  summarise(                                   # create summary columns\n    n = n(),                                     # total number of rows in group\n    deaths = sum(outcome == \"Death\", na.rm=T),   # number of deaths in group\n    prop_death = deaths/n) %&gt;%                   # proportion deaths in group\n  ggplot(                                      # begin plotting\n    mapping = aes(\n      x = hospital,\n      y = prop_death))+ \n  geom_col()+\n  theme_minimal()+\n  labs(title = \"Display y-axis original proportions\")\n\n\n\n# Display y-axis proportions as percents\n########################################\nlinelist %&gt;%         \n  group_by(hospital) %&gt;% \n  summarise(\n    n = n(),\n    deaths = sum(outcome == \"Death\", na.rm=T),\n    prop_death = deaths/n) %&gt;% \n  ggplot(\n    mapping = aes(\n      x = hospital,\n      y = prop_death))+\n  geom_col()+\n  theme_minimal()+\n  labs(title = \"Display y-axis as percents (%)\")+\n  scale_y_continuous(\n    labels = scales::percent                    # display proportions as percents\n  )\n\n\n\n\n\n\n\n\n\n\nTo transform a continuous axis to log scale, add trans = \"log2\" to the scale command. For purposes of example, we create a data frame of regions with their respective preparedness_index and cumulative cases values.\n\nplot_data &lt;- data.frame(\n  region = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"),\n  preparedness_index = c(8.8, 7.5, 3.4, 3.6, 2.1, 7.9, 7.0, 5.6, 1.0),\n  cases_cumulative = c(15, 45, 80, 20, 21, 7, 51, 30, 1442)\n)\n\nplot_data\n\n  region preparedness_index cases_cumulative\n1      A                8.8               15\n2      B                7.5               45\n3      C                3.4               80\n4      D                3.6               20\n5      E                2.1               21\n6      F                7.9                7\n7      G                7.0               51\n8      H                5.6               30\n9      I                1.0             1442\n\n\nThe cumulative cases for region “I” are dramatically greater than all the other regions. In circumstances like this, you may elect to display the y-axis using a log scale so the reader can see differences between the regions with fewer cumulative cases.\n\n# Original y-axis\npreparedness_plot &lt;- ggplot(data = plot_data,  \n       mapping = aes(\n         x = preparedness_index,\n         y = cases_cumulative))+\n  geom_point(size = 2)+            # points for each region \n  geom_text(\n    mapping = aes(label = region),\n    vjust = 1.5)+                  # add text labels\n  theme_minimal()\n\npreparedness_plot                  # print original plot\n\n\n# print with y-axis transformed\npreparedness_plot+                   # begin with plot saved above\n  scale_y_continuous(trans = \"log2\") # add transformation for y-axis\n\n\n\n\n\n\n\n\n\n\n\nFill gradient scales can involve additional nuance. The defaults are usually quite pleasing, but you may want to adjust the values, cutoffs, etc.\nTo demonstrate how to adjust a continuous color scale, we’ll use a data set that contains the ages of cases and of sources in a contact tracing case.\n\ncase_source_relationships &lt;- rio::import(\"data/godata/relationships_clean.rds\") %&gt;% \n  select(source_age, target_age) \n\nBelow, we produce a “raster” heat tile density plot. Note how the fill scale is continuous.\n\ntrans_matrix &lt;- ggplot(\n    data = case_source_relationships,\n    mapping = aes(x = source_age, y = target_age))+\n  stat_density2d(\n    geom = \"raster\",\n    mapping = aes(fill = after_stat(density)),\n    contour = FALSE)+\n  theme_minimal()\n\nNow we show some variations on the fill scale:\n\ntrans_matrix\ntrans_matrix + scale_fill_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\nNow we show some examples of actually adjusting the break points of the scale:\n\nscale_fill_gradient() accepts two colors (high/low)\n\nscale_fill_gradientn() accepts a vector of any length of colors to values = (intermediate values will be interpolated)\n\nUse scales::rescale() to adjust how colors are positioned along the gradient; it rescales your vector of positions to be between 0 and 1.\n\n\ntrans_matrix + \n  scale_fill_gradient(     # 2-sided gradient scale\n    low = \"aquamarine\",    # low value\n    high = \"purple\",       # high value\n    na.value = \"grey\",     # value for NA\n    name = \"Density\")+     # Legend title\n  labs(title = \"Manually specify high/low colors\")\n\n# 3+ colors to scale\ntrans_matrix + \n  scale_fill_gradientn(    # 3-color scale (low/mid/high)\n    colors = c(\"blue\", \"yellow\",\"red\") # provide colors in vector\n  )+\n  labs(title = \"3-color scale\")\n\n# Use of rescale() to adjust placement of colors along scale\ntrans_matrix + \n  scale_fill_gradientn(    # provide any number of colors\n    colors = c(\"blue\", \"yellow\",\"red\", \"black\"),\n    values = scales::rescale(c(0, 0.05, 0.07, 0.10, 0.15, 0.20, 0.3, 0.5)) # positions for colors are rescaled between 0 and 1\n    )+\n  labs(title = \"Colors not evenly positioned\")\n\n# use of limits to cut-off values that get fill color\ntrans_matrix + \n  scale_fill_gradientn(    \n    colors = c(\"blue\", \"yellow\",\"red\"),\n    limits = c(0, 0.0002))+\n  labs(title = \"Restrict value limits, resulting in grey space\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore generally, if you want predefined palettes, you can use the scale_xxx_brewer or scale_xxx_viridis_y functions.\nThe ‘brewer’ functions can draw from colorbrewer.org palettes.\nThe ‘viridis’ functions draw from viridis (colourblind friendly!) palettes, which “provide colour maps that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness.” (read more here and here). Define if the palette is discrete, continuous, or binned by specifying this at the end of the function (e.g. discrete is scale_xxx_viridis_d).\nIt is advised that you test your plot in this color blindness simulator. If you have a red/green color scheme, try a “hot-cold” (red-blue) scheme instead as described here\nHere is an example of using various color schemes.\n\nsymp_plot &lt;- linelist %&gt;%                                         # begin with linelist\n  select(c(case_id, fever, chills, cough, aches, vomit)) %&gt;%     # select columns\n  pivot_longer(                                                  # pivot longer\n    cols = -case_id,                                  \n    names_to = \"symptom_name\",\n    values_to = \"symptom_is_present\") %&gt;%\n  mutate(                                                        # replace missing values\n    symptom_is_present = replace_na(symptom_is_present, \"unknown\")) %&gt;% \n  ggplot(                                                        # begin ggplot!\n    mapping = aes(x = symptom_name, fill = symptom_is_present))+\n  geom_bar(position = \"fill\", col = \"black\") +                    \n  theme_classic() +\n  theme(legend.position = \"bottom\")+\n  labs(\n    x = \"Symptom\",\n    y = \"Symptom status (proportion)\"\n  )\n\nsymp_plot  # print with default colors\n\n#################################\n# print with manually-specified colors\nsymp_plot +\n  scale_fill_manual(\n    values = c(\"yes\" = \"black\",         # explicitly define colours\n               \"no\" = \"white\",\n               \"unknown\" = \"grey\"),\n    breaks = c(\"yes\", \"no\", \"unknown\"), # order the factors correctly\n    name = \"\"                           # set legend to no title\n\n  ) \n\n#################################\n# print with viridis discrete colors\nsymp_plot +\n  scale_fill_viridis_d(\n    breaks = c(\"yes\", \"no\", \"unknown\"),\n    name = \"\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChanging the order that discrete variables appear in is often difficult to understand for people who are new to ggplot2 graphs. It’s easy to understand how to do this however once you understand how ggplot2 handles discrete variables under the hood. Generally speaking, if a discrete varaible is used, it is automatically converted to a factor type - which orders factors by alphabetical order by default. To handle this, you simply have to reorder the factor levels to reflect the order you would like them to appear in the chart. For more detailed information on how to reorder factor objects, see the factor section of the guide.\nWe can look at a common example using age groups - by default the 5-9 age group will be placed in the middle of the age groups (given alphanumeric order), but we can move it behind the 0-4 age group of the chart by releveling the factors.\n\nggplot(\n  data = linelist %&gt;% drop_na(age_cat5),                         # remove rows where age_cat5 is missing\n  mapping = aes(x = fct_relevel(age_cat5, \"5-9\", after = 1))) +  # relevel factor\n\n  geom_bar() +\n  \n  labs(x = \"Age group\", y = \"Number of hospitalisations\",\n       title = \"Total hospitalisations by age group\") +\n  \n  theme_minimal()\n\n\n\n\n\n\n\nThese are a collection of less common plot types, ggplot2 extensions, and advanced examples of some of the things you can do visualizing data in R.\n\n\nContour plots are helpful when you have many points that might cover each other (“overplotting”). The case-source data used above are again plotted, but more simply using stat_density2d() and stat_density2d_filled() to produce discrete contour levels - like a topographical map. Read more about the statistics here.\n\ncase_source_relationships %&gt;% \n  ggplot(aes(x = source_age, y = target_age))+\n  stat_density2d()+\n  geom_point()+\n  theme_minimal()+\n  labs(title = \"stat_density2d() + geom_point()\")\n\n\ncase_source_relationships %&gt;% \n  ggplot(aes(x = source_age, y = target_age))+\n  stat_density2d_filled()+\n  theme_minimal()+\n  labs(title = \"stat_density2d_filled()\")\n\n\n\n\n\n\n\n\n\n\nTo show the distributions on the edges of a geom_point() scatterplot, you can use the ggExtra package and its function ggMarginal(). Save your original ggplot as an object, then pass it to ggMarginal() as shown below. Here are the key arguments:\n\nYou must specify the type = as either “histogram”, “density” “boxplot”, “violin”, or “densigram”.\n\nBy default, marginal plots will appear for both axes. You can set margins = to “x” or “y” if you only want one.\n\nOther optional arguments include fill = (bar color), color = (line color), size = (plot size relative to margin size, so larger number makes the marginal plot smaller).\n\nYou can provide other axis-specific arguments to xparams = and yparams =. For example, to have different histogram bin sizes, as shown below.\n\nYou can have the marginal plots reflect groups (columns that have been assigned to color = in your ggplot() mapped aesthetics). If this is the case, set the ggMarginal() argument groupColour = or groupFill = to TRUE, as shown below.\nRead more at this vignette, in the R Graph Gallery or the function R documentation ?ggMarginal.\n\n# Install/load ggExtra\npacman::p_load(ggExtra)\n\n# Basic scatter plot of weight and age\nscatter_plot &lt;- ggplot(data = linelist)+\n  geom_point(mapping = aes(y = wt_kg, x = age)) +\n  labs(title = \"Scatter plot of weight and age\")\n\nTo add marginal histograms use type = \"histogram\". You can optionally set groupFill = TRUE to get stacked histograms.\n\n# with histograms\nggMarginal(\n  scatter_plot,                     # add marginal histograms\n  type = \"histogram\",               # specify histograms\n  fill = \"lightblue\",               # bar fill\n  xparams = list(binwidth = 10),    # other parameters for x-axis marginal\n  yparams = list(binwidth = 5))     # other parameters for y-axis marginal\n\n\n\n\nMarginal density plot with grouped/colored values:\n\n# Scatter plot, colored by outcome\n# Outcome column is assigned as color in ggplot. groupFill in ggMarginal set to TRUE\nscatter_plot_color &lt;- ggplot(data = linelist %&gt;% drop_na(gender))+\n  geom_point(mapping = aes(y = wt_kg, x = age, color = gender)) +\n  labs(title = \"Scatter plot of weight and age\")+\n  theme(legend.position = \"bottom\")\n\nggMarginal(scatter_plot_color, type = \"density\", groupFill = TRUE)\n\n\n\n\nSet the size = arguemnt to adjust the relative size of the marginal plot. Smaller number makes a larger marginal plot. You also set color =. Below are is a marginal boxplot, with demonstration of the margins = argument so it appears on only one axis:\n\n# with boxplot \nggMarginal(\n  scatter_plot,\n  margins = \"x\",      # only show x-axis marginal plot\n  type = \"boxplot\")   \n\n\n\n\n\n\n\n\nIn ggplot2, it is also possible to add text to plots. However, this comes with the notable limitation where text labels often clash with data points in a plot, making them look messy or hard to read. There is no ideal way to deal with this in the base package, but there is a ggplot2 add-on, known as ggrepel that makes dealing with this very simple!\nThe ggrepel package provides two new functions, geom_label_repel() and geom_text_repel(), which replace geom_label() and geom_text(). Simply use these functions instead of the base functions to produce neat labels. Within the function, map the aesthetics aes() as always, but include the argument label = to which you provide a column name containing the values you want to display (e.g. patient id, or name, etc.). You can make more complex labels by combining columns and newlines (\\n) within str_glue() as shown below.\nA few tips:\n\nUse min.segment.length = 0 to always draw line segments, or min.segment.length = Inf to never draw them\n\nUse size = outside of aes() to set text size\n\nUse force = to change the degree of repulsion between labels and their respective points (default is 1)\n\nInclude fill = within aes() to have label colored by value\n\nA letter “a” may appear in the legend - add guides(fill = guide_legend(override.aes = aes(color = NA)))+ to remove it\n\n\nSee this is very in-depth tutorial for more.\n\npacman::p_load(ggrepel)\n\nlinelist %&gt;%                                               # start with linelist\n  group_by(hospital) %&gt;%                                   # group by hospital\n  summarise(                                               # create new dataset with summary values per hospital\n    n_cases = n(),                                           # number of cases per hospital\n    delay_mean = round(mean(days_onset_hosp, na.rm=T),1),    # mean delay per hospital\n  ) %&gt;% \n  ggplot(mapping = aes(x = n_cases, y = delay_mean))+      # send data frame to ggplot\n  geom_point(size = 2)+                                    # add points\n  geom_label_repel(                                        # add point labels\n    mapping = aes(\n      label = stringr::str_glue(\n        \"{hospital}\\n{n_cases} cases, {delay_mean} days\")  # how label displays\n      ), \n    size = 3,                                              # text size in labels\n    min.segment.length = 0)+                               # show all line segments                \n  labs(                                                    # add axes labels\n    title = \"Mean delay to admission, by hospital\",\n    x = \"Number of cases\",\n    y = \"Mean delay (days)\")\n\n\n\n\nYou can label only a subset of the data points - by using standard ggplot() syntax to provide different data = for each geom layer of the plot. Below, All cases are plotted, but only a few are labeled.\n\nggplot()+\n  # All points in grey\n  geom_point(\n    data = linelist,                                   # all data provided to this layer\n    mapping = aes(x = ht_cm, y = wt_kg),\n    color = \"grey\",\n    alpha = 0.5)+                                              # grey and semi-transparent\n  \n  # Few points in black\n  geom_point(\n    data = linelist %&gt;% filter(days_onset_hosp &gt; 15),  # filtered data provided to this layer\n    mapping = aes(x = ht_cm, y = wt_kg),\n    alpha = 1)+                                                # default black and not transparent\n  \n  # point labels for few points\n  geom_label_repel(\n    data = linelist %&gt;% filter(days_onset_hosp &gt; 15),  # filter the data for the labels\n    mapping = aes(\n      x = ht_cm,\n      y = wt_kg,\n      fill = outcome,                                          # label color by outcome\n      label = stringr::str_glue(\"Delay: {days_onset_hosp}d\")), # label created with str_glue()\n    min.segment.length = 0) +                                  # show line segments for all\n  \n  # remove letter \"a\" from inside legend boxes\n  guides(fill = guide_legend(override.aes = aes(color = NA)))+\n  \n  # axis labels\n  labs(\n    title = \"Cases with long delay to admission\",\n    y = \"weight (kg)\",\n    x = \"height(cm)\")\n\n\n\n\n\n\n\n\nHighlighting specific elements in a chart is a useful way to draw attention to a specific instance of a variable while also providing information on the dispersion of the full dataset. While this is not easily done in base ggplot2, there is an external package that can help to do this known as gghighlight. This is easy to use within the ggplot syntax.\nThe gghighlight package uses the gghighlight() function to achieve this effect. To use this function, supply a logical statement to the function - this can have quite flexible outcomes, but here we’ll show an example of the age distribution of cases in our linelist, highlighting them by outcome.\n\n# load gghighlight\nlibrary(gghighlight)\n\n# replace NA values with unknown in the outcome variable\nlinelist &lt;- linelist %&gt;%\n  mutate(outcome = replace_na(outcome, \"Unknown\"))\n\n# produce a histogram of all cases by age\nggplot(\n  data = linelist,\n  mapping = aes(x = age_years, fill = outcome)) +\n  geom_histogram() + \n  gghighlight::gghighlight(outcome == \"Death\")     # highlight instances where the patient has died.\n\n\n\n\nThis also works well with faceting functions - it allows the user to produce facet plots with the background data highlighted that doesn’t apply to the facet! Below we count cases by week and plot the epidemic curves by hospital (color = and facet_wrap() set to hospital column).\n\n# produce a histogram of all cases by age\nlinelist %&gt;% \n  count(week = lubridate::floor_date(date_hospitalisation, \"week\"),\n        hospital) %&gt;% \n  ggplot()+\n  geom_line(aes(x = week, y = n, color = hospital))+\n  theme_minimal()+\n  gghighlight::gghighlight() +                      # highlight instances where the patient has died\n  facet_wrap(~hospital)                              # make facets by outcome\n\n\n\n\n\n\n\nNote that properly aligning axes to plot from multiple datasets in the same plot can be difficult. Consider one of the following strategies:\n\nMerge the data prior to plotting, and convert to “long” format with a column reflecting the dataset\n\nUse cowplot or a similar package to combine two plots (see below)\n\n\n\n\n\nTwo packages that are very useful for combining plots are cowplot and patchwork. In this page we will mostly focus on cowplot, with occassional use of patchwork.\nHere is the online introduction to cowplot. You can read the more extensive documentation for each function online here. We will cover a few of the most common use cases and functions below.\nThe cowplot package works in tandem with ggplot2 - essentially, you use it to arrange and combine ggplots and their legends into compound figures. It can also accept base R graphics.\n\npacman::p_load(\n  tidyverse,      # data manipulation and visualisation\n  cowplot,        # combine plots\n  patchwork       # combine plots\n)\n\nWhile faceting is a convenient approach to plotting, sometimes its not possible to get the results you want from its relatively restrictive approach. Here, you may choose to combine plots by sticking them together into a larger plot. There are three well known packages that are great for this - cowplot, gridExtra, and patchwork. However, these packages largely do the same things, so we’ll focus on cowplot for this section.\n\n\nThe cowplot package has a fairly wide range of functions, but the easiest use of it can be achieved through the use of plot_grid(). This is effectively a way to arrange predefined plots in a grid formation. We can work through another example with the malaria dataset - here we can plot the total cases by district, and also show the epidemic curve over time.\n\nmalaria_data &lt;- rio::import(\"data/malaria_facility_count_data.rds\")\n\n# bar chart of total cases by district\np1 &lt;- ggplot(malaria_data, aes(x = District, y = malaria_tot)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    x = \"District\",\n    y = \"Total number of cases\",\n    title = \"Total malaria cases by district\"\n  ) +\n  theme_minimal()\n\n# epidemic curve over time\np2 &lt;- ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1) +\n  labs(\n    x = \"Date of data submission\",\n    y =  \"number of cases\"\n  ) +\n  theme_minimal()\n\ncowplot::plot_grid(p1, p2,\n                  # 1 column and two rows - stacked on top of each other\n                   ncol = 1,\n                   nrow = 2,\n                   # top plot is 2/3 as tall as second\n                   rel_heights = c(2, 3))\n\n\n\n\n\n\n\nIf your plots have the same legend, combining them is relatively straight-forward. Simple use the cowplot approach above to combine the plots, but remove the legend from one of them (de-duplicate).\nIf your plots have different legends, you must use an alternative approach:\n\nCreate and save your plots without legends using theme(legend.position = \"none\")\n\nExtract the legends from each plot using get_legend() as shown below - but extract legends from the plots modified to actually show the legend\n\nCombine the legends into a legends panel\n\nCombine the plots and legends panel\n\nFor demonstration we show the two plots separately, and then arranged in a grid with their own legends showing (ugly and inefficient use of space):\n\np1 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n  scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Cases by outcome\")\n\n\np2 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, age_cat) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(axis.text.y = element_blank())+\n  labs(title = \"Cases by age\")\n\nHere is how the two plots look when combined using plot_grid() without combining their legends:\n\ncowplot::plot_grid(p1, p2, rel_widths = c(0.3))\n\n\n\n\nAnd now we show how to combine the legends. Essentially what we do is to define each plot without its legend (theme(legend.position = \"none\"), and then we define each plot’s legend separately, using the get_legend() function from cowplot. When we extract the legend from the saved plot, we need to add + the legend back in, including specifying the placement (“right”) and smaller adjustments for alignment of the legends and their titles. Then, we combine the legends together vertically, and then combine the two plots with the newly-combined legends. Voila!\n\n# Define plot 1 without legend\np1 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n  scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  labs(title = \"Cases by outcome\")\n\n\n# Define plot 2 without legend\np2 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, age_cat) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(\n    legend.position = \"none\",\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank()\n  )+\n  labs(title = \"Cases by age\")\n\n\n# extract legend from p1 (from p1 + legend)\nleg_p1 &lt;- cowplot::get_legend(p1 +\n                                theme(legend.position = \"right\",        # extract vertical legend\n                                      legend.justification = c(0,0.5))+ # so legends  align\n                                labs(fill = \"Outcome\"))                 # title of legend\n# extract legend from p2 (from p2 + legend)\nleg_p2 &lt;- cowplot::get_legend(p2 + \n                                theme(legend.position = \"right\",         # extract vertical legend   \n                                      legend.justification = c(0,0.5))+  # so legends align\n                                labs(fill = \"Age Category\"))             # title of legend\n\n# create a blank plot for legend alignment\n#blank_p &lt;- patchwork::plot_spacer() + theme_void()\n\n# create legends panel, can be one on top of the other (or use spacer commented above)\nlegends &lt;- cowplot::plot_grid(leg_p1, leg_p2, nrow = 2, rel_heights = c(.3, .7))\n\n# combine two plots and the combined legends panel\ncombined &lt;- cowplot::plot_grid(p1, p2, legends, ncol = 3, rel_widths = c(.4, .4, .2))\n\ncombined  # print\n\n\n\n\nThis solution was learned from this post with a minor fix to align legends from this post.\nTIP: Fun note - the “cow” in cowplot comes from the creator’s name - Claus O. Wilke.\n\n\n\nYou can inset one plot in another using cowplot. Here are things to be aware of:\n\nDefine the main plot with theme_half_open() from cowplot; it may be best to have the legend either on top or bottom\n\nDefine the inset plot. Best is to have a plot where you do not need a legend. You can remove plot theme elements with element_blank() as shown below.\n\nCombine them by applying ggdraw() to the main plot, then adding draw_plot() on the inset plot and specifying the coordinates (x and y of lower left corner), height and width as proportion of the whole main plot.\n\n\n# Define main plot\nmain_plot &lt;- ggplot(data = linelist)+\n  geom_histogram(aes(x = date_onset, fill = hospital))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+ \n  theme_half_open()+\n  theme(legend.position = \"bottom\")+\n  labs(title = \"Epidemic curve and outcomes by hospital\")\n\n\n# Define inset plot\ninset_plot &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n    geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n    scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n    coord_flip()+\n    theme_minimal()+\n    theme(legend.position = \"none\",\n          axis.title.y = element_blank())+\n    labs(title = \"Cases by outcome\") \n\n\n# Combine main with inset\ncowplot::ggdraw(main_plot)+\n     draw_plot(inset_plot,\n               x = .6, y = .55,    #x = .07, y = .65,\n               width = .4, height = .4)\n\n\n\n\nThis technique is explained more in these two vignettes:\nWilke lab\ndraw_plot() documentation"
  },
  {
    "objectID": "readings/ggplot-customize.html#ggplot_basics_themes",
    "href": "readings/ggplot-customize.html#ggplot_basics_themes",
    "title": "Themes and Specialized Plots",
    "section": "",
    "text": "One of the best parts of ggplot2 is the amount of control you have over the plot - you can define anything! As mentioned above, the design of the plot that is not related to the data shapes/geometries are adjusted within the theme() function. For example, the plot background color, presence/absence of gridlines, and the font/size/color/alignment of text (titles, subtitles, captions, axis text…). These adjustments can be done in one of two ways:\n\nAdd a complete theme theme_() function to make sweeping adjustments - these include theme_classic(), theme_minimal(), theme_dark(), theme_light() theme_grey(), theme_bw() among others\n\nAdjust each tiny aspect of the plot individually within theme()\n\n\n\nAs they are quite straight-forward, we will demonstrate the complete theme functions below and will not describe them further here. Note that any micro-adjustments with theme() should be made after use of a complete theme.\nWrite them with empty parentheses.\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme classic\")+\n  theme_classic()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme bw\")+\n  theme_bw()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme minimal\")+\n  theme_minimal()\n\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+  \n  geom_point(color = \"darkgreen\", size = 0.5, alpha = 0.2)+\n  labs(title = \"Theme gray\")+\n  theme_gray()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe theme() function can take a large number of arguments, each of which edits a very specific aspect of the plot. There is no way we could cover all of the arguments, but we will describe the general pattern for them and show you how to find the argument name that you need. The basic syntax is this:\n\nWithin theme() write the argument name for the plot element you want to edit, like plot.title =\n\nProvide an element_() function to the argument\n\n\n\nMost often, use element_text(), but others include element_rect() for canvas background colors, or element_blank() to remove plot elements\n\n\n\nWithin the element_() function, write argument assignments to make the fine adjustments you desire\n\nSo, that description was quite abstract, so here are some examples.\nThe below plot looks quite silly, but it serves to show you a variety of the ways you can adjust your plot.\n\nWe begin with the plot age_by_wt defined just above and add theme_classic()\n\nFor finer adjustments we add theme() and include one argument for each plot element to adjust\n\nIt can be nice to organize the arguments in logical sections. To describe just some of those used below:\n\nlegend.position = is unique in that it accepts simple values like “bottom”, “top”, “left”, and “right”. But generally, text-related arguments require that you place the details within element_text().\n\nTitle size with element_text(size = 30)\n\nThe caption horizontal alignment with element_text(hjust = 0) (from right to left)\n\nThe subtitle is italicized with element_text(face = \"italic\")\n\n\nage_by_wt + \n  theme_classic()+                                 # pre-defined theme adjustments\n  theme(\n    legend.position = \"bottom\",                    # move legend to bottom\n    \n    plot.title = element_text(size = 30),          # size of title to 30\n    plot.caption = element_text(hjust = 0),        # left-align caption\n    plot.subtitle = element_text(face = \"italic\"), # italicize subtitle\n    \n    axis.text.x = element_text(color = \"red\", size = 15, angle = 90), # adjusts only x-axis text\n    axis.text.y = element_text(size = 15),         # adjusts only y-axis text\n    \n    axis.title = element_text(size = 20)           # adjusts both axes titles\n    )     \n\n\n\n\nHere are some especially common theme() arguments. You will recognize some patterns, such as appending .x or .y to apply the change only to one axis.\n\n\n\n\n\n\n\ntheme() argument\nWhat it adjusts\n\n\n\n\nplot.title = element_text()\nThe title\n\n\nplot.subtitle = element_text()\nThe subtitle\n\n\nplot.caption = element_text()\nThe caption (family, face, color, size, angle, vjust, hjust…)\n\n\naxis.title = element_text()\nAxis titles (both x and y) (size, face, angle, color…)\n\n\naxis.title.x = element_text()\nAxis title x-axis only (use .y for y-axis only)\n\n\naxis.text = element_text()\nAxis text (both x and y)\n\n\naxis.text.x = element_text()\nAxis text x-axis only (use .y for y-axis only)\n\n\naxis.ticks = element_blank()\nRemove axis ticks\n\n\naxis.line = element_line()\nAxis lines (colour, size, linetype: solid dashed dotted etc)\n\n\nstrip.text = element_text()\nFacet strip text (colour, face, size, angle…)\n\n\nstrip.background = element_rect()\nfacet strip (fill, colour, size…)\n\n\n\nBut there are so many theme arguments! How could I remember them all? Do not worry - it is impossible to remember them all. Luckily there are a few tools to help you:\nThe tidyverse documentation on modifying theme, which has a complete list.\n\n\n\n\n\n\nTip\n\n\n\nRun theme_get() from ggplot2 to print a list of all 90+ theme() arguments to the console.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you ever want to remove an element of a plot, you can also do it through theme(). Just pass element_blank() to an argument to have it disappear completely. For legends, set legend.position = \"none\"."
  },
  {
    "objectID": "readings/ggplot-customize.html#ggplot_tips_colors",
    "href": "readings/ggplot-customize.html#ggplot_tips_colors",
    "title": "Themes and Specialized Plots",
    "section": "",
    "text": "In ggplot2, when aesthetics of plotted data (e.g. size, color, shape, fill, plot axis) are mapped to columns in the data, the exact display can be adjusted with the corresponding “scale” command. In this section we explain some common scale adjustments.\n\n\nOne thing that can initially be difficult to understand with ggplot2 is control of color schemes. Note that this section discusses the color of plot objects (geoms/shapes) such as points, bars, lines, tiles, etc.\nTo control “color” of plot objects you will be adjusting either the color = aesthetic (the exterior color) or the fill = aesthetic (the interior color). One exception to this pattern is geom_point(), where you really only get to control color =, which controls the color of the point (interior and exterior).\nWhen setting colour or fill you can use colour names recognized by R like \"red\" (see complete list or enter ?colors), or a specific hex colour such as \"#ff0505\".\n\n# histogram - \nggplot(data = linelist, mapping = aes(x = age))+       # set data and axes\n  geom_histogram(              # display histogram\n    binwidth = 7,                # width of bins\n    color = \"red\",               # bin line color\n    fill = \"lightblue\")          # bin interior color (fill) \n\n\n\n\nAesthetics such as fill = and color = can be defined either outside of a mapping = aes() statement or inside of one. If outside the aes(), the assigned value should be static (e.g. color = \"blue\") and will apply for all data plotted by the geom. If inside, the aesthetic should be mapped to a column, like color = hospital, and the expression will vary by the value for that row in the data. A few examples:\n\n# Static color for points and for line\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(color = \"purple\")+\n  geom_vline(xintercept = 50, color = \"orange\")+\n  labs(title = \"Static color for points and line\")\n\n# Color mapped to continuous column\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(mapping = aes(color = temp))+         \n  labs(title = \"Color mapped to continuous column\")\n\n# Color mapped to discrete column\nggplot(data = linelist, mapping = aes(x = age, y = wt_kg))+     \n  geom_point(mapping = aes(color = gender))+         \n  labs(title = \"Color mapped to discrete column\")\n\n# bar plot, fill to discrete column, color to static value\nggplot(data = linelist, mapping = aes(x = hospital))+     \n  geom_bar(mapping = aes(fill = gender), color = \"yellow\")+         \n  labs(title = \"Fill mapped to discrete column, static color\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOnce you map a column to a plot aesthetic (e.g. x =, y =, fill =, color =…), your plot will gain a scale/legend. See above how the scale can be continuous, discrete, date, etc. values depending on the class of the assigned column. If you have multiple aesthetics mapped to columns, your plot will have multiple scales.\nYou can control the scales with the appropriate scales_() function. The scale functions of ggplot() have 3 parts that are written like this: scale_AESTHETIC_METHOD().\n\nThe first part, scale_(), is fixed.\n\nThe second part, the AESTHETIC, should be the aesthetic that you want to adjust the scale for (_fill_, _shape_, _color_, _size_, _alpha_…) - the options here also include _x_ and _y_.\n\nThe third part, the METHOD, will be either _discrete(), continuous(), _date(), _gradient(), or _manual() depending on the class of the column and how you want to control it. There are others, but these are the most-often used.\n\nBe sure that you use the correct function for the scale! Otherwise your scale command will not appear to change anything. If you have multiple scales, you may use multiple scale functions to adjust them! For example:\n\n\n\nEach kind of scale has its own arguments, though there is some overlap. Query the function like ?scale_color_discrete in the R console to see the function argument documentation.\nFor continuous scales, use breaks = to provide a sequence of values with seq() (take to =, from =, and by = as shown in the example below. Set expand = c(0,0) to eliminate padding space around the axes (this can be used on any _x_ or _y_ scale.\nFor discrete scales, you can adjust the order of level appearance with breaks =, and how the values display with the labels = argument. Provide a character vector to each of those (see example below). You can also drop NA easily by setting na.translate = FALSE.\n\n\n\nOne of the most useful tricks is using “manual” scaling functions to explicitly assign colors as you desire. These are functions with the syntax scale_xxx_manual() (e.g. scale_colour_manual() or scale_fill_manual()). Each of the below arguments are demonstrated in the code example below.\n\nAssign colors to data values with the values = argument\n\nSpecify a color for NA with na.value =\n\nChange how the values are written in the legend with the labels = argument\n\nChange the legend title with name =\n\nBelow, we create a bar plot and show how it appears by default, and then with three scales adjusted - the continuous y-axis scale, the discrete x-axis scale, and manual adjustment of the fill (interior bar color).\n\n# BASELINE - no scale adjustment\nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  labs(title = \"Baseline - no scale adjustments\")\n\n\n\n# SCALES ADJUSTED\nggplot(data = linelist)+\n  \n  geom_bar(mapping = aes(x = outcome, fill = gender), color = \"black\")+\n  \n  theme_minimal()+                   # simplify background\n  \n  scale_y_continuous(                # continuous scale for y-axis (counts)\n    expand = c(0,0),                 # no padding\n    breaks = seq(from = 0,\n                 to = 3000,\n                 by = 500))+\n  \n  scale_x_discrete(                   # discrete scale for x-axis (gender)\n    expand = c(0,0),                  # no padding\n    drop = FALSE,                     # show all factor levels (even if not in data)\n    na.translate = FALSE,             # remove NA outcomes from plot\n    labels = c(\"Died\", \"Recovered\"))+ # Change display of values\n    \n  \n  scale_fill_manual(                  # Manually specify fill (bar interior color)\n    values = c(\"m\" = \"violetred\",     # reference values in data to assign colors\n               \"f\" = \"aquamarine\"),\n    labels = c(\"m\" = \"Male\",          # re-label the legend (use \"=\" assignment to avoid mistakes)\n              \"f\" = \"Female\",\n              \"Missing\"),\n    name = \"Gender\",                  # title of legend\n    na.value = \"grey\"                 # assign a color for missing values\n  )+\n  labs(title = \"Adjustment of scales\") # Adjust the title of the fill legend\n\n\n\n\n\n\n\nWhen data are mapping to the plot axes, these too can be adjusted with scales commands. A common example is adjusting the display of an axis (e.g. y-axis) that is mapped to a column with continuous data.\nWe may want to adjust the breaks or display of the values in the ggplot using scale_y_continuous(). As noted above, use the argument breaks = to provide a sequence of values that will serve as “breaks” along the scale. These are the values at which numbers will display. To this argument, you can provide a c() vector containing the desired break values, or you can provide a regular sequence of numbers using the base R function seq(). This seq() function accepts to =, from =, and by =.\n\n# BASELINE - no scale adjustment\nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  labs(title = \"Baseline - no scale adjustments\")\n\n# \nggplot(data = linelist)+\n  geom_bar(mapping = aes(x = outcome, fill = gender))+\n  scale_y_continuous(\n    breaks = seq(\n      from = 0,\n      to = 3000,\n      by = 100)\n  )+\n  labs(title = \"Adjusted y-axis breaks\")\n\n\n\n\n\n\n\n\n\nIf your original data values are proportions, you can easily display them as percents with “%” by providing labels = scales::percent in your scales command, as shown below.\nWhile an alternative would be to convert the values to character and add a “%” character to the end, this approach will cause complications because your data will no longer be continuous numeric values.\n\n# Original y-axis proportions\n#############################\nlinelist %&gt;%                                   # start with linelist\n  group_by(hospital) %&gt;%                       # group data by hospital\n  summarise(                                   # create summary columns\n    n = n(),                                     # total number of rows in group\n    deaths = sum(outcome == \"Death\", na.rm=T),   # number of deaths in group\n    prop_death = deaths/n) %&gt;%                   # proportion deaths in group\n  ggplot(                                      # begin plotting\n    mapping = aes(\n      x = hospital,\n      y = prop_death))+ \n  geom_col()+\n  theme_minimal()+\n  labs(title = \"Display y-axis original proportions\")\n\n\n\n# Display y-axis proportions as percents\n########################################\nlinelist %&gt;%         \n  group_by(hospital) %&gt;% \n  summarise(\n    n = n(),\n    deaths = sum(outcome == \"Death\", na.rm=T),\n    prop_death = deaths/n) %&gt;% \n  ggplot(\n    mapping = aes(\n      x = hospital,\n      y = prop_death))+\n  geom_col()+\n  theme_minimal()+\n  labs(title = \"Display y-axis as percents (%)\")+\n  scale_y_continuous(\n    labels = scales::percent                    # display proportions as percents\n  )\n\n\n\n\n\n\n\n\n\n\nTo transform a continuous axis to log scale, add trans = \"log2\" to the scale command. For purposes of example, we create a data frame of regions with their respective preparedness_index and cumulative cases values.\n\nplot_data &lt;- data.frame(\n  region = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\"),\n  preparedness_index = c(8.8, 7.5, 3.4, 3.6, 2.1, 7.9, 7.0, 5.6, 1.0),\n  cases_cumulative = c(15, 45, 80, 20, 21, 7, 51, 30, 1442)\n)\n\nplot_data\n\n  region preparedness_index cases_cumulative\n1      A                8.8               15\n2      B                7.5               45\n3      C                3.4               80\n4      D                3.6               20\n5      E                2.1               21\n6      F                7.9                7\n7      G                7.0               51\n8      H                5.6               30\n9      I                1.0             1442\n\n\nThe cumulative cases for region “I” are dramatically greater than all the other regions. In circumstances like this, you may elect to display the y-axis using a log scale so the reader can see differences between the regions with fewer cumulative cases.\n\n# Original y-axis\npreparedness_plot &lt;- ggplot(data = plot_data,  \n       mapping = aes(\n         x = preparedness_index,\n         y = cases_cumulative))+\n  geom_point(size = 2)+            # points for each region \n  geom_text(\n    mapping = aes(label = region),\n    vjust = 1.5)+                  # add text labels\n  theme_minimal()\n\npreparedness_plot                  # print original plot\n\n\n# print with y-axis transformed\npreparedness_plot+                   # begin with plot saved above\n  scale_y_continuous(trans = \"log2\") # add transformation for y-axis\n\n\n\n\n\n\n\n\n\n\n\nFill gradient scales can involve additional nuance. The defaults are usually quite pleasing, but you may want to adjust the values, cutoffs, etc.\nTo demonstrate how to adjust a continuous color scale, we’ll use a data set that contains the ages of cases and of sources in a contact tracing case.\n\ncase_source_relationships &lt;- rio::import(\"data/godata/relationships_clean.rds\") %&gt;% \n  select(source_age, target_age) \n\nBelow, we produce a “raster” heat tile density plot. Note how the fill scale is continuous.\n\ntrans_matrix &lt;- ggplot(\n    data = case_source_relationships,\n    mapping = aes(x = source_age, y = target_age))+\n  stat_density2d(\n    geom = \"raster\",\n    mapping = aes(fill = after_stat(density)),\n    contour = FALSE)+\n  theme_minimal()\n\nNow we show some variations on the fill scale:\n\ntrans_matrix\ntrans_matrix + scale_fill_viridis_c(option = \"plasma\")\n\n\n\n\n\n\n\nNow we show some examples of actually adjusting the break points of the scale:\n\nscale_fill_gradient() accepts two colors (high/low)\n\nscale_fill_gradientn() accepts a vector of any length of colors to values = (intermediate values will be interpolated)\n\nUse scales::rescale() to adjust how colors are positioned along the gradient; it rescales your vector of positions to be between 0 and 1.\n\n\ntrans_matrix + \n  scale_fill_gradient(     # 2-sided gradient scale\n    low = \"aquamarine\",    # low value\n    high = \"purple\",       # high value\n    na.value = \"grey\",     # value for NA\n    name = \"Density\")+     # Legend title\n  labs(title = \"Manually specify high/low colors\")\n\n# 3+ colors to scale\ntrans_matrix + \n  scale_fill_gradientn(    # 3-color scale (low/mid/high)\n    colors = c(\"blue\", \"yellow\",\"red\") # provide colors in vector\n  )+\n  labs(title = \"3-color scale\")\n\n# Use of rescale() to adjust placement of colors along scale\ntrans_matrix + \n  scale_fill_gradientn(    # provide any number of colors\n    colors = c(\"blue\", \"yellow\",\"red\", \"black\"),\n    values = scales::rescale(c(0, 0.05, 0.07, 0.10, 0.15, 0.20, 0.3, 0.5)) # positions for colors are rescaled between 0 and 1\n    )+\n  labs(title = \"Colors not evenly positioned\")\n\n# use of limits to cut-off values that get fill color\ntrans_matrix + \n  scale_fill_gradientn(    \n    colors = c(\"blue\", \"yellow\",\"red\"),\n    limits = c(0, 0.0002))+\n  labs(title = \"Restrict value limits, resulting in grey space\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore generally, if you want predefined palettes, you can use the scale_xxx_brewer or scale_xxx_viridis_y functions.\nThe ‘brewer’ functions can draw from colorbrewer.org palettes.\nThe ‘viridis’ functions draw from viridis (colourblind friendly!) palettes, which “provide colour maps that are perceptually uniform in both colour and black-and-white. They are also designed to be perceived by viewers with common forms of colour blindness.” (read more here and here). Define if the palette is discrete, continuous, or binned by specifying this at the end of the function (e.g. discrete is scale_xxx_viridis_d).\nIt is advised that you test your plot in this color blindness simulator. If you have a red/green color scheme, try a “hot-cold” (red-blue) scheme instead as described here\nHere is an example of using various color schemes.\n\nsymp_plot &lt;- linelist %&gt;%                                         # begin with linelist\n  select(c(case_id, fever, chills, cough, aches, vomit)) %&gt;%     # select columns\n  pivot_longer(                                                  # pivot longer\n    cols = -case_id,                                  \n    names_to = \"symptom_name\",\n    values_to = \"symptom_is_present\") %&gt;%\n  mutate(                                                        # replace missing values\n    symptom_is_present = replace_na(symptom_is_present, \"unknown\")) %&gt;% \n  ggplot(                                                        # begin ggplot!\n    mapping = aes(x = symptom_name, fill = symptom_is_present))+\n  geom_bar(position = \"fill\", col = \"black\") +                    \n  theme_classic() +\n  theme(legend.position = \"bottom\")+\n  labs(\n    x = \"Symptom\",\n    y = \"Symptom status (proportion)\"\n  )\n\nsymp_plot  # print with default colors\n\n#################################\n# print with manually-specified colors\nsymp_plot +\n  scale_fill_manual(\n    values = c(\"yes\" = \"black\",         # explicitly define colours\n               \"no\" = \"white\",\n               \"unknown\" = \"grey\"),\n    breaks = c(\"yes\", \"no\", \"unknown\"), # order the factors correctly\n    name = \"\"                           # set legend to no title\n\n  ) \n\n#################################\n# print with viridis discrete colors\nsymp_plot +\n  scale_fill_viridis_d(\n    breaks = c(\"yes\", \"no\", \"unknown\"),\n    name = \"\"\n  )"
  },
  {
    "objectID": "readings/ggplot-customize.html#change-order-of-discrete-variables",
    "href": "readings/ggplot-customize.html#change-order-of-discrete-variables",
    "title": "Themes and Specialized Plots",
    "section": "",
    "text": "Changing the order that discrete variables appear in is often difficult to understand for people who are new to ggplot2 graphs. It’s easy to understand how to do this however once you understand how ggplot2 handles discrete variables under the hood. Generally speaking, if a discrete varaible is used, it is automatically converted to a factor type - which orders factors by alphabetical order by default. To handle this, you simply have to reorder the factor levels to reflect the order you would like them to appear in the chart. For more detailed information on how to reorder factor objects, see the factor section of the guide.\nWe can look at a common example using age groups - by default the 5-9 age group will be placed in the middle of the age groups (given alphanumeric order), but we can move it behind the 0-4 age group of the chart by releveling the factors.\n\nggplot(\n  data = linelist %&gt;% drop_na(age_cat5),                         # remove rows where age_cat5 is missing\n  mapping = aes(x = fct_relevel(age_cat5, \"5-9\", after = 1))) +  # relevel factor\n\n  geom_bar() +\n  \n  labs(x = \"Age group\", y = \"Number of hospitalisations\",\n       title = \"Total hospitalisations by age group\") +\n  \n  theme_minimal()"
  },
  {
    "objectID": "readings/ggplot-customize.html#advanced-ggplot-optional",
    "href": "readings/ggplot-customize.html#advanced-ggplot-optional",
    "title": "Themes and Specialized Plots",
    "section": "",
    "text": "These are a collection of less common plot types, ggplot2 extensions, and advanced examples of some of the things you can do visualizing data in R.\n\n\nContour plots are helpful when you have many points that might cover each other (“overplotting”). The case-source data used above are again plotted, but more simply using stat_density2d() and stat_density2d_filled() to produce discrete contour levels - like a topographical map. Read more about the statistics here.\n\ncase_source_relationships %&gt;% \n  ggplot(aes(x = source_age, y = target_age))+\n  stat_density2d()+\n  geom_point()+\n  theme_minimal()+\n  labs(title = \"stat_density2d() + geom_point()\")\n\n\ncase_source_relationships %&gt;% \n  ggplot(aes(x = source_age, y = target_age))+\n  stat_density2d_filled()+\n  theme_minimal()+\n  labs(title = \"stat_density2d_filled()\")\n\n\n\n\n\n\n\n\n\n\nTo show the distributions on the edges of a geom_point() scatterplot, you can use the ggExtra package and its function ggMarginal(). Save your original ggplot as an object, then pass it to ggMarginal() as shown below. Here are the key arguments:\n\nYou must specify the type = as either “histogram”, “density” “boxplot”, “violin”, or “densigram”.\n\nBy default, marginal plots will appear for both axes. You can set margins = to “x” or “y” if you only want one.\n\nOther optional arguments include fill = (bar color), color = (line color), size = (plot size relative to margin size, so larger number makes the marginal plot smaller).\n\nYou can provide other axis-specific arguments to xparams = and yparams =. For example, to have different histogram bin sizes, as shown below.\n\nYou can have the marginal plots reflect groups (columns that have been assigned to color = in your ggplot() mapped aesthetics). If this is the case, set the ggMarginal() argument groupColour = or groupFill = to TRUE, as shown below.\nRead more at this vignette, in the R Graph Gallery or the function R documentation ?ggMarginal.\n\n# Install/load ggExtra\npacman::p_load(ggExtra)\n\n# Basic scatter plot of weight and age\nscatter_plot &lt;- ggplot(data = linelist)+\n  geom_point(mapping = aes(y = wt_kg, x = age)) +\n  labs(title = \"Scatter plot of weight and age\")\n\nTo add marginal histograms use type = \"histogram\". You can optionally set groupFill = TRUE to get stacked histograms.\n\n# with histograms\nggMarginal(\n  scatter_plot,                     # add marginal histograms\n  type = \"histogram\",               # specify histograms\n  fill = \"lightblue\",               # bar fill\n  xparams = list(binwidth = 10),    # other parameters for x-axis marginal\n  yparams = list(binwidth = 5))     # other parameters for y-axis marginal\n\n\n\n\nMarginal density plot with grouped/colored values:\n\n# Scatter plot, colored by outcome\n# Outcome column is assigned as color in ggplot. groupFill in ggMarginal set to TRUE\nscatter_plot_color &lt;- ggplot(data = linelist %&gt;% drop_na(gender))+\n  geom_point(mapping = aes(y = wt_kg, x = age, color = gender)) +\n  labs(title = \"Scatter plot of weight and age\")+\n  theme(legend.position = \"bottom\")\n\nggMarginal(scatter_plot_color, type = \"density\", groupFill = TRUE)\n\n\n\n\nSet the size = arguemnt to adjust the relative size of the marginal plot. Smaller number makes a larger marginal plot. You also set color =. Below are is a marginal boxplot, with demonstration of the margins = argument so it appears on only one axis:\n\n# with boxplot \nggMarginal(\n  scatter_plot,\n  margins = \"x\",      # only show x-axis marginal plot\n  type = \"boxplot\")   \n\n\n\n\n\n\n\n\nIn ggplot2, it is also possible to add text to plots. However, this comes with the notable limitation where text labels often clash with data points in a plot, making them look messy or hard to read. There is no ideal way to deal with this in the base package, but there is a ggplot2 add-on, known as ggrepel that makes dealing with this very simple!\nThe ggrepel package provides two new functions, geom_label_repel() and geom_text_repel(), which replace geom_label() and geom_text(). Simply use these functions instead of the base functions to produce neat labels. Within the function, map the aesthetics aes() as always, but include the argument label = to which you provide a column name containing the values you want to display (e.g. patient id, or name, etc.). You can make more complex labels by combining columns and newlines (\\n) within str_glue() as shown below.\nA few tips:\n\nUse min.segment.length = 0 to always draw line segments, or min.segment.length = Inf to never draw them\n\nUse size = outside of aes() to set text size\n\nUse force = to change the degree of repulsion between labels and their respective points (default is 1)\n\nInclude fill = within aes() to have label colored by value\n\nA letter “a” may appear in the legend - add guides(fill = guide_legend(override.aes = aes(color = NA)))+ to remove it\n\n\nSee this is very in-depth tutorial for more.\n\npacman::p_load(ggrepel)\n\nlinelist %&gt;%                                               # start with linelist\n  group_by(hospital) %&gt;%                                   # group by hospital\n  summarise(                                               # create new dataset with summary values per hospital\n    n_cases = n(),                                           # number of cases per hospital\n    delay_mean = round(mean(days_onset_hosp, na.rm=T),1),    # mean delay per hospital\n  ) %&gt;% \n  ggplot(mapping = aes(x = n_cases, y = delay_mean))+      # send data frame to ggplot\n  geom_point(size = 2)+                                    # add points\n  geom_label_repel(                                        # add point labels\n    mapping = aes(\n      label = stringr::str_glue(\n        \"{hospital}\\n{n_cases} cases, {delay_mean} days\")  # how label displays\n      ), \n    size = 3,                                              # text size in labels\n    min.segment.length = 0)+                               # show all line segments                \n  labs(                                                    # add axes labels\n    title = \"Mean delay to admission, by hospital\",\n    x = \"Number of cases\",\n    y = \"Mean delay (days)\")\n\n\n\n\nYou can label only a subset of the data points - by using standard ggplot() syntax to provide different data = for each geom layer of the plot. Below, All cases are plotted, but only a few are labeled.\n\nggplot()+\n  # All points in grey\n  geom_point(\n    data = linelist,                                   # all data provided to this layer\n    mapping = aes(x = ht_cm, y = wt_kg),\n    color = \"grey\",\n    alpha = 0.5)+                                              # grey and semi-transparent\n  \n  # Few points in black\n  geom_point(\n    data = linelist %&gt;% filter(days_onset_hosp &gt; 15),  # filtered data provided to this layer\n    mapping = aes(x = ht_cm, y = wt_kg),\n    alpha = 1)+                                                # default black and not transparent\n  \n  # point labels for few points\n  geom_label_repel(\n    data = linelist %&gt;% filter(days_onset_hosp &gt; 15),  # filter the data for the labels\n    mapping = aes(\n      x = ht_cm,\n      y = wt_kg,\n      fill = outcome,                                          # label color by outcome\n      label = stringr::str_glue(\"Delay: {days_onset_hosp}d\")), # label created with str_glue()\n    min.segment.length = 0) +                                  # show line segments for all\n  \n  # remove letter \"a\" from inside legend boxes\n  guides(fill = guide_legend(override.aes = aes(color = NA)))+\n  \n  # axis labels\n  labs(\n    title = \"Cases with long delay to admission\",\n    y = \"weight (kg)\",\n    x = \"height(cm)\")\n\n\n\n\n\n\n\n\nHighlighting specific elements in a chart is a useful way to draw attention to a specific instance of a variable while also providing information on the dispersion of the full dataset. While this is not easily done in base ggplot2, there is an external package that can help to do this known as gghighlight. This is easy to use within the ggplot syntax.\nThe gghighlight package uses the gghighlight() function to achieve this effect. To use this function, supply a logical statement to the function - this can have quite flexible outcomes, but here we’ll show an example of the age distribution of cases in our linelist, highlighting them by outcome.\n\n# load gghighlight\nlibrary(gghighlight)\n\n# replace NA values with unknown in the outcome variable\nlinelist &lt;- linelist %&gt;%\n  mutate(outcome = replace_na(outcome, \"Unknown\"))\n\n# produce a histogram of all cases by age\nggplot(\n  data = linelist,\n  mapping = aes(x = age_years, fill = outcome)) +\n  geom_histogram() + \n  gghighlight::gghighlight(outcome == \"Death\")     # highlight instances where the patient has died.\n\n\n\n\nThis also works well with faceting functions - it allows the user to produce facet plots with the background data highlighted that doesn’t apply to the facet! Below we count cases by week and plot the epidemic curves by hospital (color = and facet_wrap() set to hospital column).\n\n# produce a histogram of all cases by age\nlinelist %&gt;% \n  count(week = lubridate::floor_date(date_hospitalisation, \"week\"),\n        hospital) %&gt;% \n  ggplot()+\n  geom_line(aes(x = week, y = n, color = hospital))+\n  theme_minimal()+\n  gghighlight::gghighlight() +                      # highlight instances where the patient has died\n  facet_wrap(~hospital)                              # make facets by outcome\n\n\n\n\n\n\n\nNote that properly aligning axes to plot from multiple datasets in the same plot can be difficult. Consider one of the following strategies:\n\nMerge the data prior to plotting, and convert to “long” format with a column reflecting the dataset\n\nUse cowplot or a similar package to combine two plots (see below)\n\n\n\n\n\nTwo packages that are very useful for combining plots are cowplot and patchwork. In this page we will mostly focus on cowplot, with occassional use of patchwork.\nHere is the online introduction to cowplot. You can read the more extensive documentation for each function online here. We will cover a few of the most common use cases and functions below.\nThe cowplot package works in tandem with ggplot2 - essentially, you use it to arrange and combine ggplots and their legends into compound figures. It can also accept base R graphics.\n\npacman::p_load(\n  tidyverse,      # data manipulation and visualisation\n  cowplot,        # combine plots\n  patchwork       # combine plots\n)\n\nWhile faceting is a convenient approach to plotting, sometimes its not possible to get the results you want from its relatively restrictive approach. Here, you may choose to combine plots by sticking them together into a larger plot. There are three well known packages that are great for this - cowplot, gridExtra, and patchwork. However, these packages largely do the same things, so we’ll focus on cowplot for this section.\n\n\nThe cowplot package has a fairly wide range of functions, but the easiest use of it can be achieved through the use of plot_grid(). This is effectively a way to arrange predefined plots in a grid formation. We can work through another example with the malaria dataset - here we can plot the total cases by district, and also show the epidemic curve over time.\n\nmalaria_data &lt;- rio::import(\"data/malaria_facility_count_data.rds\")\n\n# bar chart of total cases by district\np1 &lt;- ggplot(malaria_data, aes(x = District, y = malaria_tot)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    x = \"District\",\n    y = \"Total number of cases\",\n    title = \"Total malaria cases by district\"\n  ) +\n  theme_minimal()\n\n# epidemic curve over time\np2 &lt;- ggplot(malaria_data, aes(x = data_date, y = malaria_tot)) +\n  geom_col(width = 1) +\n  labs(\n    x = \"Date of data submission\",\n    y =  \"number of cases\"\n  ) +\n  theme_minimal()\n\ncowplot::plot_grid(p1, p2,\n                  # 1 column and two rows - stacked on top of each other\n                   ncol = 1,\n                   nrow = 2,\n                   # top plot is 2/3 as tall as second\n                   rel_heights = c(2, 3))\n\n\n\n\n\n\n\nIf your plots have the same legend, combining them is relatively straight-forward. Simple use the cowplot approach above to combine the plots, but remove the legend from one of them (de-duplicate).\nIf your plots have different legends, you must use an alternative approach:\n\nCreate and save your plots without legends using theme(legend.position = \"none\")\n\nExtract the legends from each plot using get_legend() as shown below - but extract legends from the plots modified to actually show the legend\n\nCombine the legends into a legends panel\n\nCombine the plots and legends panel\n\nFor demonstration we show the two plots separately, and then arranged in a grid with their own legends showing (ugly and inefficient use of space):\n\np1 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n  scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  labs(title = \"Cases by outcome\")\n\n\np2 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, age_cat) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(axis.text.y = element_blank())+\n  labs(title = \"Cases by age\")\n\nHere is how the two plots look when combined using plot_grid() without combining their legends:\n\ncowplot::plot_grid(p1, p2, rel_widths = c(0.3))\n\n\n\n\nAnd now we show how to combine the legends. Essentially what we do is to define each plot without its legend (theme(legend.position = \"none\"), and then we define each plot’s legend separately, using the get_legend() function from cowplot. When we extract the legend from the saved plot, we need to add + the legend back in, including specifying the placement (“right”) and smaller adjustments for alignment of the legends and their titles. Then, we combine the legends together vertically, and then combine the two plots with the newly-combined legends. Voila!\n\n# Define plot 1 without legend\np1 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n  scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(legend.position = \"none\")+\n  labs(title = \"Cases by outcome\")\n\n\n# Define plot 2 without legend\np2 &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, age_cat) %&gt;% \n  ggplot()+\n  geom_col(mapping = aes(x = hospital, y = n, fill = age_cat))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+\n  coord_flip()+\n  theme_minimal()+\n  theme(\n    legend.position = \"none\",\n    axis.text.y = element_blank(),\n    axis.title.y = element_blank()\n  )+\n  labs(title = \"Cases by age\")\n\n\n# extract legend from p1 (from p1 + legend)\nleg_p1 &lt;- cowplot::get_legend(p1 +\n                                theme(legend.position = \"right\",        # extract vertical legend\n                                      legend.justification = c(0,0.5))+ # so legends  align\n                                labs(fill = \"Outcome\"))                 # title of legend\n# extract legend from p2 (from p2 + legend)\nleg_p2 &lt;- cowplot::get_legend(p2 + \n                                theme(legend.position = \"right\",         # extract vertical legend   \n                                      legend.justification = c(0,0.5))+  # so legends align\n                                labs(fill = \"Age Category\"))             # title of legend\n\n# create a blank plot for legend alignment\n#blank_p &lt;- patchwork::plot_spacer() + theme_void()\n\n# create legends panel, can be one on top of the other (or use spacer commented above)\nlegends &lt;- cowplot::plot_grid(leg_p1, leg_p2, nrow = 2, rel_heights = c(.3, .7))\n\n# combine two plots and the combined legends panel\ncombined &lt;- cowplot::plot_grid(p1, p2, legends, ncol = 3, rel_widths = c(.4, .4, .2))\n\ncombined  # print\n\n\n\n\nThis solution was learned from this post with a minor fix to align legends from this post.\nTIP: Fun note - the “cow” in cowplot comes from the creator’s name - Claus O. Wilke.\n\n\n\nYou can inset one plot in another using cowplot. Here are things to be aware of:\n\nDefine the main plot with theme_half_open() from cowplot; it may be best to have the legend either on top or bottom\n\nDefine the inset plot. Best is to have a plot where you do not need a legend. You can remove plot theme elements with element_blank() as shown below.\n\nCombine them by applying ggdraw() to the main plot, then adding draw_plot() on the inset plot and specifying the coordinates (x and y of lower left corner), height and width as proportion of the whole main plot.\n\n\n# Define main plot\nmain_plot &lt;- ggplot(data = linelist)+\n  geom_histogram(aes(x = date_onset, fill = hospital))+\n  scale_fill_brewer(type = \"qual\", palette = 1, na.value = \"grey\")+ \n  theme_half_open()+\n  theme(legend.position = \"bottom\")+\n  labs(title = \"Epidemic curve and outcomes by hospital\")\n\n\n# Define inset plot\ninset_plot &lt;- linelist %&gt;% \n  mutate(hospital = recode(hospital, \"St. Mark's Maternity Hospital (SMMH)\" = \"St. Marks\")) %&gt;% \n  count(hospital, outcome) %&gt;% \n  ggplot()+\n    geom_col(mapping = aes(x = hospital, y = n, fill = outcome))+\n    scale_fill_brewer(type = \"qual\", palette = 4, na.value = \"grey\")+\n    coord_flip()+\n    theme_minimal()+\n    theme(legend.position = \"none\",\n          axis.title.y = element_blank())+\n    labs(title = \"Cases by outcome\") \n\n\n# Combine main with inset\ncowplot::ggdraw(main_plot)+\n     draw_plot(inset_plot,\n               x = .6, y = .55,    #x = .07, y = .65,\n               width = .4, height = .4)\n\n\n\n\nThis technique is explained more in these two vignettes:\nWilke lab\ndraw_plot() documentation"
  },
  {
    "objectID": "readings/manage-data.html",
    "href": "readings/manage-data.html",
    "title": "Managing Data",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of these materials from chapters 8, 13, and 14.\n\n\nThis week we will be diving deeper into operations needed for data analysis. We will look at how to manipulate column names, group data by its variables, and append datasets together.\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\nWe will be continuing with the same simulated Ebola outbreak dataset.\n\npacman::p_load(\n  rio,        # importing data  \n  here,       # relative file pathways  \n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  matchmaker, # dictionary-based cleaning\n  epikit,     # age_categories() function\n  tidyverse   # data management and visualization\n)\nlinelist &lt;- rio::import(\"data/case_linelists/linelist_cleaned.rds\")\n\n\nlinelist &lt;- import(\"linelist_cleaned.rds\")\n\n\n\n\n\n\nIn R, column names are the “header” or “top” value of a column. They are used to refer to columns in the code, and serve as a default label in figures.\nOther statistical software such as SAS and STATA use “labels” that co-exist as longer printed versions of the shorter column names. While R does offer the possibility of adding column labels to the data, this is not emphasized in most practice. To make column names “printer-friendly” for figures, one typically adjusts their display within the plotting commands that create the outputs.\nAs R column names are used very often, so they must have “clean” syntax. We suggest the following:\n\nShort names\nNo spaces (replace with underscores _ )\nNo unusual characters (&, #, &lt;, &gt;, …)\n\nSimilar style nomenclature (e.g. all date columns named like date_onset, date_report, date_death…)\n\nRe-naming columns manually is often necessary, even after the standardization step above. Below, re-naming is performed using the rename() function from the dplyr package, as part of a pipe chain. rename() uses the style NEW = OLD - the new column name is given before the old column name.\nBelow, a re-naming command is added to the cleaning pipeline. Spaces have been added strategically to align code for easier reading.\nNow you can see that the columns names have been changed:\n\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"age\"                 \n[10] \"age_unit\"             \"age_years\"            \"age_cat\"             \n[13] \"age_cat5\"             \"hospital\"             \"lon\"                 \n[16] \"lat\"                  \"infector\"             \"source\"              \n[19] \"wt_kg\"                \"ht_cm\"                \"ct_blood\"            \n[22] \"fever\"                \"chills\"               \"cough\"               \n[25] \"aches\"                \"vomit\"                \"temp\"                \n[28] \"time_admission\"       \"bmi\"                  \"days_onset_hosp\"     \n\n\n\n\nYou can also rename by column position, instead of column name, for example:\n\nrename(newNameForFirstColumn  = 1,\n       newNameForSecondColumn = 2)\n\n\n\n\nAs a shortcut, you can also rename columns within the dplyr select() and summarise() functions. select() is used to keep only certain columns and summarise() is used when grouping data. These functions also uses the format new_name = old_name. Here is an example:\n\nlinelist_raw %&gt;% \n  select(# NEW name             # OLD name\n         date_infection       = `infection date`,    # rename and KEEP ONLY these columns\n         date_hospitalisation = `hosp date`)\n\n\n\n\n\n\nR cannot have dataset columns that do not have column names (headers). So, if you import an Excel dataset with data but no column headers, R will fill-in the headers with names like “…1” or “…2”. The number represents the column number (e.g. if the 4th column in the dataset has no header, then R will name it “…4”).\nYou can clean these names manually by referencing their position number (see example above), or their assigned name (linelist_raw$...1).\n\n\n\nMerged cells in an Excel file are a common occurrence when receiving data. Merged cells can be nice for human reading of data, but are not “tidy data” and cause many problems for machine reading of data. R cannot accommodate merged cells.\nOne solution to deal with merged cells is to import the data with the function readWorkbook() from the package openxlsx. Set the argument fillMergedCells = TRUE. This gives the value in a merged cell to all cells within the merge range.\n\nlinelist_raw &lt;- openxlsx::readWorkbook(\"linelist_raw.xlsx\", fillMergedCells = TRUE)\n\n\n\n\n\n\nTwo weeks ago we learned to use select() to select the columns we wanted to keep.\n\n# linelist dataset is piped through select() command, and names() prints just the column names\nlinelist %&gt;% \n  select(case_id, date_onset, date_hospitalisation, fever) %&gt;% \n  names()  # display the column names\n\n[1] \"case_id\"              \"date_onset\"           \"date_hospitalisation\"\n[4] \"fever\"               \n\n\nLet’s look at some more complicated scenarios when we need to think a bit deeper on how we’re selecting or choosing columns in our data.\n\n\nThese helper functions exist to make it easy to specify columns to keep, discard, or transform. They are from the package tidyselect, which is included in tidyverse and underlies how columns are selected in dplyr functions.\nFor example, if you want to re-order the columns, everything() is a useful function to signify “all other columns not yet mentioned”. The command below moves columns date_onset and date_hospitalisation to the beginning (left) of the dataset, but keeps all the other columns afterward. Note that everything() is written with empty parentheses:\n\n# move date_onset and date_hospitalisation to beginning\nlinelist %&gt;% \n  select(date_onset, date_hospitalisation, everything()) %&gt;% \n  names()\n\n [1] \"date_onset\"           \"date_hospitalisation\" \"case_id\"             \n [4] \"generation\"           \"date_infection\"       \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"age\"                 \n[10] \"age_unit\"             \"age_years\"            \"age_cat\"             \n[13] \"age_cat5\"             \"hospital\"             \"lon\"                 \n[16] \"lat\"                  \"infector\"             \"source\"              \n[19] \"wt_kg\"                \"ht_cm\"                \"ct_blood\"            \n[22] \"fever\"                \"chills\"               \"cough\"               \n[25] \"aches\"                \"vomit\"                \"temp\"                \n[28] \"time_admission\"       \"bmi\"                  \"days_onset_hosp\"     \n\n\nHere are other “tidyselect” helper functions that also work within dplyr functions like select(), across(), and summarise():\n\neverything() - all other columns not mentioned\n\nlast_col() - the last column\n\nwhere() - applies a function to all columns and selects those which are TRUE\n\ncontains() - columns containing a character string\n\nexample: select(contains(\"time\"))\n\n\nstarts_with() - matches to a specified prefix\n\nexample: select(starts_with(\"date_\"))\n\n\nends_with() - matches to a specified suffix\n\nexample: select(ends_with(\"_post\"))\n\n\nmatches() - to apply a regular expression (regex)\n\nexample: select(matches(\"[pt]al\"))\n\n\nnum_range() - a numerical range like x01, x02, x03\n\nany_of() - matches IF column exists but returns no error if it is not found\n\nexample: select(any_of(date_onset, date_death, cardiac_arrest))\n\n\nIn addition, use normal operators such as c() to list several columns, : for consecutive columns, ! for opposite, & for AND, and | for OR.\nUse where() to specify logical criteria for columns. If providing a function inside where(), do not include the function’s empty parentheses. The command below selects columns that are class Numeric.\n\n# select columns that are class Numeric\nlinelist %&gt;% \n  select(where(is.numeric)) %&gt;% \n  names()\n\n [1] \"generation\"      \"age\"             \"age_years\"       \"lon\"            \n [5] \"lat\"             \"wt_kg\"           \"ht_cm\"           \"ct_blood\"       \n [9] \"temp\"            \"bmi\"             \"days_onset_hosp\"\n\n\nUse contains() to select only columns in which the column name contains a specified character string. ends_with() and starts_with() provide more nuance.\n\n# select columns containing certain characters\nlinelist %&gt;% \n  select(contains(\"date\")) %&gt;% \n  names()\n\n[1] \"date_infection\"       \"date_onset\"           \"date_hospitalisation\"\n[4] \"date_outcome\"        \n\n\nThe function matches() works similarly to contains() but can be provided a regular expression, such as multiple strings separated by OR bars within the parentheses:\n\n# searched for multiple character matches\nlinelist %&gt;% \n  select(matches(\"onset|hosp|fev\")) %&gt;%   # note the OR symbol \"|\"\n  names()\n\n[1] \"date_onset\"           \"date_hospitalisation\" \"hospital\"            \n[4] \"fever\"                \"days_onset_hosp\"     \n\n\n\n\n\n\nIn a later week we will learn more about how to de-duplicate data. Only a very simple row de-duplication example is presented here.\nThe package dplyr offers the distinct() function. This function examines every row and reduce the data frame to only the unique rows. That is, it removes rows that are 100% duplicates.\nWhen evaluating duplicate rows, it takes into account a range of columns - by default it considers all columns. As shown in the de-duplication page, you can adjust this column range so that the uniqueness of rows is only evaluated in regards to certain columns.\nIn this simple example, we just add the empty command distinct() to the pipe chain. This ensures there are no rows that are 100% duplicates of other rows (evaluated across all columns).\nWe begin with nrow(linelist) rows in linelist.\n\nlinelist &lt;- linelist %&gt;% \n  distinct()\n\nAfter de-duplication there are nrow(linelist) rows. Any removed rows would have been 100% duplicates of other rows."
  },
  {
    "objectID": "readings/manage-data.html#column-names",
    "href": "readings/manage-data.html#column-names",
    "title": "Managing Data",
    "section": "",
    "text": "In R, column names are the “header” or “top” value of a column. They are used to refer to columns in the code, and serve as a default label in figures.\nOther statistical software such as SAS and STATA use “labels” that co-exist as longer printed versions of the shorter column names. While R does offer the possibility of adding column labels to the data, this is not emphasized in most practice. To make column names “printer-friendly” for figures, one typically adjusts their display within the plotting commands that create the outputs.\nAs R column names are used very often, so they must have “clean” syntax. We suggest the following:\n\nShort names\nNo spaces (replace with underscores _ )\nNo unusual characters (&, #, &lt;, &gt;, …)\n\nSimilar style nomenclature (e.g. all date columns named like date_onset, date_report, date_death…)\n\nRe-naming columns manually is often necessary, even after the standardization step above. Below, re-naming is performed using the rename() function from the dplyr package, as part of a pipe chain. rename() uses the style NEW = OLD - the new column name is given before the old column name.\nBelow, a re-naming command is added to the cleaning pipeline. Spaces have been added strategically to align code for easier reading.\nNow you can see that the columns names have been changed:\n\n\n [1] \"case_id\"              \"generation\"           \"date_infection\"      \n [4] \"date_onset\"           \"date_hospitalisation\" \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"age\"                 \n[10] \"age_unit\"             \"age_years\"            \"age_cat\"             \n[13] \"age_cat5\"             \"hospital\"             \"lon\"                 \n[16] \"lat\"                  \"infector\"             \"source\"              \n[19] \"wt_kg\"                \"ht_cm\"                \"ct_blood\"            \n[22] \"fever\"                \"chills\"               \"cough\"               \n[25] \"aches\"                \"vomit\"                \"temp\"                \n[28] \"time_admission\"       \"bmi\"                  \"days_onset_hosp\"     \n\n\n\n\nYou can also rename by column position, instead of column name, for example:\n\nrename(newNameForFirstColumn  = 1,\n       newNameForSecondColumn = 2)\n\n\n\n\nAs a shortcut, you can also rename columns within the dplyr select() and summarise() functions. select() is used to keep only certain columns and summarise() is used when grouping data. These functions also uses the format new_name = old_name. Here is an example:\n\nlinelist_raw %&gt;% \n  select(# NEW name             # OLD name\n         date_infection       = `infection date`,    # rename and KEEP ONLY these columns\n         date_hospitalisation = `hosp date`)\n\n\n\n\n\n\nR cannot have dataset columns that do not have column names (headers). So, if you import an Excel dataset with data but no column headers, R will fill-in the headers with names like “…1” or “…2”. The number represents the column number (e.g. if the 4th column in the dataset has no header, then R will name it “…4”).\nYou can clean these names manually by referencing their position number (see example above), or their assigned name (linelist_raw$...1).\n\n\n\nMerged cells in an Excel file are a common occurrence when receiving data. Merged cells can be nice for human reading of data, but are not “tidy data” and cause many problems for machine reading of data. R cannot accommodate merged cells.\nOne solution to deal with merged cells is to import the data with the function readWorkbook() from the package openxlsx. Set the argument fillMergedCells = TRUE. This gives the value in a merged cell to all cells within the merge range.\n\nlinelist_raw &lt;- openxlsx::readWorkbook(\"linelist_raw.xlsx\", fillMergedCells = TRUE)"
  },
  {
    "objectID": "readings/manage-data.html#revisiting-select",
    "href": "readings/manage-data.html#revisiting-select",
    "title": "Managing Data",
    "section": "",
    "text": "Two weeks ago we learned to use select() to select the columns we wanted to keep.\n\n# linelist dataset is piped through select() command, and names() prints just the column names\nlinelist %&gt;% \n  select(case_id, date_onset, date_hospitalisation, fever) %&gt;% \n  names()  # display the column names\n\n[1] \"case_id\"              \"date_onset\"           \"date_hospitalisation\"\n[4] \"fever\"               \n\n\nLet’s look at some more complicated scenarios when we need to think a bit deeper on how we’re selecting or choosing columns in our data.\n\n\nThese helper functions exist to make it easy to specify columns to keep, discard, or transform. They are from the package tidyselect, which is included in tidyverse and underlies how columns are selected in dplyr functions.\nFor example, if you want to re-order the columns, everything() is a useful function to signify “all other columns not yet mentioned”. The command below moves columns date_onset and date_hospitalisation to the beginning (left) of the dataset, but keeps all the other columns afterward. Note that everything() is written with empty parentheses:\n\n# move date_onset and date_hospitalisation to beginning\nlinelist %&gt;% \n  select(date_onset, date_hospitalisation, everything()) %&gt;% \n  names()\n\n [1] \"date_onset\"           \"date_hospitalisation\" \"case_id\"             \n [4] \"generation\"           \"date_infection\"       \"date_outcome\"        \n [7] \"outcome\"              \"gender\"               \"age\"                 \n[10] \"age_unit\"             \"age_years\"            \"age_cat\"             \n[13] \"age_cat5\"             \"hospital\"             \"lon\"                 \n[16] \"lat\"                  \"infector\"             \"source\"              \n[19] \"wt_kg\"                \"ht_cm\"                \"ct_blood\"            \n[22] \"fever\"                \"chills\"               \"cough\"               \n[25] \"aches\"                \"vomit\"                \"temp\"                \n[28] \"time_admission\"       \"bmi\"                  \"days_onset_hosp\"     \n\n\nHere are other “tidyselect” helper functions that also work within dplyr functions like select(), across(), and summarise():\n\neverything() - all other columns not mentioned\n\nlast_col() - the last column\n\nwhere() - applies a function to all columns and selects those which are TRUE\n\ncontains() - columns containing a character string\n\nexample: select(contains(\"time\"))\n\n\nstarts_with() - matches to a specified prefix\n\nexample: select(starts_with(\"date_\"))\n\n\nends_with() - matches to a specified suffix\n\nexample: select(ends_with(\"_post\"))\n\n\nmatches() - to apply a regular expression (regex)\n\nexample: select(matches(\"[pt]al\"))\n\n\nnum_range() - a numerical range like x01, x02, x03\n\nany_of() - matches IF column exists but returns no error if it is not found\n\nexample: select(any_of(date_onset, date_death, cardiac_arrest))\n\n\nIn addition, use normal operators such as c() to list several columns, : for consecutive columns, ! for opposite, & for AND, and | for OR.\nUse where() to specify logical criteria for columns. If providing a function inside where(), do not include the function’s empty parentheses. The command below selects columns that are class Numeric.\n\n# select columns that are class Numeric\nlinelist %&gt;% \n  select(where(is.numeric)) %&gt;% \n  names()\n\n [1] \"generation\"      \"age\"             \"age_years\"       \"lon\"            \n [5] \"lat\"             \"wt_kg\"           \"ht_cm\"           \"ct_blood\"       \n [9] \"temp\"            \"bmi\"             \"days_onset_hosp\"\n\n\nUse contains() to select only columns in which the column name contains a specified character string. ends_with() and starts_with() provide more nuance.\n\n# select columns containing certain characters\nlinelist %&gt;% \n  select(contains(\"date\")) %&gt;% \n  names()\n\n[1] \"date_infection\"       \"date_onset\"           \"date_hospitalisation\"\n[4] \"date_outcome\"        \n\n\nThe function matches() works similarly to contains() but can be provided a regular expression, such as multiple strings separated by OR bars within the parentheses:\n\n# searched for multiple character matches\nlinelist %&gt;% \n  select(matches(\"onset|hosp|fev\")) %&gt;%   # note the OR symbol \"|\"\n  names()\n\n[1] \"date_onset\"           \"date_hospitalisation\" \"hospital\"            \n[4] \"fever\"                \"days_onset_hosp\""
  },
  {
    "objectID": "readings/manage-data.html#deduplication",
    "href": "readings/manage-data.html#deduplication",
    "title": "Managing Data",
    "section": "",
    "text": "In a later week we will learn more about how to de-duplicate data. Only a very simple row de-duplication example is presented here.\nThe package dplyr offers the distinct() function. This function examines every row and reduce the data frame to only the unique rows. That is, it removes rows that are 100% duplicates.\nWhen evaluating duplicate rows, it takes into account a range of columns - by default it considers all columns. As shown in the de-duplication page, you can adjust this column range so that the uniqueness of rows is only evaluated in regards to certain columns.\nIn this simple example, we just add the empty command distinct() to the pipe chain. This ensures there are no rows that are 100% duplicates of other rows (evaluated across all columns).\nWe begin with nrow(linelist) rows in linelist.\n\nlinelist &lt;- linelist %&gt;% \n  distinct()\n\nAfter de-duplication there are nrow(linelist) rows. Any removed rows would have been 100% duplicates of other rows."
  },
  {
    "objectID": "readings/manage-data.html#grouping",
    "href": "readings/manage-data.html#grouping",
    "title": "Managing Data",
    "section": "Grouping",
    "text": "Grouping\nThe function group_by() from dplyr groups the rows by the unique values in the column specified to it. If multiple columns are specified, rows are grouped by the unique combinations of values across the columns. Each unique value (or combination of values) constitutes a group. Subsequent changes to the dataset or calculations can then be performed within the context of each group.\nFor example, the command below takes the linelist and groups the rows by unique values in the column outcome, saving the output as a new data frame ll_by_outcome. The grouping column(s) are placed inside the parentheses of the function group_by().\n\nll_by_outcome &lt;- linelist %&gt;% \n  group_by(outcome)\n\nNote that there is no perceptible change to the dataset after running group_by(), until another dplyr verb such as mutate(), summarise(), or arrange() is applied on the “grouped” data frame.\nYou can however “see” the groupings by printing the data frame. When you print a grouped data frame, you will see it has been transformed into a tibble class object which, when printed, displays which groupings have been applied and how many groups there are - written just above the header row.\n\n# print to see which groups are active\nll_by_outcome\n\n# A tibble: 5,888 × 30\n# Groups:   outcome [3]\n   case_id generation date_infection date_onset date_hospitalisation\n   &lt;chr&gt;        &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;     &lt;date&gt;              \n 1 5fe599           4 2014-05-08     2014-05-13 2014-05-15          \n 2 8689b7           4 NA             2014-05-13 2014-05-14          \n 3 11f8ea           2 NA             2014-05-16 2014-05-18          \n 4 b8812a           3 2014-05-04     2014-05-18 2014-05-20          \n 5 893f25           3 2014-05-18     2014-05-21 2014-05-22          \n 6 be99c8           3 2014-05-03     2014-05-22 2014-05-23          \n 7 07e3e8           4 2014-05-22     2014-05-27 2014-05-29          \n 8 369449           4 2014-05-28     2014-06-02 2014-06-03          \n 9 f393b4           4 NA             2014-06-05 2014-06-06          \n10 1389ca           4 NA             2014-06-05 2014-06-07          \n# ℹ 5,878 more rows\n# ℹ 25 more variables: date_outcome &lt;date&gt;, outcome &lt;chr&gt;, gender &lt;chr&gt;,\n#   age &lt;dbl&gt;, age_unit &lt;chr&gt;, age_years &lt;dbl&gt;, age_cat &lt;fct&gt;, age_cat5 &lt;fct&gt;,\n#   hospital &lt;chr&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, infector &lt;chr&gt;, source &lt;chr&gt;,\n#   wt_kg &lt;dbl&gt;, ht_cm &lt;dbl&gt;, ct_blood &lt;dbl&gt;, fever &lt;chr&gt;, chills &lt;chr&gt;,\n#   cough &lt;chr&gt;, aches &lt;chr&gt;, vomit &lt;chr&gt;, temp &lt;dbl&gt;, time_admission &lt;chr&gt;,\n#   bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\n\nUnique groups\nThe groups created reflect each unique combination of values across the grouping columns.\nTo see the groups and the number of rows in each group, pass the grouped data to tally(). To see just the unique groups without counts you can pass to group_keys().\nSee below that there are three unique values in the grouping column outcome: “Death”, “Recover”, and NA. See that there were nrow(linelist %&gt;% filter(outcome == \"Death\")) deaths, nrow(linelist %&gt;% filter(outcome == \"Recover\")) recoveries, and nrow(linelist %&gt;% filter(is.na(outcome))) with no outcome recorded.\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  tally()\n\n# A tibble: 3 × 2\n  outcome     n\n  &lt;chr&gt;   &lt;int&gt;\n1 Death    2582\n2 Recover  1983\n3 &lt;NA&gt;     1323\n\n\nYou can group by more than one column. Below, the data frame is grouped by outcome and gender, and then tallied. Note how each unique combination of outcome and gender is registered as its own group - including missing values for either column.\n\nlinelist %&gt;% \n  group_by(outcome, gender) %&gt;% \n  tally()\n\n# A tibble: 9 × 3\n# Groups:   outcome [3]\n  outcome gender     n\n  &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt;\n1 Death   f       1227\n2 Death   m       1228\n3 Death   &lt;NA&gt;     127\n4 Recover f        953\n5 Recover m        950\n6 Recover &lt;NA&gt;      80\n7 &lt;NA&gt;    f        627\n8 &lt;NA&gt;    m        625\n9 &lt;NA&gt;    &lt;NA&gt;      71\n\n\n\n\nNew columns\nYou can also create a new grouping column within the group_by() statement. This is equivalent to calling mutate() before the group_by(). For a quick tabulation this style can be handy, but for more clarity in your code consider creating this column in its own mutate() step and then piping to group_by().\n\n# group dat based on a binary column created *within* the group_by() command\nlinelist %&gt;% \n  group_by(\n    age_class = ifelse(age &gt;= 18, \"adult\", \"child\")) %&gt;% \n  tally(sort = T)\n\n# A tibble: 3 × 2\n  age_class     n\n  &lt;chr&gt;     &lt;int&gt;\n1 child      3618\n2 adult      2184\n3 &lt;NA&gt;         86\n\n\n\n\nAdd/drop grouping columns\nBy default, if you run group_by() on data that are already grouped, the old groups will be removed and the new one(s) will apply. If you want to add new groups to the existing ones, include the argument .add = TRUE.\n\n# Grouped by outcome\nby_outcome &lt;- linelist %&gt;% \n  group_by(outcome)\n\n# Add grouping by gender in addition\nby_outcome_gender &lt;- by_outcome %&gt;% \n  group_by(gender, .add = TRUE)\n\n** Keep all groups**\nIf you group on a column of class factor there may be levels of the factor that are not currently present in the data. If you group on this column, by default those non-present levels are dropped and not included as groups. To change this so that all levels appear as groups (even if not present in the data), set .drop = FALSE in your group_by() command."
  },
  {
    "objectID": "readings/manage-data.html#un-group",
    "href": "readings/manage-data.html#un-group",
    "title": "Managing Data",
    "section": "Un-group",
    "text": "Un-group\nData that have been grouped will remain grouped until specifically ungrouped via ungroup(). If you forget to ungroup, it can lead to incorrect calculations! Below is an example of removing all groupings:\n\nlinelist %&gt;% \n  group_by(outcome, gender) %&gt;% \n  tally() %&gt;% \n  ungroup()\n\nYou can also remove grouping for only specific columns, by placing the column name inside ungroup().\n\nlinelist %&gt;% \n  group_by(outcome, gender) %&gt;% \n  tally() %&gt;% \n  ungroup(gender) # remove the grouping by gender, leave grouping by outcome\n\n\n\n\n\n\n\nTip\n\n\n\nThe verb count() automatically ungroups the data after counting."
  },
  {
    "objectID": "readings/manage-data.html#group_summarise",
    "href": "readings/manage-data.html#group_summarise",
    "title": "Managing Data",
    "section": "Summarise",
    "text": "Summarise\nThe dplyr function summarise() (or summarize()) takes a data frame and converts it into a new summary data frame, with columns containing summary statistics that you define. On an ungrouped data frame, the summary statistics will be calculated from all rows. Applying summarise() to grouped data produces those summary statistics for each group.\nThe syntax of summarise() is such that you provide the name(s) of the new summary column(s), an equals sign, and then a statistical function to apply to the data, as shown below. For example, min(), max(), median(), or sd(). Within the statistical function, list the column to be operated on and any relevant argument (e.g. na.rm = TRUE). You can use sum() to count the number of rows that meet a logical criteria (with double equals ==).\nBelow is an example of summarise() applied without grouped data. The statistics returned are produced from the entire dataset.\n\n# summary statistics on ungrouped linelist\nlinelist %&gt;% \n  summarise(\n    n_cases  = n(),\n    mean_age = mean(age_years, na.rm=T),\n    max_age  = max(age_years, na.rm=T),\n    min_age  = min(age_years, na.rm=T),\n    n_males  = sum(gender == \"m\", na.rm=T))\n\n  n_cases mean_age max_age min_age n_males\n1    5888 16.01831      84       0    2803\n\n\nIn contrast, below is the same summarise() statement applied to grouped data. The statistics are calculated for each outcome group. Note how grouping columns will carry over into the new data frame.\n\n# summary statistics on grouped linelist\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  summarise(\n    n_cases  = n(),\n    mean_age = mean(age_years, na.rm=T),\n    max_age  = max(age_years, na.rm=T),\n    min_age  = min(age_years, na.rm=T),\n    n_males    = sum(gender == \"m\", na.rm=T))\n\n# A tibble: 3 × 6\n  outcome n_cases mean_age max_age min_age n_males\n  &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;\n1 Death      2582     15.9      76       0    1228\n2 Recover    1983     16.1      84       0     950\n3 &lt;NA&gt;       1323     16.2      69       0     625"
  },
  {
    "objectID": "readings/manage-data.html#counts-and-tallies",
    "href": "readings/manage-data.html#counts-and-tallies",
    "title": "Managing Data",
    "section": "Counts and tallies",
    "text": "Counts and tallies\ncount() and tally() provide similar functionality but are different. Read more about the distinction between tally() and count() here\n\ntally()\ntally() is shorthand for summarise(n = n()), and does not group data. Thus, to achieve grouped tallys it must follow a group_by() command. You can add sort = TRUE to see the largest groups first.\n\nlinelist %&gt;% \n  tally()\n\n     n\n1 5888\n\n\n\nlinelist %&gt;% \n  group_by(outcome) %&gt;% \n  tally(sort = TRUE)\n\n# A tibble: 3 × 2\n  outcome     n\n  &lt;chr&gt;   &lt;int&gt;\n1 Death    2582\n2 Recover  1983\n3 &lt;NA&gt;     1323\n\n\n\n\ncount()\nIn contrast, count() does the following:\n\napplies group_by() on the specified column(s)\n\napplies summarise() and returns column n with the number of rows per group\n\napplies ungroup()\n\n\nlinelist %&gt;% \n  count(outcome)\n\n  outcome    n\n1   Death 2582\n2 Recover 1983\n3    &lt;NA&gt; 1323\n\n\nJust like with group_by() you can create a new column within the count() command:\n\nlinelist %&gt;% \n  count(age_class = ifelse(age &gt;= 18, \"adult\", \"child\"), sort = T)\n\n  age_class    n\n1     child 3618\n2     adult 2184\n3      &lt;NA&gt;   86\n\n\ncount() can be called multiple times, with the functionality “rolling up”. For example, to summarise the number of hospitals present for each gender, run the following. Note, the name of the final column is changed from default “n” for clarity (with name  =).\n\nlinelist %&gt;% \n  # produce counts by unique outcome-gender groups\n  count(gender, hospital) %&gt;% \n  # gather rows by gender (3) and count number of hospitals per gender (6)\n  count(gender, name = \"hospitals per gender\" ) \n\n  gender hospitals per gender\n1      f                    6\n2      m                    6\n3   &lt;NA&gt;                    6\n\n\n\n\nAdd counts\nIn contrast to count() and summarise(), you can use add_count() to add a new column n with the counts of rows per group while retaining all the other data frame columns.\nThis means that a group’s count number, in the new column n, will be printed in each row of the group. For demonstration purposes, we add this column and then re-arrange the columns for easier viewing. See the section below on filter on group size for another example.\n\nlinelist %&gt;% \n  as_tibble() %&gt;%                   # convert to tibble for nicer printing \n  add_count(hospital) %&gt;%           # add column n with counts by hospital\n  select(hospital, n, everything()) # re-arrange for demo purposes\n\n# A tibble: 5,888 × 31\n   hospital                       n case_id generation date_infection date_onset\n   &lt;chr&gt;                      &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;    \n 1 Other                        885 5fe599           4 2014-05-08     2014-05-13\n 2 Missing                     1469 8689b7           4 NA             2014-05-13\n 3 St. Mark's Maternity Hosp…   422 11f8ea           2 NA             2014-05-16\n 4 Port Hospital               1762 b8812a           3 2014-05-04     2014-05-18\n 5 Military Hospital            896 893f25           3 2014-05-18     2014-05-21\n 6 Port Hospital               1762 be99c8           3 2014-05-03     2014-05-22\n 7 Missing                     1469 07e3e8           4 2014-05-22     2014-05-27\n 8 Missing                     1469 369449           4 2014-05-28     2014-06-02\n 9 Missing                     1469 f393b4           4 NA             2014-06-05\n10 Missing                     1469 1389ca           4 NA             2014-06-05\n# ℹ 5,878 more rows\n# ℹ 25 more variables: date_hospitalisation &lt;date&gt;, date_outcome &lt;date&gt;,\n#   outcome &lt;chr&gt;, gender &lt;chr&gt;, age &lt;dbl&gt;, age_unit &lt;chr&gt;, age_years &lt;dbl&gt;,\n#   age_cat &lt;fct&gt;, age_cat5 &lt;fct&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, infector &lt;chr&gt;,\n#   source &lt;chr&gt;, wt_kg &lt;dbl&gt;, ht_cm &lt;dbl&gt;, ct_blood &lt;dbl&gt;, fever &lt;chr&gt;,\n#   chills &lt;chr&gt;, cough &lt;chr&gt;, aches &lt;chr&gt;, vomit &lt;chr&gt;, temp &lt;dbl&gt;,\n#   time_admission &lt;chr&gt;, bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\n\n\nAdd totals\nTo easily add total sum rows or columns after using tally() or count() you can use the tabyl function from janitor. This package also offers functions like adorn_totals() and adorn_percentages() to add totals and convert to show percentages. Below is a brief example:\n\nlinelist %&gt;%                                  # case linelist\n  tabyl(age_cat, gender) %&gt;%                  # cross-tabulate counts of two columns\n  adorn_totals(where = \"row\") %&gt;%             # add a total row\n  adorn_percentages(denominator = \"col\") %&gt;%  # convert to proportions with column denominator\n  adorn_pct_formatting() %&gt;%                  # convert proportions to percents\n  adorn_ns(position = \"front\") %&gt;%            # display as: \"count (percent)\"\n  adorn_title(                                # adjust titles\n    row_name = \"Age Category\",\n    col_name = \"Gender\")\n\n                      Gender                            \n Age Category              f              m          NA_\n          0-4   640  (22.8%)   416  (14.8%)  39  (14.0%)\n          5-9   641  (22.8%)   412  (14.7%)  42  (15.1%)\n        10-14   518  (18.5%)   383  (13.7%)  40  (14.4%)\n        15-19   359  (12.8%)   364  (13.0%)  20   (7.2%)\n        20-29   468  (16.7%)   575  (20.5%)  30  (10.8%)\n        30-49   179   (6.4%)   557  (19.9%)  18   (6.5%)\n        50-69     2   (0.1%)    91   (3.2%)   2   (0.7%)\n          70+     0   (0.0%)     5   (0.2%)   1   (0.4%)\n         &lt;NA&gt;     0   (0.0%)     0   (0.0%)  86  (30.9%)\n        Total 2,807 (100.0%) 2,803 (100.0%) 278 (100.0%)\n\n\n\n\nArranging grouped data\nUsing the dplyr verb arrange() to order the rows in a data frame behaves the same when the data are grouped, unless you set the argument .by_group =TRUE. In this case the rows are ordered first by the grouping columns and then by any other columns you specify to arrange().\n\n\nFilter on grouped data\n\nfilter()\nWhen applied in conjunction with functions that evaluate the data frame (like max(), min(), mean()), these functions will now be applied to the groups. For example, if you want to filter and keep rows where patients are above the median age, this will now apply per group - filtering to keep rows above the group’s median age.\n\n\nSlice rows per group\nThe dplyr function slice(), which filters rows based on their position in the data, can also be applied per group. Remember to account for sorting the data within each group to get the desired “slice”.\nFor example, to retrieve only the latest 5 admissions from each hospital:\n\nGroup the linelist by column hospital\n\nArrange the records from latest to earliest date_hospitalisation within each hospital group\n\nSlice to retrieve the first 5 rows from each hospital\n\n\nlinelist %&gt;%\n  group_by(hospital) %&gt;%\n  arrange(hospital, date_hospitalisation) %&gt;%\n  slice_head(n = 5) %&gt;% \n  arrange(hospital) %&gt;%                            # for display\n  select(case_id, hospital, date_hospitalisation)  # for display\n\n# A tibble: 30 × 3\n# Groups:   hospital [6]\n   case_id hospital          date_hospitalisation\n   &lt;chr&gt;   &lt;chr&gt;             &lt;date&gt;              \n 1 20b688  Central Hospital  2014-05-06          \n 2 d58402  Central Hospital  2014-05-10          \n 3 b8f2fd  Central Hospital  2014-05-13          \n 4 acf422  Central Hospital  2014-05-28          \n 5 275cc7  Central Hospital  2014-05-28          \n 6 d1fafd  Military Hospital 2014-04-17          \n 7 974bc1  Military Hospital 2014-05-13          \n 8 6a9004  Military Hospital 2014-05-13          \n 9 09e386  Military Hospital 2014-05-14          \n10 865581  Military Hospital 2014-05-15          \n# ℹ 20 more rows\n\n\nslice_head() - selects n rows from the top\nslice_tail() - selects n rows from the end\nslice_sample() - randomly selects n rows\nslice_min() - selects n rows with highest values in order_by = column, use with_ties = TRUE to keep ties\nslice_max() - selects n rows with lowest values in order_by = column, use with_ties = TRUE to keep ties\n\n\nFilter on group size\nThe function add_count() adds a column n to the original data giving the number of rows in that row’s group.\nShown below, add_count() is applied to the column hospital, so the values in the new column n reflect the number of rows in that row’s hospital group. Note how values in column n are repeated. In the example below, the column name n could be changed using name = within add_count(). For demonstration purposes we re-arrange the columns with select().\n\nlinelist %&gt;% \n  as_tibble() %&gt;% \n  add_count(hospital) %&gt;%          # add \"number of rows admitted to same hospital as this row\" \n  select(hospital, n, everything())\n\n# A tibble: 5,888 × 31\n   hospital                       n case_id generation date_infection date_onset\n   &lt;chr&gt;                      &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;date&gt;         &lt;date&gt;    \n 1 Other                        885 5fe599           4 2014-05-08     2014-05-13\n 2 Missing                     1469 8689b7           4 NA             2014-05-13\n 3 St. Mark's Maternity Hosp…   422 11f8ea           2 NA             2014-05-16\n 4 Port Hospital               1762 b8812a           3 2014-05-04     2014-05-18\n 5 Military Hospital            896 893f25           3 2014-05-18     2014-05-21\n 6 Port Hospital               1762 be99c8           3 2014-05-03     2014-05-22\n 7 Missing                     1469 07e3e8           4 2014-05-22     2014-05-27\n 8 Missing                     1469 369449           4 2014-05-28     2014-06-02\n 9 Missing                     1469 f393b4           4 NA             2014-06-05\n10 Missing                     1469 1389ca           4 NA             2014-06-05\n# ℹ 5,878 more rows\n# ℹ 25 more variables: date_hospitalisation &lt;date&gt;, date_outcome &lt;date&gt;,\n#   outcome &lt;chr&gt;, gender &lt;chr&gt;, age &lt;dbl&gt;, age_unit &lt;chr&gt;, age_years &lt;dbl&gt;,\n#   age_cat &lt;fct&gt;, age_cat5 &lt;fct&gt;, lon &lt;dbl&gt;, lat &lt;dbl&gt;, infector &lt;chr&gt;,\n#   source &lt;chr&gt;, wt_kg &lt;dbl&gt;, ht_cm &lt;dbl&gt;, ct_blood &lt;dbl&gt;, fever &lt;chr&gt;,\n#   chills &lt;chr&gt;, cough &lt;chr&gt;, aches &lt;chr&gt;, vomit &lt;chr&gt;, temp &lt;dbl&gt;,\n#   time_admission &lt;chr&gt;, bmi &lt;dbl&gt;, days_onset_hosp &lt;dbl&gt;\n\n\nIt then becomes easy to filter for case rows who were hospitalized at a “small” hospital, say, a hospital that admitted fewer than 500 patients:\n\nlinelist %&gt;% \n  add_count(hospital) %&gt;% \n  filter(n &lt; 500)\n\n\n\n\nMutate on grouped data\nTo retain all columns and rows (not summarise) and add a new column containing group statistics, use mutate() after group_by() instead of summarise().\nThis is useful if you want group statistics in the original dataset with all other columns present - e.g. for calculations that compare one row to its group.\nFor example, this code below calculates the difference between a row’s delay-to-admission and the median delay for their hospital. The steps are:\n\nGroup the data by hospital\n\nUse the column days_onset_hosp (delay to hospitalisation) to create a new column containing the mean delay at the hospital of that row\n\nCalculate the difference between the two columns\n\nWe select() only certain columns to display, for demonstration purposes.\n\nlinelist %&gt;% \n  # group data by hospital (no change to linelist yet)\n  group_by(hospital) %&gt;% \n  \n  # new columns\n  mutate(\n    # mean days to admission per hospital (rounded to 1 decimal)\n    group_delay_admit = round(mean(days_onset_hosp, na.rm=T), 1),\n    \n    # difference between row's delay and mean delay at their hospital (rounded to 1 decimal)\n    diff_to_group     = round(days_onset_hosp - group_delay_admit, 1)) %&gt;%\n  \n  # select certain rows only - for demonstration/viewing purposes\n  select(case_id, hospital, days_onset_hosp, group_delay_admit, diff_to_group)\n\n# A tibble: 5,888 × 5\n# Groups:   hospital [6]\n   case_id hospital              days_onset_hosp group_delay_admit diff_to_group\n   &lt;chr&gt;   &lt;chr&gt;                           &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;\n 1 5fe599  Other                               2               2             0  \n 2 8689b7  Missing                             1               2.1          -1.1\n 3 11f8ea  St. Mark's Maternity…               2               2.1          -0.1\n 4 b8812a  Port Hospital                       2               2.1          -0.1\n 5 893f25  Military Hospital                   1               2.1          -1.1\n 6 be99c8  Port Hospital                       1               2.1          -1.1\n 7 07e3e8  Missing                             2               2.1          -0.1\n 8 369449  Missing                             1               2.1          -1.1\n 9 f393b4  Missing                             1               2.1          -1.1\n10 1389ca  Missing                             2               2.1          -0.1\n# ℹ 5,878 more rows\n\n\n\n\nSelect on grouped data\nThe verb select() works on grouped data, but the grouping columns are always included (even if not mentioned in select()). If you do not want these grouping columns, use ungroup() first."
  },
  {
    "objectID": "readings/manage-data.html#appending-datasets",
    "href": "readings/manage-data.html#appending-datasets",
    "title": "Managing Data",
    "section": "Appending Datasets",
    "text": "Appending Datasets\nWe often need to combine multiple sources of data. Later on we’ll see more complex methods for combining data based on matching ID’s or other values. However, to start we’ll look at the case where we want to add additional observations to a dataset. You can also think of this as “appending” or “adding” rows.\n\nBind rows\nTo bind rows of one data frame to the bottom of another data frame, use bind_rows() from dplyr. It is very inclusive, so any column present in either data frame will be included in the output. A few notes:\n\nUnlike the base R version row.bind(), dplyr’s bind_rows() does not require that the order of columns be the same in both data frames. As long as the column names are spelled identically, it will align them correctly.\n\nYou can optionally specify the argument .id =. Provide a character column name. This will produce a new column that serves to identify which data frame each row originally came from.\n\nYou can use bind_rows() on a list of similarly-structured data frames to combine them into one data frame.\n\nOne common example of row binding is to bind a “total” row onto a descriptive table made with dplyr’s summarise() function. Below we create a table of case counts and median CT values by hospital with a total row.\nThe function summarise() is used on data grouped by hospital to return a summary data frame by hospital. But the function summarise() does not automatically produce a “totals” row, so we create it by summarising the data again, but with the data not grouped by hospital. This produces a second data frame of just one row. We can then bind these data frames together to achieve the final table.\n\n# Create core table\n###################\nhosp_summary &lt;- linelist %&gt;% \n  group_by(hospital) %&gt;%                        # Group data by hospital\n  summarise(                                    # Create new summary columns of indicators of interest\n    cases = n(),                                  # Number of rows per hospital-outcome group     \n    ct_value_med = median(ct_blood, na.rm=T))     # median CT value per group\n\nHere is the hosp_summary data frame:\n\n\n\n\n\n\n\nCreate a data frame with the “total” statistics (not grouped by hospital). This will return just one row.\n\n# create totals\n###############\ntotals &lt;- linelist %&gt;% \n  summarise(\n    cases = n(),                               # Number of rows for whole dataset     \n    ct_value_med = median(ct_blood, na.rm=T))  # Median CT for whole dataset\n\nAnd below is that totals data frame. Note how there are only two columns. These columns are also in hosp_summary, but there is one column in hosp_summary that is not in totals (hospital).\n\n\n\n\n\n\n\n  cases ct_value_med\n1  5888           22\n\n\nNow we can bind the rows together with bind_rows().\n\n# Bind data frames together\ncombined &lt;- bind_rows(hosp_summary, totals)\n\nNow we can view the result. See how in the final row, an empty NA value fills in for the column hospital that was not in hosp_summary.\n\n\n\n\n\n\n\n# A tibble: 7 × 3\n  hospital                             cases ct_value_med\n  &lt;chr&gt;                                &lt;int&gt;        &lt;dbl&gt;\n1 Central Hospital                       454           22\n2 Military Hospital                      896           21\n3 Missing                               1469           21\n4 Other                                  885           22\n5 Port Hospital                         1762           22\n6 St. Mark's Maternity Hospital (SMMH)   422           22\n7 &lt;NA&gt;                                  5888           22"
  },
  {
    "objectID": "readings/pivoting.html",
    "href": "readings/pivoting.html",
    "title": "Pivoting data",
    "section": "",
    "text": "::: callout-tip ## Extended Materials\nYou can find the original, extended version of this chapter here. ::\n\n\n\n\n\n\n\n\n\nWhen managing data, pivoting can be understood to refer to one of two processes:\n\nThe creation of pivot tables, which are tables of statistics that summarise the data of a more extensive table\n\nThe conversion of a table from long to wide format, or vice versa.\n\nIn this page, we will focus on the latter definition. This page discusses the formats of data. It is useful to be aware of the idea of “tidy data”, in which each variable has it’s own column, each observation has it’s own row, and each value has it’s own cell. More about this topic can be found at this online chapter in R for Data Science.\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\npacman::p_load(\n  rio,          # File import\n  here,         # File locator\n  tidyverse)    # data management + ggplot2 graphics\n\n\n\n\n\n\n\nIn this page, we will use a fictional dataset of daily malaria cases, by facility and age group. If you want to follow along, click here to download (as .rds file).\n\n# Import data\ncount_data &lt;- import(\"malaria_facility_count_data.rds\")\n\nThe first 50 rows are displayed below.\n\n\n\n\n\n\n\n\n\n\nIn the later part of this page, we will also use the dataset of cases from a simulated Ebola epidemic. If you want to follow along, click to download the “clean” linelist (as .rds file).\n\n# import your dataset\nlinelist &lt;- import(\"linelist_cleaned.xlsx\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData are often entered and stored in a “wide” format - where a subject’s characteristics or responses are stored in a single row. While this may be useful for presentation, it is not ideal for some types of analysis.\nLet us take the count_data dataset imported in the Preparation section above as an example. You can see that each row represents a “facility-day”. The actual case counts (the right-most columns) are stored in a “wide” format such that the information for every age group on a given facility-day is stored in a single row.\n\n\n\n\n\n\n\nEach observation in this dataset refers to the malaria counts at one of 65 facilities on a given date, ranging from count_data$data_date %&gt;% min() to count_data$data_date %&gt;% max(). These facilities are located in one Province (North) and four Districts (Spring, Bolo, Dingo, and Barnard). The dataset provides the overall counts of malaria, as well as age-specific counts in each of three age groups - &lt;4 years, 5-14 years, and 15 years and older.\n“Wide” data like this are not adhering to “tidy data” standards, because the column headers do not actually represent “variables” - they represent values of a hypothetical “age group” variable.\nThis format can be useful for presenting the information in a table, or for entering data (e.g. in Excel) from case report forms. However, in the analysis stage, these data typically should be transformed to a “longer” format more aligned with “tidy data” standards. The plotting R package ggplot2 in particular works best when data are in a “long” format.\nVisualising the total malaria counts over time poses no difficulty with the data in it’s current format:\n\nggplot(count_data) +\n  geom_col(aes(x = data_date, y = malaria_tot), width = 1)\n\n\n\n\nHowever, what if we wanted to display the relative contributions of each age group to this total count? In this case, we need to ensure that the variable of interest (age group), appears in the dataset in a single column that can be passed to {ggplot2}’s “mapping aesthetics” aes() argument.\n\n\n\n\nThe tidyr function pivot_longer() makes data “longer”. tidyr is part of the tidyverse of R packages.\nIt accepts a range of columns to transform (specified to cols =). Therefore, it can operate on only a part of a dataset. This is useful for the malaria data, as we only want to pivot the case count columns.\nIn this process, you will end up with two “new” columns - one with the categories (the former column names), and one with the corresponding values (e.g. case counts). You can accept the default names for these new columns, or you can specify your own to names_to = and values_to = respectively.\nLet’s see pivot_longer() in action…\n\n\n\nWe want to use tidyr’s pivot_longer() function to convert the “wide” data to a “long” format. Specifically, to convert the four numeric columns with data on malaria counts to two new columns: one which holds the age groups and one which holds the corresponding values.\n\ndf_long &lt;- count_data %&gt;% \n  pivot_longer(\n    cols = c(`malaria_rdt_0-4`, `malaria_rdt_5-14`, `malaria_rdt_15`, `malaria_tot`)\n  )\n\ndf_long\n\nNotice that the newly created data frame (df_long) has more rows (12,152 vs 3,038); it has become longer. In fact, it is precisely four times as long, because each row in the original dataset now represents four rows in df_long, one for each of the malaria count observations (&lt;4y, 5-14y, 15y+, and total).\nIn addition to becoming longer, the new dataset has fewer columns (8 vs 10), as the data previously stored in four columns (those beginning with the prefix malaria_) is now stored in two.\nSince the names of these four columns all begin with the prefix malaria_, we could have made use of the handy “tidyselect” function starts_with() to achieve the same result.\n\n# provide column with a tidyselect helper function\ncount_data %&gt;% \n  pivot_longer(\n    cols = starts_with(\"malaria_\")\n  )\n\n# A tibble: 12,152 × 8\n   location_name data_date  submitted_date Province District newid name    value\n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;int&gt;\n 1 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    11\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    12\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    23\n 4 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    46\n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    11\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    10\n 7 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…     5\n 8 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    26\n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malari…     8\n10 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malari…     5\n# ℹ 12,142 more rows\n\n\nor by position:\n\n# provide columns by position\ncount_data %&gt;% \n  pivot_longer(\n    cols = 6:9\n  )\n\nor by named range:\n\n# provide range of consecutive columns\ncount_data %&gt;% \n  pivot_longer(\n    cols = `malaria_rdt_0-4`:malaria_tot\n  )\n\nThese two new columns are given the default names of name and value, but we can override these defaults to provide more meaningful names, which can help remember what is stored within, using the names_to and values_to arguments. Let’s use the names age_group and counts:\n\ndf_long &lt;- \n  count_data %&gt;% \n  pivot_longer(\n    cols = starts_with(\"malaria_\"),\n    names_to = \"age_group\",\n    values_to = \"counts\"\n  )\n\ndf_long\n\n# A tibble: 12,152 × 8\n   location_name data_date  submitted_date Province District newid age_group    \n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;        \n 1 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 4 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_tot  \n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 7 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 8 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_tot  \n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malaria_rdt_…\n10 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malaria_rdt_…\n# ℹ 12,142 more rows\n# ℹ 1 more variable: counts &lt;int&gt;\n\n\nWe can now pass this new dataset to {ggplot2}, and map the new column count to the y-axis and new column age_group to the fill = argument (the column internal color). This will display the malaria counts in a stacked bar chart, by age group:\n\nggplot(data = df_long) +\n  geom_col(\n    mapping = aes(x = data_date, y = counts, fill = age_group),\n    width = 1\n  )\n\n\n\n\nExamine this new plot, and compare it with the plot we created earlier - what has gone wrong?\nWe have encountered a common problem when wrangling surveillance data - we have also included the total counts from the malaria_tot column, so the magnitude of each bar in the plot is twice as high as it should be.\nWe can handle this in a number of ways. We could simply filter these totals from the dataset before we pass it to ggplot():\n\ndf_long %&gt;% \n  filter(age_group != \"malaria_tot\") %&gt;% \n  ggplot() +\n  geom_col(\n    aes(x = data_date, y = counts, fill = age_group),\n    width = 1\n  )\n\n\n\n\nAlternatively, we could have excluded this variable when we ran pivot_longer(), thereby maintaining it in the dataset as a separate variable. See how its values “expand” to fill the new rows.\n\ncount_data %&gt;% \n  pivot_longer(\n    cols = `malaria_rdt_0-4`:malaria_rdt_15,   # does not include the totals column\n    names_to = \"age_group\",\n    values_to = \"counts\"\n  )\n\n# A tibble: 9,114 × 9\n   location_name data_date  submitted_date Province District malaria_tot newid\n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;          &lt;int&gt; &lt;int&gt;\n 1 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 4 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 7 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n 8 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n10 Facility 4    2020-08-11 2020-08-12     North    Bolo              49     4\n# ℹ 9,104 more rows\n# ℹ 2 more variables: age_group &lt;chr&gt;, counts &lt;int&gt;\n\n\n\n\n\nThe above example works well in situations in which all the columns you want to “pivot longer” are of the same class (character, numeric, logical…).\nHowever, there will be many cases when, as a field epidemiologist, you will be working with data that was prepared by non-specialists and which follow their own non-standard logic - as Hadley Wickham noted (referencing Tolstoy) in his seminal article on Tidy Data principles: “Like families, tidy datasets are all alike but every messy dataset is messy in its own way.”\nOne particularly common problem you will encounter will be the need to pivot columns that contain different classes of data. This pivot will result in storing these different data types in a single column, which is not a good situation. There are various approaches one can take to separate out the mess this creates, but there is an important step you can take using pivot_longer() to avoid creating such a situation yourself.\nTake a situation in which there have been a series of observations at different time steps for each of three items A, B and C. Examples of such items could be individuals (e.g. contacts of an Ebola case being traced each day for 21 days) or remote village health posts being monitored once per year to ensure they are still functional. Let’s use the contact tracing example. Imagine that the data are stored as follows:\n\n\n\n\n\n\n\nAs can be seen, the data are a bit complicated. Each row stores information about one item, but with the time series running further and further away to the right as time progresses. Moreover, the column classes alternate between date and character values.\nOne particularly bad example of this encountered by this author involved cholera surveillance data, in which 8 new columns of observations were added each day over the course of 4 years. Simply opening the Excel file in which these data were stored took &gt;10 minuntes on my laptop!\nIn order to work with these data, we need to transform the data frame to long format, but keeping the separation between a date column and a character (status) column, for each observation for each item. If we don’t, we might end up with a mixture of variable types in a single column (a very big “no-no” when it comes to data management and tidy data):\n\ndf %&gt;% \n  pivot_longer(\n    cols = -id,\n    names_to = c(\"observation\")\n  )\n\n# A tibble: 18 × 3\n   id    observation value     \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     \n 1 A     obs1_date   2021-04-23\n 2 A     obs1_status Healthy   \n 3 A     obs2_date   2021-04-24\n 4 A     obs2_status Healthy   \n 5 A     obs3_date   2021-04-25\n 6 A     obs3_status Unwell    \n 7 B     obs1_date   2021-04-23\n 8 B     obs1_status Healthy   \n 9 B     obs2_date   2021-04-24\n10 B     obs2_status Healthy   \n11 B     obs3_date   2021-04-25\n12 B     obs3_status Healthy   \n13 C     obs1_date   2021-04-23\n14 C     obs1_status Missing   \n15 C     obs2_date   2021-04-24\n16 C     obs2_status Healthy   \n17 C     obs3_date   2021-04-25\n18 C     obs3_status Healthy   \n\n\nAbove, our pivot has merged dates and characters into a single value column. R will react by converting the entire column to class character, and the utility of the dates is lost.\nTo prevent this situation, we can take advantage of the syntax structure of the original column names. There is a common naming structure, with the observation number, an underscore, and then either “status” or “date”. We can leverage this syntax to keep these two data types in separate columns after the pivot.\nWe do this by:\n\nProviding a character vector to the names_to = argument, with the second item being (\".value\" ). This special term indicates that the pivoted columns will be split based on a character in their name…\n\nYou must also provide the “splitting” character to the names_sep = argument. In this case, it is the underscore “_“.\n\nThus, the naming and split of new columns is based around the underscore in the existing variable names.\n\ndf_long &lt;- \n  df %&gt;% \n  pivot_longer(\n    cols = -id,\n    names_to = c(\"observation\", \".value\"),\n    names_sep = \"_\"\n  )\n\ndf_long\n\n# A tibble: 9 × 4\n  id    observation date       status \n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;  \n1 A     obs1        2021-04-23 Healthy\n2 A     obs2        2021-04-24 Healthy\n3 A     obs3        2021-04-25 Unwell \n4 B     obs1        2021-04-23 Healthy\n5 B     obs2        2021-04-24 Healthy\n6 B     obs3        2021-04-25 Healthy\n7 C     obs1        2021-04-23 Missing\n8 C     obs2        2021-04-24 Healthy\n9 C     obs3        2021-04-25 Healthy\n\n\nFinishing touches:\nNote that the date column is currently in character class - we can easily convert this into it’s proper date class using the mutate() and as_date() functions.\nWe may also want to convert the observation column to a numeric format by dropping the “obs” prefix and converting to numeric. We cando this with str_remove_all() from the stringr package.\n\ndf_long &lt;- \n  df_long %&gt;% \n  mutate(\n    date = date %&gt;% lubridate::as_date(),\n    observation = \n      observation %&gt;% \n      str_remove_all(\"obs\") %&gt;% \n      as.numeric()\n  )\n\ndf_long\n\n# A tibble: 9 × 4\n  id    observation date       status \n  &lt;chr&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;  \n1 A               1 2021-04-23 Healthy\n2 A               2 2021-04-24 Healthy\n3 A               3 2021-04-25 Unwell \n4 B               1 2021-04-23 Healthy\n5 B               2 2021-04-24 Healthy\n6 B               3 2021-04-25 Healthy\n7 C               1 2021-04-23 Missing\n8 C               2 2021-04-24 Healthy\n9 C               3 2021-04-25 Healthy\n\n\nAnd now, we can start to work with the data in this format, e.g. by plotting a descriptive heat tile:\n\nggplot(data = df_long, mapping = aes(x = date, y = id, fill = status)) +\n  geom_tile(colour = \"black\") +\n  scale_fill_manual(\n    values = \n      c(\"Healthy\" = \"lightgreen\", \n        \"Unwell\" = \"red\", \n        \"Missing\" = \"orange\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn some instances, we may wish to convert a dataset to a wider format. For this, we can use the pivot_wider() function.\nA typical use-case is when we want to transform the results of an analysis into a format which is more digestible for the reader (such as a [Table for presentation][Tables for presentation]). Usually, this involves transforming a dataset in which information for one subject is are spread over multiple rows into a format in which that information is stored in a single row.\n\n\nFor this section of the page, we will use the case linelist (see the Preparation section), which contains one row per case.\nHere are the first 50 rows:\n\n\n\n\n\n\n\nSuppose that we want to know the counts of individuals in the different age groups, by gender:\n\ndf_wide &lt;- \n  linelist %&gt;% \n  count(age_cat, gender)\n\ndf_wide\n\n   age_cat gender   n\n1      0-4      f 640\n2      0-4      m 416\n3      0-4   &lt;NA&gt;  39\n4      5-9      f 641\n5      5-9      m 412\n6      5-9   &lt;NA&gt;  42\n7    10-14      f 518\n8    10-14      m 383\n9    10-14   &lt;NA&gt;  40\n10   15-19      f 359\n11   15-19      m 364\n12   15-19   &lt;NA&gt;  20\n13   20-29      f 468\n14   20-29      m 575\n15   20-29   &lt;NA&gt;  30\n16   30-49      f 179\n17   30-49      m 557\n18   30-49   &lt;NA&gt;  18\n19   50-69      f   2\n20   50-69      m  91\n21   50-69   &lt;NA&gt;   2\n22     70+      m   5\n23     70+   &lt;NA&gt;   1\n24    &lt;NA&gt;   &lt;NA&gt;  86\n\n\nThis gives us a long dataset that is great for producing visualisations in ggplot2, but not ideal for presentation in a table:\n\nggplot(df_wide) +\n  geom_col(aes(x = age_cat, y = n, fill = gender))\n\n\n\n\n\n\n\nTherefore, we can use pivot_wider() to transform the data into a better format for inclusion as tables in our reports.\nThe argument names_from specifies the column from which to generate the new column names, while the argument values_from specifies the column from which to take the values to populate the cells. The argument id_cols = is optional, but can be provided a vector of column names that should not be pivoted, and will thus identify each row.\n\ntable_wide &lt;- \n  df_wide %&gt;% \n  pivot_wider(\n    id_cols = age_cat,\n    names_from = gender,\n    values_from = n\n  )\n\ntable_wide\n\n# A tibble: 9 × 4\n  age_cat     f     m  `NA`\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 0-4       640   416    39\n2 5-9       641   412    42\n3 10-14     518   383    40\n4 15-19     359   364    20\n5 20-29     468   575    30\n6 30-49     179   557    18\n7 50-69       2    91     2\n8 70+        NA     5     1\n9 &lt;NA&gt;       NA    NA    86\n\n\nThis table is much more reader-friendly, and therefore better for inclusion in our reports. You can convert into a pretty table with several packages including flextable and knitr.\n\ntable_wide %&gt;% \n  janitor::adorn_totals(c(\"row\", \"col\")) %&gt;% # adds row and column totals\n  knitr::kable() %&gt;% \n  kableExtra::row_spec(row = 10, bold = TRUE) %&gt;% \n  kableExtra::column_spec(column = 5, bold = TRUE) \n\n\n\n\nage_cat\nf\nm\nNA\nTotal\n\n\n\n\n0-4\n640\n416\n39\n1095\n\n\n5-9\n641\n412\n42\n1095\n\n\n10-14\n518\n383\n40\n941\n\n\n15-19\n359\n364\n20\n743\n\n\n20-29\n468\n575\n30\n1073\n\n\n30-49\n179\n557\n18\n754\n\n\n50-69\n2\n91\n2\n95\n\n\n70+\nNA\n5\n1\n6\n\n\nNA\nNA\nNA\n86\n86\n\n\nTotal\n2807\n2803\n278\n5888\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn some situations after a pivot, and more commonly after a bind, we are left with gaps in some cells that we would like to fill.\n\n\n\nFor example, take two datasets, each with observations for the measurement number, the name of the facility, and the case count at that time. However, the second dataset also has a variable Year.\n\ndf1 &lt;- \n  tibble::tribble(\n       ~Measurement, ~Facility, ~Cases,\n                  1,  \"Hosp 1\",     66,\n                  2,  \"Hosp 1\",     26,\n                  3,  \"Hosp 1\",      8,\n                  1,  \"Hosp 2\",     71,\n                  2,  \"Hosp 2\",     62,\n                  3,  \"Hosp 2\",     70,\n                  1,  \"Hosp 3\",     47,\n                  2,  \"Hosp 3\",     70,\n                  3,  \"Hosp 3\",     38,\n       )\n\ndf1 \n\n# A tibble: 9 × 3\n  Measurement Facility Cases\n        &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1           1 Hosp 1      66\n2           2 Hosp 1      26\n3           3 Hosp 1       8\n4           1 Hosp 2      71\n5           2 Hosp 2      62\n6           3 Hosp 2      70\n7           1 Hosp 3      47\n8           2 Hosp 3      70\n9           3 Hosp 3      38\n\ndf2 &lt;- \n  tibble::tribble(\n    ~Year, ~Measurement, ~Facility, ~Cases,\n     2000,            1,  \"Hosp 4\",     82,\n     2001,            2,  \"Hosp 4\",     87,\n     2002,            3,  \"Hosp 4\",     46\n  )\n\ndf2\n\n# A tibble: 3 × 4\n   Year Measurement Facility Cases\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1  2000           1 Hosp 4      82\n2  2001           2 Hosp 4      87\n3  2002           3 Hosp 4      46\n\n\nWhen we perform a bind_rows() to join the two datasets together, the Year variable is filled with NA for those rows where there was no prior information (i.e. the first dataset):\n\ndf_combined &lt;- \n  bind_rows(df1, df2) %&gt;% \n  arrange(Measurement, Facility)\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 1      66    NA\n 2           1 Hosp 2      71    NA\n 3           1 Hosp 3      47    NA\n 4           1 Hosp 4      82  2000\n 5           2 Hosp 1      26    NA\n 6           2 Hosp 2      62    NA\n 7           2 Hosp 3      70    NA\n 8           2 Hosp 4      87  2001\n 9           3 Hosp 1       8    NA\n10           3 Hosp 2      70    NA\n11           3 Hosp 3      38    NA\n12           3 Hosp 4      46  2002\n\n\n\n\n\n\nIn this case, Year is a useful variable to include, particularly if we want to explore trends over time. Therefore, we use fill() to fill in those empty cells, by specifying the column to fill and the direction (in this case up):\n\ndf_combined %&gt;% \n  fill(Year, .direction = \"up\")\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 1      66  2000\n 2           1 Hosp 2      71  2000\n 3           1 Hosp 3      47  2000\n 4           1 Hosp 4      82  2000\n 5           2 Hosp 1      26  2001\n 6           2 Hosp 2      62  2001\n 7           2 Hosp 3      70  2001\n 8           2 Hosp 4      87  2001\n 9           3 Hosp 1       8  2002\n10           3 Hosp 2      70  2002\n11           3 Hosp 3      38  2002\n12           3 Hosp 4      46  2002\n\n\nAlternatively, we can rearrange the data so that we would need to fill in a downward direction:\n\ndf_combined &lt;- \n  df_combined %&gt;% \n  arrange(Measurement, desc(Facility))\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 4      82  2000\n 2           1 Hosp 3      47    NA\n 3           1 Hosp 2      71    NA\n 4           1 Hosp 1      66    NA\n 5           2 Hosp 4      87  2001\n 6           2 Hosp 3      70    NA\n 7           2 Hosp 2      62    NA\n 8           2 Hosp 1      26    NA\n 9           3 Hosp 4      46  2002\n10           3 Hosp 3      38    NA\n11           3 Hosp 2      70    NA\n12           3 Hosp 1       8    NA\n\ndf_combined &lt;- \n  df_combined %&gt;% \n  fill(Year, .direction = \"down\")\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 4      82  2000\n 2           1 Hosp 3      47  2000\n 3           1 Hosp 2      71  2000\n 4           1 Hosp 1      66  2000\n 5           2 Hosp 4      87  2001\n 6           2 Hosp 3      70  2001\n 7           2 Hosp 2      62  2001\n 8           2 Hosp 1      26  2001\n 9           3 Hosp 4      46  2002\n10           3 Hosp 3      38  2002\n11           3 Hosp 2      70  2002\n12           3 Hosp 1       8  2002\n\n\nWe now have a useful dataset for plotting:\n\nggplot(df_combined) +\n  aes(Year, Cases, fill = Facility) +\n  geom_col()\n\n\n\n\nBut less useful for presenting in a table, so let’s practice converting this long, untidy dataframe into a wider, tidy dataframe:\n\ndf_combined %&gt;% \n  pivot_wider(\n    id_cols = c(Measurement, Facility),\n    names_from = \"Year\",\n    values_from = \"Cases\"\n  ) %&gt;% \n  arrange(Facility) %&gt;% \n  janitor::adorn_totals(c(\"row\", \"col\")) %&gt;% \n  knitr::kable() %&gt;% \n  kableExtra::row_spec(row = 5, bold = TRUE) %&gt;% \n  kableExtra::column_spec(column = 5, bold = TRUE) \n\n\n\n\nMeasurement\nFacility\n2000\n2001\n2002\nTotal\n\n\n\n\n1\nHosp 1\n66\nNA\nNA\n66\n\n\n2\nHosp 1\nNA\n26\nNA\n26\n\n\n3\nHosp 1\nNA\nNA\n8\n8\n\n\n1\nHosp 2\n71\nNA\nNA\n71\n\n\n2\nHosp 2\nNA\n62\nNA\n62\n\n\n3\nHosp 2\nNA\nNA\n70\n70\n\n\n1\nHosp 3\n47\nNA\nNA\n47\n\n\n2\nHosp 3\nNA\n70\nNA\n70\n\n\n3\nHosp 3\nNA\nNA\n38\n38\n\n\n1\nHosp 4\n82\nNA\nNA\n82\n\n\n2\nHosp 4\nNA\n87\nNA\n87\n\n\n3\nHosp 4\nNA\nNA\n46\n46\n\n\nTotal\n-\n266\n245\n162\n673\n\n\n\n\n\n\n\nN.B. In this case, we had to specify to only include the three variables Facility, Year, and Cases as the additional variable Measurement would interfere with the creation of the table:\n\ndf_combined %&gt;% \n  pivot_wider(\n    names_from = \"Year\",\n    values_from = \"Cases\"\n  ) %&gt;% \n  knitr::kable()\n\n\n\n\nMeasurement\nFacility\n2000\n2001\n2002\n\n\n\n\n1\nHosp 4\n82\nNA\nNA\n\n\n1\nHosp 3\n47\nNA\nNA\n\n\n1\nHosp 2\n71\nNA\nNA\n\n\n1\nHosp 1\n66\nNA\nNA\n\n\n2\nHosp 4\nNA\n87\nNA\n\n\n2\nHosp 3\nNA\n70\nNA\n\n\n2\nHosp 2\nNA\n62\nNA\n\n\n2\nHosp 1\nNA\n26\nNA\n\n\n3\nHosp 4\nNA\nNA\n46\n\n\n3\nHosp 3\nNA\nNA\n38\n\n\n3\nHosp 2\nNA\nNA\n70\n\n\n3\nHosp 1\nNA\nNA\n8"
  },
  {
    "objectID": "readings/pivoting.html#wide-to-long",
    "href": "readings/pivoting.html#wide-to-long",
    "title": "Pivoting data",
    "section": "",
    "text": "Data are often entered and stored in a “wide” format - where a subject’s characteristics or responses are stored in a single row. While this may be useful for presentation, it is not ideal for some types of analysis.\nLet us take the count_data dataset imported in the Preparation section above as an example. You can see that each row represents a “facility-day”. The actual case counts (the right-most columns) are stored in a “wide” format such that the information for every age group on a given facility-day is stored in a single row.\n\n\n\n\n\n\n\nEach observation in this dataset refers to the malaria counts at one of 65 facilities on a given date, ranging from count_data$data_date %&gt;% min() to count_data$data_date %&gt;% max(). These facilities are located in one Province (North) and four Districts (Spring, Bolo, Dingo, and Barnard). The dataset provides the overall counts of malaria, as well as age-specific counts in each of three age groups - &lt;4 years, 5-14 years, and 15 years and older.\n“Wide” data like this are not adhering to “tidy data” standards, because the column headers do not actually represent “variables” - they represent values of a hypothetical “age group” variable.\nThis format can be useful for presenting the information in a table, or for entering data (e.g. in Excel) from case report forms. However, in the analysis stage, these data typically should be transformed to a “longer” format more aligned with “tidy data” standards. The plotting R package ggplot2 in particular works best when data are in a “long” format.\nVisualising the total malaria counts over time poses no difficulty with the data in it’s current format:\n\nggplot(count_data) +\n  geom_col(aes(x = data_date, y = malaria_tot), width = 1)\n\n\n\n\nHowever, what if we wanted to display the relative contributions of each age group to this total count? In this case, we need to ensure that the variable of interest (age group), appears in the dataset in a single column that can be passed to {ggplot2}’s “mapping aesthetics” aes() argument.\n\n\n\n\nThe tidyr function pivot_longer() makes data “longer”. tidyr is part of the tidyverse of R packages.\nIt accepts a range of columns to transform (specified to cols =). Therefore, it can operate on only a part of a dataset. This is useful for the malaria data, as we only want to pivot the case count columns.\nIn this process, you will end up with two “new” columns - one with the categories (the former column names), and one with the corresponding values (e.g. case counts). You can accept the default names for these new columns, or you can specify your own to names_to = and values_to = respectively.\nLet’s see pivot_longer() in action…\n\n\n\nWe want to use tidyr’s pivot_longer() function to convert the “wide” data to a “long” format. Specifically, to convert the four numeric columns with data on malaria counts to two new columns: one which holds the age groups and one which holds the corresponding values.\n\ndf_long &lt;- count_data %&gt;% \n  pivot_longer(\n    cols = c(`malaria_rdt_0-4`, `malaria_rdt_5-14`, `malaria_rdt_15`, `malaria_tot`)\n  )\n\ndf_long\n\nNotice that the newly created data frame (df_long) has more rows (12,152 vs 3,038); it has become longer. In fact, it is precisely four times as long, because each row in the original dataset now represents four rows in df_long, one for each of the malaria count observations (&lt;4y, 5-14y, 15y+, and total).\nIn addition to becoming longer, the new dataset has fewer columns (8 vs 10), as the data previously stored in four columns (those beginning with the prefix malaria_) is now stored in two.\nSince the names of these four columns all begin with the prefix malaria_, we could have made use of the handy “tidyselect” function starts_with() to achieve the same result.\n\n# provide column with a tidyselect helper function\ncount_data %&gt;% \n  pivot_longer(\n    cols = starts_with(\"malaria_\")\n  )\n\n# A tibble: 12,152 × 8\n   location_name data_date  submitted_date Province District newid name    value\n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;   &lt;int&gt;\n 1 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    11\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    12\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    23\n 4 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malari…    46\n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    11\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    10\n 7 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…     5\n 8 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malari…    26\n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malari…     8\n10 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malari…     5\n# ℹ 12,142 more rows\n\n\nor by position:\n\n# provide columns by position\ncount_data %&gt;% \n  pivot_longer(\n    cols = 6:9\n  )\n\nor by named range:\n\n# provide range of consecutive columns\ncount_data %&gt;% \n  pivot_longer(\n    cols = `malaria_rdt_0-4`:malaria_tot\n  )\n\nThese two new columns are given the default names of name and value, but we can override these defaults to provide more meaningful names, which can help remember what is stored within, using the names_to and values_to arguments. Let’s use the names age_group and counts:\n\ndf_long &lt;- \n  count_data %&gt;% \n  pivot_longer(\n    cols = starts_with(\"malaria_\"),\n    names_to = \"age_group\",\n    values_to = \"counts\"\n  )\n\ndf_long\n\n# A tibble: 12,152 × 8\n   location_name data_date  submitted_date Province District newid age_group    \n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;        \n 1 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_rdt_…\n 4 Facility 1    2020-08-11 2020-08-12     North    Spring       1 malaria_tot  \n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 7 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_rdt_…\n 8 Facility 2    2020-08-11 2020-08-12     North    Bolo         2 malaria_tot  \n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malaria_rdt_…\n10 Facility 3    2020-08-11 2020-08-12     North    Dingo        3 malaria_rdt_…\n# ℹ 12,142 more rows\n# ℹ 1 more variable: counts &lt;int&gt;\n\n\nWe can now pass this new dataset to {ggplot2}, and map the new column count to the y-axis and new column age_group to the fill = argument (the column internal color). This will display the malaria counts in a stacked bar chart, by age group:\n\nggplot(data = df_long) +\n  geom_col(\n    mapping = aes(x = data_date, y = counts, fill = age_group),\n    width = 1\n  )\n\n\n\n\nExamine this new plot, and compare it with the plot we created earlier - what has gone wrong?\nWe have encountered a common problem when wrangling surveillance data - we have also included the total counts from the malaria_tot column, so the magnitude of each bar in the plot is twice as high as it should be.\nWe can handle this in a number of ways. We could simply filter these totals from the dataset before we pass it to ggplot():\n\ndf_long %&gt;% \n  filter(age_group != \"malaria_tot\") %&gt;% \n  ggplot() +\n  geom_col(\n    aes(x = data_date, y = counts, fill = age_group),\n    width = 1\n  )\n\n\n\n\nAlternatively, we could have excluded this variable when we ran pivot_longer(), thereby maintaining it in the dataset as a separate variable. See how its values “expand” to fill the new rows.\n\ncount_data %&gt;% \n  pivot_longer(\n    cols = `malaria_rdt_0-4`:malaria_rdt_15,   # does not include the totals column\n    names_to = \"age_group\",\n    values_to = \"counts\"\n  )\n\n# A tibble: 9,114 × 9\n   location_name data_date  submitted_date Province District malaria_tot newid\n   &lt;chr&gt;         &lt;date&gt;     &lt;date&gt;         &lt;chr&gt;    &lt;chr&gt;          &lt;int&gt; &lt;int&gt;\n 1 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 2 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 3 Facility 1    2020-08-11 2020-08-12     North    Spring            46     1\n 4 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 5 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 6 Facility 2    2020-08-11 2020-08-12     North    Bolo              26     2\n 7 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n 8 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n 9 Facility 3    2020-08-11 2020-08-12     North    Dingo             18     3\n10 Facility 4    2020-08-11 2020-08-12     North    Bolo              49     4\n# ℹ 9,104 more rows\n# ℹ 2 more variables: age_group &lt;chr&gt;, counts &lt;int&gt;\n\n\n\n\n\nThe above example works well in situations in which all the columns you want to “pivot longer” are of the same class (character, numeric, logical…).\nHowever, there will be many cases when, as a field epidemiologist, you will be working with data that was prepared by non-specialists and which follow their own non-standard logic - as Hadley Wickham noted (referencing Tolstoy) in his seminal article on Tidy Data principles: “Like families, tidy datasets are all alike but every messy dataset is messy in its own way.”\nOne particularly common problem you will encounter will be the need to pivot columns that contain different classes of data. This pivot will result in storing these different data types in a single column, which is not a good situation. There are various approaches one can take to separate out the mess this creates, but there is an important step you can take using pivot_longer() to avoid creating such a situation yourself.\nTake a situation in which there have been a series of observations at different time steps for each of three items A, B and C. Examples of such items could be individuals (e.g. contacts of an Ebola case being traced each day for 21 days) or remote village health posts being monitored once per year to ensure they are still functional. Let’s use the contact tracing example. Imagine that the data are stored as follows:\n\n\n\n\n\n\n\nAs can be seen, the data are a bit complicated. Each row stores information about one item, but with the time series running further and further away to the right as time progresses. Moreover, the column classes alternate between date and character values.\nOne particularly bad example of this encountered by this author involved cholera surveillance data, in which 8 new columns of observations were added each day over the course of 4 years. Simply opening the Excel file in which these data were stored took &gt;10 minuntes on my laptop!\nIn order to work with these data, we need to transform the data frame to long format, but keeping the separation between a date column and a character (status) column, for each observation for each item. If we don’t, we might end up with a mixture of variable types in a single column (a very big “no-no” when it comes to data management and tidy data):\n\ndf %&gt;% \n  pivot_longer(\n    cols = -id,\n    names_to = c(\"observation\")\n  )\n\n# A tibble: 18 × 3\n   id    observation value     \n   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     \n 1 A     obs1_date   2021-04-23\n 2 A     obs1_status Healthy   \n 3 A     obs2_date   2021-04-24\n 4 A     obs2_status Healthy   \n 5 A     obs3_date   2021-04-25\n 6 A     obs3_status Unwell    \n 7 B     obs1_date   2021-04-23\n 8 B     obs1_status Healthy   \n 9 B     obs2_date   2021-04-24\n10 B     obs2_status Healthy   \n11 B     obs3_date   2021-04-25\n12 B     obs3_status Healthy   \n13 C     obs1_date   2021-04-23\n14 C     obs1_status Missing   \n15 C     obs2_date   2021-04-24\n16 C     obs2_status Healthy   \n17 C     obs3_date   2021-04-25\n18 C     obs3_status Healthy   \n\n\nAbove, our pivot has merged dates and characters into a single value column. R will react by converting the entire column to class character, and the utility of the dates is lost.\nTo prevent this situation, we can take advantage of the syntax structure of the original column names. There is a common naming structure, with the observation number, an underscore, and then either “status” or “date”. We can leverage this syntax to keep these two data types in separate columns after the pivot.\nWe do this by:\n\nProviding a character vector to the names_to = argument, with the second item being (\".value\" ). This special term indicates that the pivoted columns will be split based on a character in their name…\n\nYou must also provide the “splitting” character to the names_sep = argument. In this case, it is the underscore “_“.\n\nThus, the naming and split of new columns is based around the underscore in the existing variable names.\n\ndf_long &lt;- \n  df %&gt;% \n  pivot_longer(\n    cols = -id,\n    names_to = c(\"observation\", \".value\"),\n    names_sep = \"_\"\n  )\n\ndf_long\n\n# A tibble: 9 × 4\n  id    observation date       status \n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;  \n1 A     obs1        2021-04-23 Healthy\n2 A     obs2        2021-04-24 Healthy\n3 A     obs3        2021-04-25 Unwell \n4 B     obs1        2021-04-23 Healthy\n5 B     obs2        2021-04-24 Healthy\n6 B     obs3        2021-04-25 Healthy\n7 C     obs1        2021-04-23 Missing\n8 C     obs2        2021-04-24 Healthy\n9 C     obs3        2021-04-25 Healthy\n\n\nFinishing touches:\nNote that the date column is currently in character class - we can easily convert this into it’s proper date class using the mutate() and as_date() functions.\nWe may also want to convert the observation column to a numeric format by dropping the “obs” prefix and converting to numeric. We cando this with str_remove_all() from the stringr package.\n\ndf_long &lt;- \n  df_long %&gt;% \n  mutate(\n    date = date %&gt;% lubridate::as_date(),\n    observation = \n      observation %&gt;% \n      str_remove_all(\"obs\") %&gt;% \n      as.numeric()\n  )\n\ndf_long\n\n# A tibble: 9 × 4\n  id    observation date       status \n  &lt;chr&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;  \n1 A               1 2021-04-23 Healthy\n2 A               2 2021-04-24 Healthy\n3 A               3 2021-04-25 Unwell \n4 B               1 2021-04-23 Healthy\n5 B               2 2021-04-24 Healthy\n6 B               3 2021-04-25 Healthy\n7 C               1 2021-04-23 Missing\n8 C               2 2021-04-24 Healthy\n9 C               3 2021-04-25 Healthy\n\n\nAnd now, we can start to work with the data in this format, e.g. by plotting a descriptive heat tile:\n\nggplot(data = df_long, mapping = aes(x = date, y = id, fill = status)) +\n  geom_tile(colour = \"black\") +\n  scale_fill_manual(\n    values = \n      c(\"Healthy\" = \"lightgreen\", \n        \"Unwell\" = \"red\", \n        \"Missing\" = \"orange\")\n  )"
  },
  {
    "objectID": "readings/pivoting.html#long-to-wide",
    "href": "readings/pivoting.html#long-to-wide",
    "title": "Pivoting data",
    "section": "",
    "text": "In some instances, we may wish to convert a dataset to a wider format. For this, we can use the pivot_wider() function.\nA typical use-case is when we want to transform the results of an analysis into a format which is more digestible for the reader (such as a [Table for presentation][Tables for presentation]). Usually, this involves transforming a dataset in which information for one subject is are spread over multiple rows into a format in which that information is stored in a single row.\n\n\nFor this section of the page, we will use the case linelist (see the Preparation section), which contains one row per case.\nHere are the first 50 rows:\n\n\n\n\n\n\n\nSuppose that we want to know the counts of individuals in the different age groups, by gender:\n\ndf_wide &lt;- \n  linelist %&gt;% \n  count(age_cat, gender)\n\ndf_wide\n\n   age_cat gender   n\n1      0-4      f 640\n2      0-4      m 416\n3      0-4   &lt;NA&gt;  39\n4      5-9      f 641\n5      5-9      m 412\n6      5-9   &lt;NA&gt;  42\n7    10-14      f 518\n8    10-14      m 383\n9    10-14   &lt;NA&gt;  40\n10   15-19      f 359\n11   15-19      m 364\n12   15-19   &lt;NA&gt;  20\n13   20-29      f 468\n14   20-29      m 575\n15   20-29   &lt;NA&gt;  30\n16   30-49      f 179\n17   30-49      m 557\n18   30-49   &lt;NA&gt;  18\n19   50-69      f   2\n20   50-69      m  91\n21   50-69   &lt;NA&gt;   2\n22     70+      m   5\n23     70+   &lt;NA&gt;   1\n24    &lt;NA&gt;   &lt;NA&gt;  86\n\n\nThis gives us a long dataset that is great for producing visualisations in ggplot2, but not ideal for presentation in a table:\n\nggplot(df_wide) +\n  geom_col(aes(x = age_cat, y = n, fill = gender))\n\n\n\n\n\n\n\nTherefore, we can use pivot_wider() to transform the data into a better format for inclusion as tables in our reports.\nThe argument names_from specifies the column from which to generate the new column names, while the argument values_from specifies the column from which to take the values to populate the cells. The argument id_cols = is optional, but can be provided a vector of column names that should not be pivoted, and will thus identify each row.\n\ntable_wide &lt;- \n  df_wide %&gt;% \n  pivot_wider(\n    id_cols = age_cat,\n    names_from = gender,\n    values_from = n\n  )\n\ntable_wide\n\n# A tibble: 9 × 4\n  age_cat     f     m  `NA`\n  &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 0-4       640   416    39\n2 5-9       641   412    42\n3 10-14     518   383    40\n4 15-19     359   364    20\n5 20-29     468   575    30\n6 30-49     179   557    18\n7 50-69       2    91     2\n8 70+        NA     5     1\n9 &lt;NA&gt;       NA    NA    86\n\n\nThis table is much more reader-friendly, and therefore better for inclusion in our reports. You can convert into a pretty table with several packages including flextable and knitr.\n\ntable_wide %&gt;% \n  janitor::adorn_totals(c(\"row\", \"col\")) %&gt;% # adds row and column totals\n  knitr::kable() %&gt;% \n  kableExtra::row_spec(row = 10, bold = TRUE) %&gt;% \n  kableExtra::column_spec(column = 5, bold = TRUE) \n\n\n\n\nage_cat\nf\nm\nNA\nTotal\n\n\n\n\n0-4\n640\n416\n39\n1095\n\n\n5-9\n641\n412\n42\n1095\n\n\n10-14\n518\n383\n40\n941\n\n\n15-19\n359\n364\n20\n743\n\n\n20-29\n468\n575\n30\n1073\n\n\n30-49\n179\n557\n18\n754\n\n\n50-69\n2\n91\n2\n95\n\n\n70+\nNA\n5\n1\n6\n\n\nNA\nNA\nNA\n86\n86\n\n\nTotal\n2807\n2803\n278\n5888"
  },
  {
    "objectID": "readings/pivoting.html#fill",
    "href": "readings/pivoting.html#fill",
    "title": "Pivoting data",
    "section": "",
    "text": "In some situations after a pivot, and more commonly after a bind, we are left with gaps in some cells that we would like to fill.\n\n\n\nFor example, take two datasets, each with observations for the measurement number, the name of the facility, and the case count at that time. However, the second dataset also has a variable Year.\n\ndf1 &lt;- \n  tibble::tribble(\n       ~Measurement, ~Facility, ~Cases,\n                  1,  \"Hosp 1\",     66,\n                  2,  \"Hosp 1\",     26,\n                  3,  \"Hosp 1\",      8,\n                  1,  \"Hosp 2\",     71,\n                  2,  \"Hosp 2\",     62,\n                  3,  \"Hosp 2\",     70,\n                  1,  \"Hosp 3\",     47,\n                  2,  \"Hosp 3\",     70,\n                  3,  \"Hosp 3\",     38,\n       )\n\ndf1 \n\n# A tibble: 9 × 3\n  Measurement Facility Cases\n        &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1           1 Hosp 1      66\n2           2 Hosp 1      26\n3           3 Hosp 1       8\n4           1 Hosp 2      71\n5           2 Hosp 2      62\n6           3 Hosp 2      70\n7           1 Hosp 3      47\n8           2 Hosp 3      70\n9           3 Hosp 3      38\n\ndf2 &lt;- \n  tibble::tribble(\n    ~Year, ~Measurement, ~Facility, ~Cases,\n     2000,            1,  \"Hosp 4\",     82,\n     2001,            2,  \"Hosp 4\",     87,\n     2002,            3,  \"Hosp 4\",     46\n  )\n\ndf2\n\n# A tibble: 3 × 4\n   Year Measurement Facility Cases\n  &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1  2000           1 Hosp 4      82\n2  2001           2 Hosp 4      87\n3  2002           3 Hosp 4      46\n\n\nWhen we perform a bind_rows() to join the two datasets together, the Year variable is filled with NA for those rows where there was no prior information (i.e. the first dataset):\n\ndf_combined &lt;- \n  bind_rows(df1, df2) %&gt;% \n  arrange(Measurement, Facility)\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 1      66    NA\n 2           1 Hosp 2      71    NA\n 3           1 Hosp 3      47    NA\n 4           1 Hosp 4      82  2000\n 5           2 Hosp 1      26    NA\n 6           2 Hosp 2      62    NA\n 7           2 Hosp 3      70    NA\n 8           2 Hosp 4      87  2001\n 9           3 Hosp 1       8    NA\n10           3 Hosp 2      70    NA\n11           3 Hosp 3      38    NA\n12           3 Hosp 4      46  2002\n\n\n\n\n\n\nIn this case, Year is a useful variable to include, particularly if we want to explore trends over time. Therefore, we use fill() to fill in those empty cells, by specifying the column to fill and the direction (in this case up):\n\ndf_combined %&gt;% \n  fill(Year, .direction = \"up\")\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 1      66  2000\n 2           1 Hosp 2      71  2000\n 3           1 Hosp 3      47  2000\n 4           1 Hosp 4      82  2000\n 5           2 Hosp 1      26  2001\n 6           2 Hosp 2      62  2001\n 7           2 Hosp 3      70  2001\n 8           2 Hosp 4      87  2001\n 9           3 Hosp 1       8  2002\n10           3 Hosp 2      70  2002\n11           3 Hosp 3      38  2002\n12           3 Hosp 4      46  2002\n\n\nAlternatively, we can rearrange the data so that we would need to fill in a downward direction:\n\ndf_combined &lt;- \n  df_combined %&gt;% \n  arrange(Measurement, desc(Facility))\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 4      82  2000\n 2           1 Hosp 3      47    NA\n 3           1 Hosp 2      71    NA\n 4           1 Hosp 1      66    NA\n 5           2 Hosp 4      87  2001\n 6           2 Hosp 3      70    NA\n 7           2 Hosp 2      62    NA\n 8           2 Hosp 1      26    NA\n 9           3 Hosp 4      46  2002\n10           3 Hosp 3      38    NA\n11           3 Hosp 2      70    NA\n12           3 Hosp 1       8    NA\n\ndf_combined &lt;- \n  df_combined %&gt;% \n  fill(Year, .direction = \"down\")\n\ndf_combined\n\n# A tibble: 12 × 4\n   Measurement Facility Cases  Year\n         &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n 1           1 Hosp 4      82  2000\n 2           1 Hosp 3      47  2000\n 3           1 Hosp 2      71  2000\n 4           1 Hosp 1      66  2000\n 5           2 Hosp 4      87  2001\n 6           2 Hosp 3      70  2001\n 7           2 Hosp 2      62  2001\n 8           2 Hosp 1      26  2001\n 9           3 Hosp 4      46  2002\n10           3 Hosp 3      38  2002\n11           3 Hosp 2      70  2002\n12           3 Hosp 1       8  2002\n\n\nWe now have a useful dataset for plotting:\n\nggplot(df_combined) +\n  aes(Year, Cases, fill = Facility) +\n  geom_col()\n\n\n\n\nBut less useful for presenting in a table, so let’s practice converting this long, untidy dataframe into a wider, tidy dataframe:\n\ndf_combined %&gt;% \n  pivot_wider(\n    id_cols = c(Measurement, Facility),\n    names_from = \"Year\",\n    values_from = \"Cases\"\n  ) %&gt;% \n  arrange(Facility) %&gt;% \n  janitor::adorn_totals(c(\"row\", \"col\")) %&gt;% \n  knitr::kable() %&gt;% \n  kableExtra::row_spec(row = 5, bold = TRUE) %&gt;% \n  kableExtra::column_spec(column = 5, bold = TRUE) \n\n\n\n\nMeasurement\nFacility\n2000\n2001\n2002\nTotal\n\n\n\n\n1\nHosp 1\n66\nNA\nNA\n66\n\n\n2\nHosp 1\nNA\n26\nNA\n26\n\n\n3\nHosp 1\nNA\nNA\n8\n8\n\n\n1\nHosp 2\n71\nNA\nNA\n71\n\n\n2\nHosp 2\nNA\n62\nNA\n62\n\n\n3\nHosp 2\nNA\nNA\n70\n70\n\n\n1\nHosp 3\n47\nNA\nNA\n47\n\n\n2\nHosp 3\nNA\n70\nNA\n70\n\n\n3\nHosp 3\nNA\nNA\n38\n38\n\n\n1\nHosp 4\n82\nNA\nNA\n82\n\n\n2\nHosp 4\nNA\n87\nNA\n87\n\n\n3\nHosp 4\nNA\nNA\n46\n46\n\n\nTotal\n-\n266\n245\n162\n673\n\n\n\n\n\n\n\nN.B. In this case, we had to specify to only include the three variables Facility, Year, and Cases as the additional variable Measurement would interfere with the creation of the table:\n\ndf_combined %&gt;% \n  pivot_wider(\n    names_from = \"Year\",\n    values_from = \"Cases\"\n  ) %&gt;% \n  knitr::kable()\n\n\n\n\nMeasurement\nFacility\n2000\n2001\n2002\n\n\n\n\n1\nHosp 4\n82\nNA\nNA\n\n\n1\nHosp 3\n47\nNA\nNA\n\n\n1\nHosp 2\n71\nNA\nNA\n\n\n1\nHosp 1\n66\nNA\nNA\n\n\n2\nHosp 4\nNA\n87\nNA\n\n\n2\nHosp 3\nNA\n70\nNA\n\n\n2\nHosp 2\nNA\n62\nNA\n\n\n2\nHosp 1\nNA\n26\nNA\n\n\n3\nHosp 4\nNA\nNA\n46\n\n\n3\nHosp 3\nNA\nNA\n38\n\n\n3\nHosp 2\nNA\nNA\n70\n\n\n3\nHosp 1\nNA\nNA\n8"
  },
  {
    "objectID": "readings/recoding-values.html",
    "href": "readings/recoding-values.html",
    "title": "Recoding Values",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\nHere are a few scenarios where you need to re-code (change) values:\n\nto edit one specific value (e.g. one date with an incorrect year or format)\n\nto reconcile values not spelled the same\nto create a new column of categorical values\n\nto create a new column of numeric categories (e.g. age categories)\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\n\n\n\nTo change values manually you can use the recode() function within the mutate() function.\nImagine there is a nonsensical date in the data (e.g. “2014-14-15”): you could fix the date manually in the raw source data, or, you could write the change into the cleaning pipeline via mutate() and recode(). The latter is more transparent and reproducible to anyone else seeking to understand or repeat your analysis.\n\n# fix incorrect values                   # old value       # new value\nlinelist &lt;- linelist %&gt;% \n  mutate(date_onset = recode(date_onset, \"2014-14-15\" = \"2014-04-15\"))\n\nThe mutate() line above can be read as: “mutate the column date_onset to equal the column date_onset re-coded so that OLD VALUE is changed to NEW VALUE”. Note that this pattern (OLD = NEW) for recode() is the opposite of most R patterns (new = old). The R development community is working on revising this.\n\n\n\nBelow we demonstrate how to re-code values in a column using logic and conditions:\n\nUsing replace(), ifelse() and if_else() for simple logic\nUsing case_when() for more complex logic\n\n\n\n\n\nTo re-code with simple logical criteria, you can use replace() within mutate(). replace() is a function from base R. Use a logic condition to specify the rows to change . The general syntax is:\nmutate(col_to_change = replace(col_to_change, criteria for rows, new value)).\nOne common situation to use replace() is changing just one value in one row, using an unique row identifier. Below, the gender is changed to “Female” in the row where the column case_id is “2195”.\n\n# Example: change gender of one specific observation to \"Female\" \nlinelist &lt;- linelist %&gt;% \n  mutate(gender = replace(gender, case_id == \"2195\", \"Female\"))\n\nThe equivalent command using base R syntax and indexing brackets [ ] is below. It reads as “Change the value of the dataframe linelist‘s column gender (for the rows where linelist’s column case_id has the value ’2195’) to ‘Female’”.\n\nlinelist$gender[linelist$case_id == \"2195\"] &lt;- \"Female\"\n\n\n\n\nAnother tool for simple logic is ifelse() and its partner if_else(). However, in most cases for re-coding it is more clear to use case_when() (detailed below). These “if else” commands are simplified versions of an if and else programming statement. The general syntax is:\nifelse(condition, value to return if condition evaluates to TRUE, value to return if condition evaluates to FALSE)\nBelow, the column source_known is defined. Its value in a given row is set to “known” if the row’s value in column source is not missing. If the value in source is missing, then the value in source_known is set to “unknown”.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(source_known = ifelse(!is.na(source), \"known\", \"unknown\"))\n\nif_else() is a special version from dplyr that handles dates. Note that if the ‘true’ value is a date, the ‘false’ value must also qualify a date, hence using the special value NA_real_ instead of just NA.\n\n# Create a date of death column, which is NA if patient has not died.\nlinelist &lt;- linelist %&gt;% \n  mutate(date_death = if_else(outcome == \"Death\", date_outcome, NA_real_))\n\nAvoid stringing together many ifelse commands… use case_when() instead! case_when() is much easier to read and you’ll make fewer errors.\n\n\n\n\n\n\n\n\n\nOutside of the context of a data frame, if you want to have an object used in your code switch its value, consider using switch() from base R.\n\n\n\n\nUse dplyr’s case_when() if you are re-coding into many new groups, or if you need to use complex logic statements to re-code values. This function evaluates every row in the data frame, assess whether the rows meets specified criteria, and assigns the correct new value.\ncase_when() commands consist of statements that have a Right-Hand Side (RHS) and a Left-Hand Side (LHS) separated by a “tilde” ~. The logic criteria are in the left side and the pursuant values are in the right side of each statement. Statements are separated by commas.\nFor example, here we utilize the columns age and age_unit to create a column age_years:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(age_years = case_when(\n       age_unit == \"years\"  ~ age,       # if age unit is years\n       age_unit == \"months\" ~ age/12,    # if age unit is months, divide age by 12\n       is.na(age_unit)      ~ age))      # if age unit is missing, assume years\n                                         # any other circumstance, assign NA (missing)\n\nAs each row in the data is evaluated, the criteria are applied/evaluated in the order the case_when() statements are written - from top-to-bottom. If the top criteria evaluates to TRUE for a given row, the RHS value is assigned, and the remaining criteria are not even tested for that row in the data. Thus, it is best to write the most specific criteria first, and the most general last. A data row that does not meet any of the RHS criteria will be assigned NA.\nSometimes, you may with to write a final statement that assigns a value for all other scenarios not described by one of the previous lines. To do this, place TRUE on the left-side, which will capture any row that did not meet any of the previous criteria. The right-side of this statement could be assigned a value like “check me!” or missing.\nBelow is another example of case_when() used to create a new column with the patient classification, according to a case definition for confirmed and suspect cases:\n\nlinelist &lt;- linelist %&gt;% \n     mutate(case_status = case_when(\n          \n          # if patient had lab test and it is positive,\n          # then they are marked as a confirmed case \n          ct_blood &lt; 20                   ~ \"Confirmed\",\n          \n          # given that a patient does not have a positive lab result,\n          # if patient has a \"source\" (epidemiological link) AND has fever, \n          # then they are marked as a suspect case\n          !is.na(source) & fever == \"yes\" ~ \"Suspect\",\n          \n          # any other patient not addressed above \n          # is marked for follow up\n          TRUE                            ~ \"To investigate\"))\n\n\n\n\n\nHere we describe some special approaches for creating categories from numerical columns. Common examples include age categories, groups of lab values, etc. Here we will discuss:\n\nage_categories(), from the epikit package\n\ncut(), from base R\n\ncase_when()\n\nquantile breaks with quantile() and ntile()\n\n\n\nFor this example we will create an age_cat column using the age_years column.\n\n#check the class of the linelist variable age\nclass(linelist$age_years)\n\n[1] \"numeric\"\n\n\n\n\n\nThe basic syntax within cut() is to first provide the numeric column to be cut (age_years), and then the breaks argument, which is a numeric vector c() of break points. Using cut(), the resulting column is an ordered factor.\nBy default, the categorization occurs so that the right/upper side is “open” and inclusive (and the left/lower side is “closed” or exclusive). This is the opposite behavior from the age_categories() function. The default labels use the notation “(A, B]”, which means A is not included but B is. Reverse this behavior by providing the right = TRUE argument.\nThus, by default, “0” values are excluded from the lowest group, and categorized as NA! “0” values could be infants coded as age 0 so be careful! To change this, add the argument include.lowest = TRUE so that any “0” values will be included in the lowest group. The automatically-generated label for the lowest category will then be “[A],B]”. Note that if you include the include.lowest = TRUE argument and right = TRUE, the extreme inclusion will now apply to the highest break point value and category, not the lowest.\nYou can provide a vector of customized labels using the labels = argument. As these are manually written, be very careful to ensure they are accurate! Check your work using cross-tabulation, as described below.\nAn example of cut() applied to age_years to make the new variable age_cat is below:\n\n# Create new variable, by cutting the numeric age variable\n# lower break is excluded but upper break is included in each category\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    age_cat = cut(\n      age_years,\n      breaks = c(0, 5, 10, 15, 20,\n                 30, 50, 70, 100),\n      include.lowest = TRUE         # include 0 in lowest group\n      ))\n\n# tabulate the number of observations per group\ntable(linelist$age_cat, useNA = \"always\")\n\n\n   [0,5]   (5,10]  (10,15]  (15,20]  (20,30]  (30,50]  (50,70] (70,100] \n    1315     1065      930      696     1013      694       84        5 \n    &lt;NA&gt; \n      86 \n\n\nCheck your work!!! Verify that each age value was assigned to the correct category by cross-tabulating the numeric and category columns. Examine assignment of boundary values (e.g. 15, if neighboring categories are 10-15 and 16-20).\n\n\n\nIn common understanding, “quantiles” or “percentiles” typically refer to a value below which a proportion of values fall. For example, the 95th percentile of ages in linelist would be the age below which 95% of the age fall.\nHowever in common speech, “quartiles” and “deciles” can also refer to the groups of data as equally divided into 4, or 10 groups (note there will be one more break point than group).\nTo get quantile break points, you can use quantile() from the stats package from base R. You provide a numeric vector (e.g. a column in a dataset) and vector of numeric probability values ranging from 0 to 1.0. The break points are returned as a numeric vector. Explore the details of the statistical methodologies by entering ?quantile.\n\nIf your input numeric vector has any missing values it is best to set na.rm = TRUE\n\nSet names = FALSE to get an un-named numeric vector\n\n\nquantile(linelist$age_years,               # specify numeric vector to work on\n  probs = c(0, .25, .50, .75, .90, .95),   # specify the percentiles you want\n  na.rm = TRUE)                            # ignore missing values \n\n  0%  25%  50%  75%  90%  95% \n 0.0  6.0 13.0 23.0 33.9 41.0 \n\n\nYou can use the results of quantile() as break points in age_categories() or cut(). Below we create a new column deciles using cut() where the breaks are defined using quantiles() on age_years. Below, we display the results using tabyl() from janitor so you can see the percentages. Note how they are not exactly 10% in each group.\n\nlinelist %&gt;%                                # begin with linelist\n  mutate(deciles = cut(age_years,           # create new column decile as cut() on column age_years\n    breaks = quantile(                      # define cut breaks using quantile()\n      age_years,                               # operate on age_years\n      probs = seq(0, 1, by = 0.1),             # 0.0 to 1.0 by 0.1\n      na.rm = TRUE),                           # ignore missing values\n    include.lowest = TRUE)) %&gt;%             # for cut() include age 0\n  janitor::tabyl(deciles)                   # pipe to table to display\n\n   deciles   n    percent valid_percent\n     [0,2] 658 0.11175272    0.11340917\n     (2,5] 657 0.11158288    0.11323681\n     (5,7] 447 0.07591712    0.07704240\n    (7,10] 618 0.10495924    0.10651499\n   (10,13] 572 0.09714674    0.09858669\n   (13,17] 674 0.11447011    0.11616684\n   (17,21] 520 0.08831522    0.08962427\n   (21,26] 547 0.09290082    0.09427784\n (26,33.9] 528 0.08967391    0.09100310\n (33.9,84] 581 0.09867527    0.10013788\n      &lt;NA&gt;  86 0.01460598            NA"
  },
  {
    "objectID": "readings/recoding-values.html#manually-changing-specific-values",
    "href": "readings/recoding-values.html#manually-changing-specific-values",
    "title": "Recoding Values",
    "section": "",
    "text": "To change values manually you can use the recode() function within the mutate() function.\nImagine there is a nonsensical date in the data (e.g. “2014-14-15”): you could fix the date manually in the raw source data, or, you could write the change into the cleaning pipeline via mutate() and recode(). The latter is more transparent and reproducible to anyone else seeking to understand or repeat your analysis.\n\n# fix incorrect values                   # old value       # new value\nlinelist &lt;- linelist %&gt;% \n  mutate(date_onset = recode(date_onset, \"2014-14-15\" = \"2014-04-15\"))\n\nThe mutate() line above can be read as: “mutate the column date_onset to equal the column date_onset re-coded so that OLD VALUE is changed to NEW VALUE”. Note that this pattern (OLD = NEW) for recode() is the opposite of most R patterns (new = old). The R development community is working on revising this."
  },
  {
    "objectID": "readings/recoding-values.html#by-logic",
    "href": "readings/recoding-values.html#by-logic",
    "title": "Recoding Values",
    "section": "",
    "text": "Below we demonstrate how to re-code values in a column using logic and conditions:\n\nUsing replace(), ifelse() and if_else() for simple logic\nUsing case_when() for more complex logic\n\n\n\n\n\nTo re-code with simple logical criteria, you can use replace() within mutate(). replace() is a function from base R. Use a logic condition to specify the rows to change . The general syntax is:\nmutate(col_to_change = replace(col_to_change, criteria for rows, new value)).\nOne common situation to use replace() is changing just one value in one row, using an unique row identifier. Below, the gender is changed to “Female” in the row where the column case_id is “2195”.\n\n# Example: change gender of one specific observation to \"Female\" \nlinelist &lt;- linelist %&gt;% \n  mutate(gender = replace(gender, case_id == \"2195\", \"Female\"))\n\nThe equivalent command using base R syntax and indexing brackets [ ] is below. It reads as “Change the value of the dataframe linelist‘s column gender (for the rows where linelist’s column case_id has the value ’2195’) to ‘Female’”.\n\nlinelist$gender[linelist$case_id == \"2195\"] &lt;- \"Female\"\n\n\n\n\nAnother tool for simple logic is ifelse() and its partner if_else(). However, in most cases for re-coding it is more clear to use case_when() (detailed below). These “if else” commands are simplified versions of an if and else programming statement. The general syntax is:\nifelse(condition, value to return if condition evaluates to TRUE, value to return if condition evaluates to FALSE)\nBelow, the column source_known is defined. Its value in a given row is set to “known” if the row’s value in column source is not missing. If the value in source is missing, then the value in source_known is set to “unknown”.\n\nlinelist &lt;- linelist %&gt;% \n  mutate(source_known = ifelse(!is.na(source), \"known\", \"unknown\"))\n\nif_else() is a special version from dplyr that handles dates. Note that if the ‘true’ value is a date, the ‘false’ value must also qualify a date, hence using the special value NA_real_ instead of just NA.\n\n# Create a date of death column, which is NA if patient has not died.\nlinelist &lt;- linelist %&gt;% \n  mutate(date_death = if_else(outcome == \"Death\", date_outcome, NA_real_))\n\nAvoid stringing together many ifelse commands… use case_when() instead! case_when() is much easier to read and you’ll make fewer errors.\n\n\n\n\n\n\n\n\n\nOutside of the context of a data frame, if you want to have an object used in your code switch its value, consider using switch() from base R.\n\n\n\n\nUse dplyr’s case_when() if you are re-coding into many new groups, or if you need to use complex logic statements to re-code values. This function evaluates every row in the data frame, assess whether the rows meets specified criteria, and assigns the correct new value.\ncase_when() commands consist of statements that have a Right-Hand Side (RHS) and a Left-Hand Side (LHS) separated by a “tilde” ~. The logic criteria are in the left side and the pursuant values are in the right side of each statement. Statements are separated by commas.\nFor example, here we utilize the columns age and age_unit to create a column age_years:\n\nlinelist &lt;- linelist %&gt;% \n  mutate(age_years = case_when(\n       age_unit == \"years\"  ~ age,       # if age unit is years\n       age_unit == \"months\" ~ age/12,    # if age unit is months, divide age by 12\n       is.na(age_unit)      ~ age))      # if age unit is missing, assume years\n                                         # any other circumstance, assign NA (missing)\n\nAs each row in the data is evaluated, the criteria are applied/evaluated in the order the case_when() statements are written - from top-to-bottom. If the top criteria evaluates to TRUE for a given row, the RHS value is assigned, and the remaining criteria are not even tested for that row in the data. Thus, it is best to write the most specific criteria first, and the most general last. A data row that does not meet any of the RHS criteria will be assigned NA.\nSometimes, you may with to write a final statement that assigns a value for all other scenarios not described by one of the previous lines. To do this, place TRUE on the left-side, which will capture any row that did not meet any of the previous criteria. The right-side of this statement could be assigned a value like “check me!” or missing.\nBelow is another example of case_when() used to create a new column with the patient classification, according to a case definition for confirmed and suspect cases:\n\nlinelist &lt;- linelist %&gt;% \n     mutate(case_status = case_when(\n          \n          # if patient had lab test and it is positive,\n          # then they are marked as a confirmed case \n          ct_blood &lt; 20                   ~ \"Confirmed\",\n          \n          # given that a patient does not have a positive lab result,\n          # if patient has a \"source\" (epidemiological link) AND has fever, \n          # then they are marked as a suspect case\n          !is.na(source) & fever == \"yes\" ~ \"Suspect\",\n          \n          # any other patient not addressed above \n          # is marked for follow up\n          TRUE                            ~ \"To investigate\"))"
  },
  {
    "objectID": "readings/recoding-values.html#num_cats",
    "href": "readings/recoding-values.html#num_cats",
    "title": "Recoding Values",
    "section": "",
    "text": "Here we describe some special approaches for creating categories from numerical columns. Common examples include age categories, groups of lab values, etc. Here we will discuss:\n\nage_categories(), from the epikit package\n\ncut(), from base R\n\ncase_when()\n\nquantile breaks with quantile() and ntile()\n\n\n\nFor this example we will create an age_cat column using the age_years column.\n\n#check the class of the linelist variable age\nclass(linelist$age_years)\n\n[1] \"numeric\"\n\n\n\n\n\nThe basic syntax within cut() is to first provide the numeric column to be cut (age_years), and then the breaks argument, which is a numeric vector c() of break points. Using cut(), the resulting column is an ordered factor.\nBy default, the categorization occurs so that the right/upper side is “open” and inclusive (and the left/lower side is “closed” or exclusive). This is the opposite behavior from the age_categories() function. The default labels use the notation “(A, B]”, which means A is not included but B is. Reverse this behavior by providing the right = TRUE argument.\nThus, by default, “0” values are excluded from the lowest group, and categorized as NA! “0” values could be infants coded as age 0 so be careful! To change this, add the argument include.lowest = TRUE so that any “0” values will be included in the lowest group. The automatically-generated label for the lowest category will then be “[A],B]”. Note that if you include the include.lowest = TRUE argument and right = TRUE, the extreme inclusion will now apply to the highest break point value and category, not the lowest.\nYou can provide a vector of customized labels using the labels = argument. As these are manually written, be very careful to ensure they are accurate! Check your work using cross-tabulation, as described below.\nAn example of cut() applied to age_years to make the new variable age_cat is below:\n\n# Create new variable, by cutting the numeric age variable\n# lower break is excluded but upper break is included in each category\nlinelist &lt;- linelist %&gt;% \n  mutate(\n    age_cat = cut(\n      age_years,\n      breaks = c(0, 5, 10, 15, 20,\n                 30, 50, 70, 100),\n      include.lowest = TRUE         # include 0 in lowest group\n      ))\n\n# tabulate the number of observations per group\ntable(linelist$age_cat, useNA = \"always\")\n\n\n   [0,5]   (5,10]  (10,15]  (15,20]  (20,30]  (30,50]  (50,70] (70,100] \n    1315     1065      930      696     1013      694       84        5 \n    &lt;NA&gt; \n      86 \n\n\nCheck your work!!! Verify that each age value was assigned to the correct category by cross-tabulating the numeric and category columns. Examine assignment of boundary values (e.g. 15, if neighboring categories are 10-15 and 16-20).\n\n\n\nIn common understanding, “quantiles” or “percentiles” typically refer to a value below which a proportion of values fall. For example, the 95th percentile of ages in linelist would be the age below which 95% of the age fall.\nHowever in common speech, “quartiles” and “deciles” can also refer to the groups of data as equally divided into 4, or 10 groups (note there will be one more break point than group).\nTo get quantile break points, you can use quantile() from the stats package from base R. You provide a numeric vector (e.g. a column in a dataset) and vector of numeric probability values ranging from 0 to 1.0. The break points are returned as a numeric vector. Explore the details of the statistical methodologies by entering ?quantile.\n\nIf your input numeric vector has any missing values it is best to set na.rm = TRUE\n\nSet names = FALSE to get an un-named numeric vector\n\n\nquantile(linelist$age_years,               # specify numeric vector to work on\n  probs = c(0, .25, .50, .75, .90, .95),   # specify the percentiles you want\n  na.rm = TRUE)                            # ignore missing values \n\n  0%  25%  50%  75%  90%  95% \n 0.0  6.0 13.0 23.0 33.9 41.0 \n\n\nYou can use the results of quantile() as break points in age_categories() or cut(). Below we create a new column deciles using cut() where the breaks are defined using quantiles() on age_years. Below, we display the results using tabyl() from janitor so you can see the percentages. Note how they are not exactly 10% in each group.\n\nlinelist %&gt;%                                # begin with linelist\n  mutate(deciles = cut(age_years,           # create new column decile as cut() on column age_years\n    breaks = quantile(                      # define cut breaks using quantile()\n      age_years,                               # operate on age_years\n      probs = seq(0, 1, by = 0.1),             # 0.0 to 1.0 by 0.1\n      na.rm = TRUE),                           # ignore missing values\n    include.lowest = TRUE)) %&gt;%             # for cut() include age 0\n  janitor::tabyl(deciles)                   # pipe to table to display\n\n   deciles   n    percent valid_percent\n     [0,2] 658 0.11175272    0.11340917\n     (2,5] 657 0.11158288    0.11323681\n     (5,7] 447 0.07591712    0.07704240\n    (7,10] 618 0.10495924    0.10651499\n   (10,13] 572 0.09714674    0.09858669\n   (13,17] 674 0.11447011    0.11616684\n   (17,21] 520 0.08831522    0.08962427\n   (21,26] 547 0.09290082    0.09427784\n (26,33.9] 528 0.08967391    0.09100310\n (33.9,84] 581 0.09867527    0.10013788\n      &lt;NA&gt;  86 0.01460598            NA"
  },
  {
    "objectID": "readings/rstudio.html",
    "href": "readings/rstudio.html",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "RStudio is freely available open-source Integrated Development Environment (IDE). RStudio provides an environment with many features to make using R easier and is a great alternative to working on R in the terminal.\nYou can see the complete RStudio user-interface cheatsheet (PDF) here\n\n\n\nLet’s create a new project directory for the Research and Design course.\n\nOpen RStudio\nGo to the File menu and select New Project.\nIn the New Project window, choose New Directory. Then, choose New Project. Name your new directory whatever you want and then “Create the project as subdirectory of:” the Desktop (or location of your choice).\nClick on Create Project.\nAfter your project is completed, if the project does not automatically open in RStudio, then go to the File menu, select Open Project, and choose [your project name].Rproj.\nWhen RStudio opens, you will see three panels in the window.\nGo to the File menu and select New File, and select R Script. The RStudio interface should now look like the screenshot below.\n\nTIP: If your RStudio displays only one left pane it is because you have no scripts open yet.\n\n\n\nRStudio interface\n\n\n\n\nIt is simply a directory that contains everything related your analyses for a specific project. RStudio projects are useful when you are working on context- specific analyses and you wish to keep them separate. When creating a project in RStudio you associate it with a working directory of your choice (either an existing one, or a new one). A . RProj file is created within that directory and that keeps track of your command history and variables in the environment. The . RProj file can be used to open the project in its current state but at a later date.\nWhen a project is (re) opened within RStudio the following actions are taken:\n\nA new R session (process) is started\nThe .RData file in the project’s main directory is loaded, populating the environment with any objects that were present when the project was closed.\nThe .Rhistory file in the project’s main directory is loaded into the RStudio History pane (and used for Console Up/Down arrow command history).\nThe current working directory is set to the project directory.\nPreviously edited source documents are restored into editor tabs\nOther RStudio settings (e.g. active tabs, splitter positions, etc.) are restored to where they were the last time the project was closed.\n\nInformation adapted from RStudio Support Site\n\n\n\n\nThe RStudio interface has four main panels:\n\nConsole: where you can type commands and see output. The console is all you would see if you ran R in the command line without RStudio.\nScript editor: where you can type out commands and save to file. You can also submit the commands to run in the console.\nEnvironment/History: environment shows all active objects and history keeps track of all commands run in console\nFiles/Plots/Packages/Help\n\n\n\n\n\n\nBefore we organize our working directory, let’s check to see where our current working directory is located by typing into the console:\n\ngetwd()\n\nYour working directory should be the Intro-to-R folder constructed when you created the project. The working directory is where RStudio will automatically look for any files you bring in and where it will automatically save any files you create, unless otherwise specified.\nYou can visualize your working directory by selecting the Files tab from the Files/Plots/Packages/Help window.\n\n\n\nViewing your working directory\n\n\nIf you wanted to choose a different directory to be your working directory, you could navigate to a different folder in the Files tab, then, click on the More dropdown menu and select Set As Working Directory.\n\n\n\nSetting your working directory\n\n\n\n\n\nTo organize your working directory for a particular analysis, you typically want to separate the original data (raw data) from intermediate datasets. For instance, you may want to create a data/ directory within your working directory that stores the raw data, and have a results/ directory for intermediate datasets and a figures/ directory for the plots you will generate.\nLet’s create these three directories within your working directory by clicking on New Folder within the Files tab.\n\n\n\nStructuring your working directory\n\n\nWhen finished, your working directory should look like:\n\n\n\nYour organized working directory\n\n\n\n\n\nThis is more of a housekeeping task. We will be writing long lines of code in our script editor and want to make sure that the lines “wrap” and you don’t have to scroll back and forth to look at your long line of code.\nClick on “Tools” at the top of your RStudio screen and click on “Global Options” in the pull down menu.\n\n\n\noptions\n\n\nOn the left, select “Code” and put a check against “Soft-wrap R source files”. Make sure you click the “Apply” button at the bottom of the Window before saying “OK”.\n\n\n\nwrap_options\n\n\n\n\n\n\nNow that we have our interface and directory structure set up, let’s start playing with R! There are two main ways of interacting with R in RStudio: using the console or by using script editor (plain text files that contain your code).\n\n\nThe console window (in RStudio, the bottom left panel) is the place where R is waiting for you to tell it what to do, and where it will show the results of a command. You can type commands directly into the console, but they will be forgotten when you close the session.\n\n\n\nRunning in the console\n\n\n\n\n\nBest practice is to enter the commands in the script editor, and save the script. You are encouraged to comment liberally to describe the commands you are running using #. This way, you have a complete record of what you did, you can easily show others how you did it and you can do it again later on if needed.\nThe Rstudio script editor allows you to ‘send’ the current line or the currently highlighted text to the R console by clicking on the Run button in the upper-right hand corner of the script editor. Alternatively, you can run by simply pressing the Ctrl and Enter keys at the same time as a shortcut.\nNow let’s try entering commands to the script editor and using the comments character # to add descriptions and highlighting the text to run:\n\n    # Session 1\n    # Feb 3, 2023\n\n    # Interacting with R\n    \n    # I am adding 3 and 5. \n    3+5\n\n\n\n\nRunning in the script editor\n\n\nYou should see the command run in the console and output the result.\n\n\n\nScript editor output\n\n\nWhat happens if we do that same command without the comment symbol #? Re-run the command after removing the # sign in the front:\n\nI am adding 3 and 5. R is fun!\n3+5\n\nNow R is trying to run that sentence as a command, and it doesn’t work. We get an error in the console “Error: unexpected symbol in”I am” means that the R interpreter did not know what to do with that command.”\n\n\n\nInterpreting the command prompt can help understand when R is ready to accept commands. Below lists the different states of the command prompt and how you can exit a command:\nConsole is ready to accept commands: &gt;.\nIf R is ready to accept commands, the R console shows a &gt; prompt.\nWhen the console receives a command (by directly typing into the console or running from the script editor (Ctrl-Enter), R will try to execute it.\nAfter running, the console will show the results and come back with a new &gt; prompt to wait for new commands.\nConsole is waiting for you to enter more data: +.\nIf R is still waiting for you to enter more data because it isn’t complete yet, the console will show a + prompt. It means that you haven’t finished entering a complete command. Often this can be due to you having not ‘closed’ a parenthesis or quotation.\nEscaping a command and getting a new prompt: esc\nIf you’re in Rstudio and you can’t figure out why your command isn’t running, you can click inside the console window and press esc to escape the command and bring back a new prompt &gt;.\n\n\n\nIn addition to some of the shortcuts described earlier in this lesson, we have listed a few more that can be helpful as you work in RStudio.\n\n\n\n\n\n\nShortcuts Table\n\n\n\n\n\nSome very useful keyboard shortcuts are below. See all the keyboard shortcuts for Windows, Max, and Linux in the second page of this RStudio user interface cheatsheet.\n\n\n\n\n\n\n\n\nWindows/Linux\nMac\nAction\n\n\n\n\nEsc\nEsc\nInterrupt current command (useful if you accidentally ran an incomplete command and cannot escape seeing “+” in the R console)\n\n\nCtrl+s\nCmd+s\nSave (script)\n\n\nTab\nTab\nAuto-complete\n\n\nCtrl + Enter\nCmd + Enter\nRun current line(s)/selection of code\n\n\nCtrl + Shift + C\nCmd + Shift + c\ncomment/uncomment the highlighted lines\n\n\nAlt + -\nOption + -\nInsert &lt;-\n\n\nCtrl + Shift + m\nCmd + Shift + m\nInsert %&gt;%\n\n\nCtrl + l\nCmd + l\nClear the R console\n\n\nCtrl + Alt + b\nCmd + Option + b\nRun from start to current line\n\n\nCtrl + Alt + t\nCmd + Option + t\nRun the current code section (R Markdown)\n\n\nCtrl + Alt + i\nCmd + Shift + r\nInsert code chunk (into R Markdown)\n\n\nCtrl + Alt + c\nCmd + Option + c\nRun current code chunk (R Markdown)\n\n\nup/down arrows in R console\nSame\nToggle through recently run commands\n\n\nShift + up/down arrows in script\nSame\nSelect multiple code lines\n\n\nCtrl + f\nCmd + f\nFind and replace in current script\n\n\nCtrl + Shift + f\nCmd + Shift + f\nFind in files (search/replace across many scripts)\n\n\nAlt + l\nCmd + Option + l\nFold selected code\n\n\nShift + Alt + l\nCmd + Shift + Option+l\nUnfold selected code\n\n\n\n\n\n\n\n\n\n\n\n\nTake advantage of auto-complete\n\n\n\nUse your Tab key when typing to engage RStudio’s auto-complete functionality. This can prevent spelling errors. Press Tab while typing to produce a drop-down menu of likely functions and objects, based on what you have typed so far.\n\n\n\n\n\n\nNow that we know how to talk with R via the script editor or the console, we want to use R for something more than adding numbers. To do this, we need to know more about the R syntax.\nThe main “parts of speech” in R (syntax) include:\n\nthe comments # and how they are used to document function and its content\nvariables and functions\nthe assignment operator &lt;-\nthe = for arguments in functions\n\nWe will go through each of these “parts of speech” in more detail, starting with the assignment operator.\n\n\n\nTo do useful and interesting things in R, we need to assign values to variables using the assignment operator, &lt;-. For example, we can use the assignment operator to assign the value of 3 to x by executing:\n\nx &lt;- 3\n\nThe assignment operator (&lt;-) assigns values on the right to variables on the left.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key, on Mac type option + -) will write &lt;- in a single keystroke.\n\n\n\nA variable is a symbolic name for (or reference to) information. Variables in computer programming are analogous to “buckets”, where information can be maintained and referenced. On the outside of the bucket is a name. When referring to the bucket, we use the name of the bucket, not the data stored in the bucket.\nIn the example above, we created a variable or a ‘bucket’ called x. Inside we put a value, 3.\nLet’s create another variable called y and give it a value of 5.\n\ny &lt;- 5\n\nWhen assigning a value to an variable, R does not print anything to the console. You can force to print the value by using parentheses or by typing the variable name.\n\ny\n\nYou can also view information on the variable by looking in your Environment window in the upper right-hand corner of the RStudio interface.\n\n\n\nViewing your environment\n\n\nNow we can reference these buckets by name to perform mathematical operations on the values contained within. What do you get in the console for the following operation:\n\nx + y\n\nTry assigning the results of this operation to another variable called number.\n\nnumber &lt;- x + y\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "readings/rstudio.html#what-is-rstudio",
    "href": "readings/rstudio.html#what-is-rstudio",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "RStudio is freely available open-source Integrated Development Environment (IDE). RStudio provides an environment with many features to make using R easier and is a great alternative to working on R in the terminal.\nYou can see the complete RStudio user-interface cheatsheet (PDF) here"
  },
  {
    "objectID": "readings/rstudio.html#creating-a-new-project-directory-in-rstudio",
    "href": "readings/rstudio.html#creating-a-new-project-directory-in-rstudio",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "Let’s create a new project directory for the Research and Design course.\n\nOpen RStudio\nGo to the File menu and select New Project.\nIn the New Project window, choose New Directory. Then, choose New Project. Name your new directory whatever you want and then “Create the project as subdirectory of:” the Desktop (or location of your choice).\nClick on Create Project.\nAfter your project is completed, if the project does not automatically open in RStudio, then go to the File menu, select Open Project, and choose [your project name].Rproj.\nWhen RStudio opens, you will see three panels in the window.\nGo to the File menu and select New File, and select R Script. The RStudio interface should now look like the screenshot below.\n\nTIP: If your RStudio displays only one left pane it is because you have no scripts open yet.\n\n\n\nRStudio interface\n\n\n\n\nIt is simply a directory that contains everything related your analyses for a specific project. RStudio projects are useful when you are working on context- specific analyses and you wish to keep them separate. When creating a project in RStudio you associate it with a working directory of your choice (either an existing one, or a new one). A . RProj file is created within that directory and that keeps track of your command history and variables in the environment. The . RProj file can be used to open the project in its current state but at a later date.\nWhen a project is (re) opened within RStudio the following actions are taken:\n\nA new R session (process) is started\nThe .RData file in the project’s main directory is loaded, populating the environment with any objects that were present when the project was closed.\nThe .Rhistory file in the project’s main directory is loaded into the RStudio History pane (and used for Console Up/Down arrow command history).\nThe current working directory is set to the project directory.\nPreviously edited source documents are restored into editor tabs\nOther RStudio settings (e.g. active tabs, splitter positions, etc.) are restored to where they were the last time the project was closed.\n\nInformation adapted from RStudio Support Site"
  },
  {
    "objectID": "readings/rstudio.html#rstudio-interface",
    "href": "readings/rstudio.html#rstudio-interface",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "The RStudio interface has four main panels:\n\nConsole: where you can type commands and see output. The console is all you would see if you ran R in the command line without RStudio.\nScript editor: where you can type out commands and save to file. You can also submit the commands to run in the console.\nEnvironment/History: environment shows all active objects and history keeps track of all commands run in console\nFiles/Plots/Packages/Help"
  },
  {
    "objectID": "readings/rstudio.html#organizing-your-working-directory-setting-up",
    "href": "readings/rstudio.html#organizing-your-working-directory-setting-up",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "Before we organize our working directory, let’s check to see where our current working directory is located by typing into the console:\n\ngetwd()\n\nYour working directory should be the Intro-to-R folder constructed when you created the project. The working directory is where RStudio will automatically look for any files you bring in and where it will automatically save any files you create, unless otherwise specified.\nYou can visualize your working directory by selecting the Files tab from the Files/Plots/Packages/Help window.\n\n\n\nViewing your working directory\n\n\nIf you wanted to choose a different directory to be your working directory, you could navigate to a different folder in the Files tab, then, click on the More dropdown menu and select Set As Working Directory.\n\n\n\nSetting your working directory\n\n\n\n\n\nTo organize your working directory for a particular analysis, you typically want to separate the original data (raw data) from intermediate datasets. For instance, you may want to create a data/ directory within your working directory that stores the raw data, and have a results/ directory for intermediate datasets and a figures/ directory for the plots you will generate.\nLet’s create these three directories within your working directory by clicking on New Folder within the Files tab.\n\n\n\nStructuring your working directory\n\n\nWhen finished, your working directory should look like:\n\n\n\nYour organized working directory\n\n\n\n\n\nThis is more of a housekeeping task. We will be writing long lines of code in our script editor and want to make sure that the lines “wrap” and you don’t have to scroll back and forth to look at your long line of code.\nClick on “Tools” at the top of your RStudio screen and click on “Global Options” in the pull down menu.\n\n\n\noptions\n\n\nOn the left, select “Code” and put a check against “Soft-wrap R source files”. Make sure you click the “Apply” button at the bottom of the Window before saying “OK”.\n\n\n\nwrap_options"
  },
  {
    "objectID": "readings/rstudio.html#interacting-with-r",
    "href": "readings/rstudio.html#interacting-with-r",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "Now that we have our interface and directory structure set up, let’s start playing with R! There are two main ways of interacting with R in RStudio: using the console or by using script editor (plain text files that contain your code).\n\n\nThe console window (in RStudio, the bottom left panel) is the place where R is waiting for you to tell it what to do, and where it will show the results of a command. You can type commands directly into the console, but they will be forgotten when you close the session.\n\n\n\nRunning in the console\n\n\n\n\n\nBest practice is to enter the commands in the script editor, and save the script. You are encouraged to comment liberally to describe the commands you are running using #. This way, you have a complete record of what you did, you can easily show others how you did it and you can do it again later on if needed.\nThe Rstudio script editor allows you to ‘send’ the current line or the currently highlighted text to the R console by clicking on the Run button in the upper-right hand corner of the script editor. Alternatively, you can run by simply pressing the Ctrl and Enter keys at the same time as a shortcut.\nNow let’s try entering commands to the script editor and using the comments character # to add descriptions and highlighting the text to run:\n\n    # Session 1\n    # Feb 3, 2023\n\n    # Interacting with R\n    \n    # I am adding 3 and 5. \n    3+5\n\n\n\n\nRunning in the script editor\n\n\nYou should see the command run in the console and output the result.\n\n\n\nScript editor output\n\n\nWhat happens if we do that same command without the comment symbol #? Re-run the command after removing the # sign in the front:\n\nI am adding 3 and 5. R is fun!\n3+5\n\nNow R is trying to run that sentence as a command, and it doesn’t work. We get an error in the console “Error: unexpected symbol in”I am” means that the R interpreter did not know what to do with that command.”\n\n\n\nInterpreting the command prompt can help understand when R is ready to accept commands. Below lists the different states of the command prompt and how you can exit a command:\nConsole is ready to accept commands: &gt;.\nIf R is ready to accept commands, the R console shows a &gt; prompt.\nWhen the console receives a command (by directly typing into the console or running from the script editor (Ctrl-Enter), R will try to execute it.\nAfter running, the console will show the results and come back with a new &gt; prompt to wait for new commands.\nConsole is waiting for you to enter more data: +.\nIf R is still waiting for you to enter more data because it isn’t complete yet, the console will show a + prompt. It means that you haven’t finished entering a complete command. Often this can be due to you having not ‘closed’ a parenthesis or quotation.\nEscaping a command and getting a new prompt: esc\nIf you’re in Rstudio and you can’t figure out why your command isn’t running, you can click inside the console window and press esc to escape the command and bring back a new prompt &gt;.\n\n\n\nIn addition to some of the shortcuts described earlier in this lesson, we have listed a few more that can be helpful as you work in RStudio.\n\n\n\n\n\n\nShortcuts Table\n\n\n\n\n\nSome very useful keyboard shortcuts are below. See all the keyboard shortcuts for Windows, Max, and Linux in the second page of this RStudio user interface cheatsheet.\n\n\n\n\n\n\n\n\nWindows/Linux\nMac\nAction\n\n\n\n\nEsc\nEsc\nInterrupt current command (useful if you accidentally ran an incomplete command and cannot escape seeing “+” in the R console)\n\n\nCtrl+s\nCmd+s\nSave (script)\n\n\nTab\nTab\nAuto-complete\n\n\nCtrl + Enter\nCmd + Enter\nRun current line(s)/selection of code\n\n\nCtrl + Shift + C\nCmd + Shift + c\ncomment/uncomment the highlighted lines\n\n\nAlt + -\nOption + -\nInsert &lt;-\n\n\nCtrl + Shift + m\nCmd + Shift + m\nInsert %&gt;%\n\n\nCtrl + l\nCmd + l\nClear the R console\n\n\nCtrl + Alt + b\nCmd + Option + b\nRun from start to current line\n\n\nCtrl + Alt + t\nCmd + Option + t\nRun the current code section (R Markdown)\n\n\nCtrl + Alt + i\nCmd + Shift + r\nInsert code chunk (into R Markdown)\n\n\nCtrl + Alt + c\nCmd + Option + c\nRun current code chunk (R Markdown)\n\n\nup/down arrows in R console\nSame\nToggle through recently run commands\n\n\nShift + up/down arrows in script\nSame\nSelect multiple code lines\n\n\nCtrl + f\nCmd + f\nFind and replace in current script\n\n\nCtrl + Shift + f\nCmd + Shift + f\nFind in files (search/replace across many scripts)\n\n\nAlt + l\nCmd + Option + l\nFold selected code\n\n\nShift + Alt + l\nCmd + Shift + Option+l\nUnfold selected code\n\n\n\n\n\n\n\n\n\n\n\n\nTake advantage of auto-complete\n\n\n\nUse your Tab key when typing to engage RStudio’s auto-complete functionality. This can prevent spelling errors. Press Tab while typing to produce a drop-down menu of likely functions and objects, based on what you have typed so far."
  },
  {
    "objectID": "readings/rstudio.html#r-syntax",
    "href": "readings/rstudio.html#r-syntax",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "Now that we know how to talk with R via the script editor or the console, we want to use R for something more than adding numbers. To do this, we need to know more about the R syntax.\nThe main “parts of speech” in R (syntax) include:\n\nthe comments # and how they are used to document function and its content\nvariables and functions\nthe assignment operator &lt;-\nthe = for arguments in functions\n\nWe will go through each of these “parts of speech” in more detail, starting with the assignment operator."
  },
  {
    "objectID": "readings/rstudio.html#assignment-operator",
    "href": "readings/rstudio.html#assignment-operator",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "To do useful and interesting things in R, we need to assign values to variables using the assignment operator, &lt;-. For example, we can use the assignment operator to assign the value of 3 to x by executing:\n\nx &lt;- 3\n\nThe assignment operator (&lt;-) assigns values on the right to variables on the left.\nIn RStudio, typing Alt + - (push Alt at the same time as the - key, on Mac type option + -) will write &lt;- in a single keystroke."
  },
  {
    "objectID": "readings/rstudio.html#variables",
    "href": "readings/rstudio.html#variables",
    "title": "Introduction to RStudio",
    "section": "",
    "text": "A variable is a symbolic name for (or reference to) information. Variables in computer programming are analogous to “buckets”, where information can be maintained and referenced. On the outside of the bucket is a name. When referring to the bucket, we use the name of the bucket, not the data stored in the bucket.\nIn the example above, we created a variable or a ‘bucket’ called x. Inside we put a value, 3.\nLet’s create another variable called y and give it a value of 5.\n\ny &lt;- 5\n\nWhen assigning a value to an variable, R does not print anything to the console. You can force to print the value by using parentheses or by typing the variable name.\n\ny\n\nYou can also view information on the variable by looking in your Environment window in the upper right-hand corner of the RStudio interface.\n\n\n\nViewing your environment\n\n\nNow we can reference these buckets by name to perform mathematical operations on the values contained within. What do you get in the console for the following operation:\n\nx + y\n\nTry assigning the results of this operation to another variable called number.\n\nnumber &lt;- x + y\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/). These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "readings/survey-analysis.html",
    "href": "readings/survey-analysis.html",
    "title": "Survey analysis",
    "section": "",
    "text": "Extended Materials\n\n\n\nYou can find the original, extended version of this chapter here.\n\n\n\n\n\nIdeally, survey respondents represent a completely random sample of the study population. However, this is rarely the case. Selection bias, non-random patterns in who responds to a survey, as well as other biases can influence the demographic makeup of survey respondents to be different than that of the study population. To help combat this large-scale surveys, include NHANES, often include survey weights.\nIn this chapter, we will use survey weights to calculate estimated statistics for an entire population based on a survey sample. Most survey R packages rely on the survey package for doing weighted analysis.\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\n## load packages from CRAN\npacman::p_load(rio,          # File import\n               here,         # File locator\n               tidyverse,    # data management + ggplot2 graphics\n               tsibble,      # handle time series datasets\n               survey,       # for survey functions\n               gtsummary,    # wrapper for survey package to produce tables\n               apyramid,     # a package dedicated to creating age pyramids\n               patchwork,    # for combining ggplots\n               ggforce,      # for alluvial/sankey plots\n               epikit        # for age_categories \n               ) \n\n\n\n\nThe example dataset used in this section:\n\nfictional mortality survey data.\nfictional population counts for the survey area.\ndata dictionary for the fictional mortality survey data.\n\nThis is based off the MSF OCA ethical review board pre-approved survey. The fictional dataset was produced as part of the “R4Epis” project. This is all based off data collected using KoboToolbox, which is a data collection software based off Open Data Kit.\nKobo allows you to export both the collected data, as well as the data dictionary for that dataset. We strongly recommend doing this as it simplifies data cleaning and is useful for looking up variables/questions.\n\n# import the survey data\nsurvey_data &lt;- rio::import(\"survey_data.xlsx\")\n\n# import the dictionary into R\nsurvey_dict &lt;- rio::import(\"survey_dict.xlsx\") \n\nThe first 10 rows of the survey are displayed below.\n\n\n\n\n\n\n\nWe also want to import the data on sampling population so that we can produce appropriate weights. This data can be in different formats, however we would suggest to have it as seen below (this can just be typed in to an excel).\n\n# import the population data\npopulation &lt;- rio::import(\"population.xlsx\")\n\nThe first 10 rows of the survey are displayed below.\n\n\n\n\n\n\n\nFor cluster surveys you may want to add survey weights at the cluster level. You could read this data in as above. Alternatively if there are only a few counts, these could be entered as below in to a tibble. In any case you will need to have one column with a cluster identifier which matches your survey data, and another column with the number of households in each cluster.\n\n## define the number of households in each cluster\ncluster_counts &lt;- tibble(cluster = c(\"village_1\", \"village_2\", \"village_3\", \"village_4\", \n                                     \"village_5\", \"village_6\", \"village_7\", \"village_8\",\n                                     \"village_9\", \"village_10\"), \n                         households = c(700, 400, 600, 500, 300, \n                                        800, 700, 400, 500, 500))\n\n\n\n\nThe below makes sure that the date column is in the appropriate format. There are several other ways of doing this, however using the dictionary to define dates is quick and easy.\nWe also create an age group variable using the age_categories() function from epikit. In addition, we create a character variable defining which district the various clusters are in. Finally, we recode all of the yes/no variables to TRUE/FALSE variables - otherwise these cant be used by the survey proportion functions.\n\n## select the date variable names from the dictionary \nDATEVARS &lt;- survey_dict %&gt;% \n  filter(type == \"date\") %&gt;% \n  filter(name %in% names(survey_data)) %&gt;% \n  ## filter to match the column names of your data\n  pull(name) # select date vars\n  \n## change to dates \nsurvey_data &lt;- survey_data %&gt;%\n  mutate(across(all_of(DATEVARS), as.Date))\n\n\n## add those with only age in months to the year variable (divide by twelve)\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(age_years = if_else(is.na(age_years), \n                             age_months / 12, \n                             age_years))\n\n## define age group variable\nsurvey_data &lt;- survey_data %&gt;% \n     mutate(age_group = age_categories(age_years, \n                                    breakers = c(0, 3, 15, 30, 45)\n                                    ))\n\n\n## create a character variable based off groups of a different variable \nsurvey_data &lt;- survey_data %&gt;% \n  mutate(health_district = case_when(\n    cluster_number %in% c(1:5) ~ \"district_a\", \n    TRUE ~ \"district_b\"\n  ))\n\n\n## select the yes/no variable names from the dictionary \nYNVARS &lt;- survey_dict %&gt;% \n  filter(type == \"yn\") %&gt;% \n  filter(name %in% names(survey_data)) %&gt;% \n  ## filter to match the column names of your data\n  pull(name) # select yn vars\n  \n## change to dates \nsurvey_data &lt;- survey_data %&gt;%\n  mutate(across(all_of(YNVARS), \n                str_detect, \n                pattern = \"yes\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(all_of(YNVARS), str_detect, pattern = \"yes\")`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\nThere numerous different sampling designs that can be used for surveys. Here we will demonstrate code for: - Stratified - Cluster - Stratified and cluster\nAs described above (depending on how you design your questionnaire) the data for each level would be exported as a separate dataset. In our example there is one level for households and one level for individuals within those households.\nThese two levels are linked by a unique identifier. For a Kobo dataset this variable is “_index” at the household level, which matches the “_parent_index” at the individual level. This will create new rows for household with each matching individual, see the handbook section on joining for details.\n\n## join the individual and household data to form a complete data set\nsurvey_data &lt;- left_join(survey_data_hh, \n                         survey_data_indiv,\n                         by = c(\"_index\" = \"_parent_index\"))\n\n\n## create a unique identifier by combining indeces of the two levels \nsurvey_data &lt;- survey_data %&gt;% \n     mutate(uid = str_glue(\"{index}_{index_y}\"))\n\n\n\n\n\nFor mortality surveys we want to now how long each individual was present for in the location to be able to calculate an appropriate mortality rate for our period of interest. This is not relevant to all surveys, but particularly for mortality surveys this is important as they are conducted frequently among mobile or displaced populations.\nTo do this we first define our time period of interest, also known as a recall period (i.e. the time that participants are asked to report on when answering questions). We can then use this period to set inappropriate dates to missing, i.e. if deaths are reported from outside the period of interest.\n\n## set the start/end of recall period\n## can be changed to date variables from dataset \n## (e.g. arrival date & date questionnaire)\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(recall_start = as.Date(\"2018-01-01\"), \n         recall_end   = as.Date(\"2018-05-01\")\n  )\n\n\n# set inappropriate dates to NA based on rules \n## e.g. arrivals before start, departures departures after end\nsurvey_data &lt;- survey_data %&gt;%\n      mutate(\n           arrived_date = if_else(arrived_date &lt; recall_start, \n                                 as.Date(NA),\n                                  arrived_date),\n           birthday_date = if_else(birthday_date &lt; recall_start,\n                                  as.Date(NA),\n                                  birthday_date),\n           left_date = if_else(left_date &gt; recall_end,\n                              as.Date(NA),\n                               left_date),\n           death_date = if_else(death_date &gt; recall_end,\n                               as.Date(NA),\n                               death_date)\n           )\n\nWe can then use our date variables to define start and end dates for each individual. We can use the find_start_date() function from sitrep to fine the causes for the dates and then use that to calculate the difference between days (person-time).\nstart date: Earliest appropriate arrival event within your recall period Either the beginning of your recall period (which you define in advance), or a date after the start of recall if applicable (e.g. arrivals or births)\nend date: Earliest appropriate departure event within your recall period Either the end of your recall period, or a date before the end of recall if applicable (e.g. departures, deaths)\n\n## create new variables for start and end dates/causes\nsurvey_data &lt;- survey_data %&gt;% \n     ## choose earliest date entered in survey\n     ## from births, household arrivals, and camp arrivals \n     find_start_date(\"birthday_date\",\n                  \"arrived_date\",\n                  period_start = \"recall_start\",\n                  period_end   = \"recall_end\",\n                  datecol      = \"startdate\",\n                  datereason   = \"startcause\" \n                 ) %&gt;%\n     ## choose earliest date entered in survey\n     ## from camp departures, death and end of the study\n     find_end_date(\"left_date\",\n                \"death_date\",\n                period_start = \"recall_start\",\n                period_end   = \"recall_end\",\n                datecol      = \"enddate\",\n                datereason   = \"endcause\" \n               )\n\n\n## label those that were present at the start/end (except births/deaths)\nsurvey_data &lt;- survey_data %&gt;% \n     mutate(\n       ## fill in start date to be the beginning of recall period (for those empty) \n       startdate = if_else(is.na(startdate), recall_start, startdate), \n       ## set the start cause to present at start if equal to recall period \n       ## unless it is equal to the birth date \n       startcause = if_else(startdate == recall_start & startcause != \"birthday_date\",\n                              \"Present at start\", startcause), \n       ## fill in end date to be end of recall period (for those empty) \n       enddate = if_else(is.na(enddate), recall_end, enddate), \n       ## set the end cause to present at end if equall to recall end \n       ## unless it is equal to the death date\n       endcause = if_else(enddate == recall_end & endcause != \"death_date\", \n                            \"Present at end\", endcause))\n\n\n## Define observation time in days\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(obstime = as.numeric(enddate - startdate))\n\n\n\n\n\nIt is important that you drop erroneous observations before adding survey weights. For example if you have observations with negative observation time, you will need to check those (you can do this with the assert_positive_timespan() function from sitrep. Another thing is if you want to drop empty rows (e.g. with drop_na(uid)) or remove duplicates (see handbook section on [De-duplication] for details). Those without consent need to be dropped too.\nIn this example we filter for the cases we want to drop and store them in a separate data frame - this way we can describe those that were excluded from the survey. We then use the anti_join() function from dplyr to remove these dropped cases from our survey data.\n\n\n\n\n\n\nWarning\n\n\n\nYou cant have missing values in your weight variable, or any of the variables relevant to your survey design (e.g. age, sex, strata or cluster variables).\n\n\n\n## store the cases that you drop so you can describe them (e.g. non-consenting \n## or wrong village/cluster)\ndropped &lt;- survey_data %&gt;% \n  filter(!consent | is.na(startdate) | is.na(enddate) | village_name == \"other\")\n\n## use the dropped cases to remove the unused rows from the survey data set  \nsurvey_data &lt;- anti_join(survey_data, dropped, by = names(dropped))\n\nAs mentioned above we demonstrate how to add weights for three different study designs (stratified, cluster and stratified cluster). These require information on the source population and/or the clusters surveyed. We will use the stratified cluster code for this example, but use whichever is most appropriate for your study design.\n\n# stratified ------------------------------------------------------------------\n# create a variable called \"surv_weight_strata\"\n# contains weights for each individual - by age group, sex and health district\nsurvey_data &lt;- add_weights_strata(x = survey_data,\n                                         p = population,\n                                         surv_weight = \"surv_weight_strata\",\n                                         surv_weight_ID = \"surv_weight_ID_strata\",\n                                         age_group, sex, health_district)\n\n## cluster ---------------------------------------------------------------------\n\n# get the number of people of individuals interviewed per household\n# adds a variable with counts of the household (parent) index variable\nsurvey_data &lt;- survey_data %&gt;%\n  add_count(index, name = \"interviewed\")\n\n\n## create cluster weights\nsurvey_data &lt;- add_weights_cluster(x = survey_data,\n                                          cl = cluster_counts,\n                                          eligible = member_number,\n                                          interviewed = interviewed,\n                                          cluster_x = village_name,\n                                          cluster_cl = cluster,\n                                          household_x = index,\n                                          household_cl = households,\n                                          surv_weight = \"surv_weight_cluster\",\n                                          surv_weight_ID = \"surv_weight_ID_cluster\",\n                                          ignore_cluster = FALSE,\n                                          ignore_household = FALSE)\n\n\n# stratified and cluster ------------------------------------------------------\n# create a survey weight for cluster and strata\nsurvey_data &lt;- survey_data %&gt;%\n  mutate(surv_weight_cluster_strata = surv_weight_strata * surv_weight_cluster)\n\n\n\n\n\nCreate survey object according to your study design. Used the same way as data frames to calculate weight proportions etc. Make sure that all necessary variables are created before this.\nThere are four options, comment out those you do not use: - Simple random - Stratified - Cluster - Stratified cluster\nFor this template - we will pretend that we cluster surveys in two separate strata (health districts A and B). So to get overall estimates we need have combined cluster and strata weights.\nAs mentioned previously, there are two packages available for doing this. The classic one is survey and then there is a wrapper package called srvyr that makes tidyverse-friendly objects and functions. We will demonstrate both, but note that most of the code in this chapter will use srvyr based objects. The one exception is that the gtsummary package only accepts survey objects.\n\n\nThe survey package effectively uses base R coding, and so it is not possible to use pipes (%&gt;%) or other dplyr syntax. With the survey package we use the svydesign() function to define a survey object with appropriate clusters, weights and strata.\nNOTE: we need to use the tilde (~) in front of variables, this is because the package uses the base R syntax of assigning variables based on formulae. \n\n# simple random ---------------------------------------------------------------\nbase_survey_design_simple &lt;- svydesign(ids = ~1, # 1 for no cluster ids\n                   weights = NULL,               # No weight added\n                   strata = NULL,                # sampling was simple (no strata)\n                   data = survey_data            # have to specify the dataset\n                  )\n\n## stratified ------------------------------------------------------------------\nbase_survey_design_strata &lt;- svydesign(ids = ~1,  # 1 for no cluster ids\n                   weights = ~surv_weight_strata, # weight variable created above\n                   strata = ~health_district,     # sampling was stratified by district\n                   data = survey_data             # have to specify the dataset\n                  )\n\n# cluster ---------------------------------------------------------------------\nbase_survey_design_cluster &lt;- svydesign(ids = ~village_name, # cluster ids\n                   weights = ~surv_weight_cluster, # weight variable created above\n                   strata = NULL,                 # sampling was simple (no strata)\n                   data = survey_data              # have to specify the dataset\n                  )\n\n# stratified cluster ----------------------------------------------------------\nbase_survey_design &lt;- svydesign(ids = ~village_name,      # cluster ids\n                   weights = ~surv_weight_cluster_strata, # weight variable created above\n                   strata = ~health_district,             # sampling was stratified by district\n                   data = survey_data                     # have to specify the dataset\n                  )\n\n\n\n\n\n\nIn this section we will focus on how to investigate and visualize bias in a sample. We will also look at visualising population flow in a survey setting using alluvial/sankey diagrams.\nIn general, you should consider including the following descriptive analyses:\n\nFinal number of clusters, households and individuals included\n\nNumber of excluded individuals and the reasons for exclusion\nMedian (range) number of households per cluster and individuals per household\n\n\n\nCompare the proportions in each age group between your sample and the source population. This is important to be able to highlight potential sampling bias. You could similarly repeat this looking at distributions by sex.\nNote that these p-values are just indicative, and a descriptive discussion (or visualisation with age-pyramids below) of the distributions in your study sample compared to the source population is more important than the binomial test itself. This is because increasing sample size will more often than not lead to differences that may be irrelevant after weighting your data.\n\n## counts and props of the study population\nag &lt;- survey_data %&gt;% \n  group_by(age_group) %&gt;% \n  drop_na(age_group) %&gt;% \n  tally() %&gt;% \n  mutate(proportion = n / sum(n), \n         n_total = sum(n))\n\n## counts and props of the source population\npropcount &lt;- population %&gt;% \n  group_by(age_group) %&gt;%\n    tally(population) %&gt;%\n    mutate(proportion = n / sum(n))\n\n## bind together the columns of two tables, group by age, and perform a \n## binomial test to see if n/total is significantly different from population\n## proportion.\n  ## suffix here adds to text to the end of columns in each of the two datasets\nleft_join(ag, propcount, by = \"age_group\", suffix = c(\"\", \"_pop\")) %&gt;%\n  group_by(age_group) %&gt;%\n  ## broom::tidy(binom.test()) makes a data frame out of the binomial test and\n  ## will add the variables p.value, parameter, conf.low, conf.high, method, and\n  ## alternative. We will only use p.value here. You can include other\n  ## columns if you want to report confidence intervals\n  mutate(binom = list(broom::tidy(binom.test(n, n_total, proportion_pop)))) %&gt;%\n  unnest(cols = c(binom)) %&gt;% # important for expanding the binom.test data frame\n  mutate(proportion_pop = proportion_pop * 100) %&gt;%\n  ## Adjusting the p-values to correct for false positives \n  ## (because testing multiple age groups). This will only make \n  ## a difference if you have many age categories\n  mutate(p.value = p.adjust(p.value, method = \"holm\")) %&gt;%\n                      \n  ## Only show p-values over 0.001 (those under report as &lt;0.001)\n  mutate(p.value = ifelse(p.value &lt; 0.001, \n                          \"&lt;0.001\", \n                          as.character(round(p.value, 3)))) %&gt;% \n  \n  ## rename the columns appropriately\n  select(\n    \"Age group\" = age_group,\n    \"Study population (n)\" = n,\n    \"Study population (%)\" = proportion,\n    \"Source population (n)\" = n_pop,\n    \"Source population (%)\" = proportion_pop,\n    \"P-value\" = p.value\n  )\n\n# A tibble: 5 × 6\n# Groups:   Age group [5]\n  `Age group` `Study population (n)` `Study population (%)`\n  &lt;chr&gt;                        &lt;int&gt;                  &lt;dbl&gt;\n1 0-2                             12                 0.0256\n2 3-14                            42                 0.0896\n3 15-29                           64                 0.136 \n4 30-44                           52                 0.111 \n5 45+                            299                 0.638 \n# ℹ 3 more variables: `Source population (n)` &lt;dbl&gt;,\n#   `Source population (%)` &lt;dbl&gt;, `P-value` &lt;chr&gt;\n\n\n\n\n\n\n\nThis section will detail how to produce tables for weighted counts and proportions, with associated confidence intervals and design effect.\n\n\nWe can use the svyciprop() function from survey to get weighted proportions and accompanying 95% confidence intervals. An appropriate design effect can be extracted using the svymean() rather than svyprop() function. It is worth noting that svyprop() only appears to accept variables between 0 and 1 (or TRUE/FALSE), so categorical variables will not work.\nNOTE: Functions from survey also accept srvyr design objects, but here we have used the survey design object just for consistency \n\n## produce weighted counts \nsvytable(~died, base_survey_design)\n\ndied\n     FALSE       TRUE \n1406244.43   76213.01 \n\n## produce weighted proportions\nsvyciprop(~died, base_survey_design, na.rm = T)\n\n              2.5% 97.5%\ndied 0.0514 0.0208  0.12\n\n## get the design effect \nsvymean(~died, base_survey_design, na.rm = T, deff = T) %&gt;% \n  deff()\n\ndiedFALSE  diedTRUE \n 3.755508  3.755508 \n\n\nWe can combine the functions from survey shown above in to a function which we define ourselves below, called svy_prop; and we can then use that function together with map() from the purrr package to iterate over several variables and create a table. See the handbook iteration chapter for details on purrr.\n\n# Define function to calculate weighted counts, proportions, CI and design effect\n# x is the variable in quotation marks \n# design is your survey design object\n\nsvy_prop &lt;- function(design, x) {\n  \n  ## put the variable of interest in a formula \n  form &lt;- as.formula(paste0( \"~\" , x))\n  ## only keep the TRUE column of counts from svytable\n  weighted_counts &lt;- svytable(form, design)[[2]]\n  ## calculate proportions (multiply by 100 to get percentages)\n  weighted_props &lt;- svyciprop(form, design, na.rm = TRUE) * 100\n  ## extract the confidence intervals and multiply to get percentages\n  weighted_confint &lt;- confint(weighted_props) * 100\n  ## use svymean to calculate design effect and only keep the TRUE column\n  design_eff &lt;- deff(svymean(form, design, na.rm = TRUE, deff = TRUE))[[TRUE]]\n  \n  ## combine in to one data frame\n  full_table &lt;- cbind(\n    \"Variable\"        = x,\n    \"Count\"           = weighted_counts,\n    \"Proportion\"      = weighted_props,\n    weighted_confint, \n    \"Design effect\"   = design_eff\n    )\n  \n  ## return table as a dataframe\n  full_table &lt;- data.frame(full_table, \n             ## remove the variable names from rows (is a separate column now)\n             row.names = NULL)\n  \n  ## change numerics back to numeric\n  full_table[ , 2:6] &lt;- as.numeric(full_table[, 2:6])\n  \n  ## return dataframe\n  full_table\n}\n\n## iterate over several variables to create a table \npurrr::map(\n  ## define variables of interest\n  c(\"left\", \"died\", \"arrived\"), \n  ## state function using and arguments for that function (design)\n  svy_prop, design = base_survey_design) %&gt;% \n  ## collapse list in to a single data frame\n  bind_rows() %&gt;% \n  ## round \n  mutate(across(where(is.numeric), round, digits = 1))\n\n  Variable    Count Proportion X2.5. X97.5. Design.effect\n1     left 701199.1       47.3  39.2   55.5           2.4\n2     died  76213.0        5.1   2.1   12.1           3.8\n3  arrived 761799.0       51.4  40.9   61.7           3.9\n\n\n\n\n\n\n\nSimilarly for weighted ratios (such as for mortality ratios) you can use survey:::svyratio:\n\n\n\nratio &lt;- svyratio(~died, \n         denominator = ~obstime, \n         design = base_survey_design)\n\nci &lt;- confint(ratio)\n\ncbind(\n  ratio$ratio * 10000, \n  ci * 10000\n)\n\n      obstime    2.5 %   97.5 %\ndied 5.981922 1.194294 10.76955"
  },
  {
    "objectID": "readings/survey-analysis.html#overview",
    "href": "readings/survey-analysis.html#overview",
    "title": "Survey analysis",
    "section": "",
    "text": "Ideally, survey respondents represent a completely random sample of the study population. However, this is rarely the case. Selection bias, non-random patterns in who responds to a survey, as well as other biases can influence the demographic makeup of survey respondents to be different than that of the study population. To help combat this large-scale surveys, include NHANES, often include survey weights.\nIn this chapter, we will use survey weights to calculate estimated statistics for an entire population based on a survey sample. Most survey R packages rely on the survey package for doing weighted analysis.\n\n\n\n\n\n\n\nData Preparation\n\n\n\n\n\n\n\n\n## load packages from CRAN\npacman::p_load(rio,          # File import\n               here,         # File locator\n               tidyverse,    # data management + ggplot2 graphics\n               tsibble,      # handle time series datasets\n               survey,       # for survey functions\n               gtsummary,    # wrapper for survey package to produce tables\n               apyramid,     # a package dedicated to creating age pyramids\n               patchwork,    # for combining ggplots\n               ggforce,      # for alluvial/sankey plots\n               epikit        # for age_categories \n               ) \n\n\n\n\nThe example dataset used in this section:\n\nfictional mortality survey data.\nfictional population counts for the survey area.\ndata dictionary for the fictional mortality survey data.\n\nThis is based off the MSF OCA ethical review board pre-approved survey. The fictional dataset was produced as part of the “R4Epis” project. This is all based off data collected using KoboToolbox, which is a data collection software based off Open Data Kit.\nKobo allows you to export both the collected data, as well as the data dictionary for that dataset. We strongly recommend doing this as it simplifies data cleaning and is useful for looking up variables/questions.\n\n# import the survey data\nsurvey_data &lt;- rio::import(\"survey_data.xlsx\")\n\n# import the dictionary into R\nsurvey_dict &lt;- rio::import(\"survey_dict.xlsx\") \n\nThe first 10 rows of the survey are displayed below.\n\n\n\n\n\n\n\nWe also want to import the data on sampling population so that we can produce appropriate weights. This data can be in different formats, however we would suggest to have it as seen below (this can just be typed in to an excel).\n\n# import the population data\npopulation &lt;- rio::import(\"population.xlsx\")\n\nThe first 10 rows of the survey are displayed below.\n\n\n\n\n\n\n\nFor cluster surveys you may want to add survey weights at the cluster level. You could read this data in as above. Alternatively if there are only a few counts, these could be entered as below in to a tibble. In any case you will need to have one column with a cluster identifier which matches your survey data, and another column with the number of households in each cluster.\n\n## define the number of households in each cluster\ncluster_counts &lt;- tibble(cluster = c(\"village_1\", \"village_2\", \"village_3\", \"village_4\", \n                                     \"village_5\", \"village_6\", \"village_7\", \"village_8\",\n                                     \"village_9\", \"village_10\"), \n                         households = c(700, 400, 600, 500, 300, \n                                        800, 700, 400, 500, 500))\n\n\n\n\nThe below makes sure that the date column is in the appropriate format. There are several other ways of doing this, however using the dictionary to define dates is quick and easy.\nWe also create an age group variable using the age_categories() function from epikit. In addition, we create a character variable defining which district the various clusters are in. Finally, we recode all of the yes/no variables to TRUE/FALSE variables - otherwise these cant be used by the survey proportion functions.\n\n## select the date variable names from the dictionary \nDATEVARS &lt;- survey_dict %&gt;% \n  filter(type == \"date\") %&gt;% \n  filter(name %in% names(survey_data)) %&gt;% \n  ## filter to match the column names of your data\n  pull(name) # select date vars\n  \n## change to dates \nsurvey_data &lt;- survey_data %&gt;%\n  mutate(across(all_of(DATEVARS), as.Date))\n\n\n## add those with only age in months to the year variable (divide by twelve)\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(age_years = if_else(is.na(age_years), \n                             age_months / 12, \n                             age_years))\n\n## define age group variable\nsurvey_data &lt;- survey_data %&gt;% \n     mutate(age_group = age_categories(age_years, \n                                    breakers = c(0, 3, 15, 30, 45)\n                                    ))\n\n\n## create a character variable based off groups of a different variable \nsurvey_data &lt;- survey_data %&gt;% \n  mutate(health_district = case_when(\n    cluster_number %in% c(1:5) ~ \"district_a\", \n    TRUE ~ \"district_b\"\n  ))\n\n\n## select the yes/no variable names from the dictionary \nYNVARS &lt;- survey_dict %&gt;% \n  filter(type == \"yn\") %&gt;% \n  filter(name %in% names(survey_data)) %&gt;% \n  ## filter to match the column names of your data\n  pull(name) # select yn vars\n  \n## change to dates \nsurvey_data &lt;- survey_data %&gt;%\n  mutate(across(all_of(YNVARS), \n                str_detect, \n                pattern = \"yes\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(all_of(YNVARS), str_detect, pattern = \"yes\")`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))"
  },
  {
    "objectID": "readings/survey-analysis.html#survey-data",
    "href": "readings/survey-analysis.html#survey-data",
    "title": "Survey analysis",
    "section": "",
    "text": "There numerous different sampling designs that can be used for surveys. Here we will demonstrate code for: - Stratified - Cluster - Stratified and cluster\nAs described above (depending on how you design your questionnaire) the data for each level would be exported as a separate dataset. In our example there is one level for households and one level for individuals within those households.\nThese two levels are linked by a unique identifier. For a Kobo dataset this variable is “_index” at the household level, which matches the “_parent_index” at the individual level. This will create new rows for household with each matching individual, see the handbook section on joining for details.\n\n## join the individual and household data to form a complete data set\nsurvey_data &lt;- left_join(survey_data_hh, \n                         survey_data_indiv,\n                         by = c(\"_index\" = \"_parent_index\"))\n\n\n## create a unique identifier by combining indeces of the two levels \nsurvey_data &lt;- survey_data %&gt;% \n     mutate(uid = str_glue(\"{index}_{index_y}\"))"
  },
  {
    "objectID": "readings/survey-analysis.html#observation-time",
    "href": "readings/survey-analysis.html#observation-time",
    "title": "Survey analysis",
    "section": "",
    "text": "For mortality surveys we want to now how long each individual was present for in the location to be able to calculate an appropriate mortality rate for our period of interest. This is not relevant to all surveys, but particularly for mortality surveys this is important as they are conducted frequently among mobile or displaced populations.\nTo do this we first define our time period of interest, also known as a recall period (i.e. the time that participants are asked to report on when answering questions). We can then use this period to set inappropriate dates to missing, i.e. if deaths are reported from outside the period of interest.\n\n## set the start/end of recall period\n## can be changed to date variables from dataset \n## (e.g. arrival date & date questionnaire)\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(recall_start = as.Date(\"2018-01-01\"), \n         recall_end   = as.Date(\"2018-05-01\")\n  )\n\n\n# set inappropriate dates to NA based on rules \n## e.g. arrivals before start, departures departures after end\nsurvey_data &lt;- survey_data %&gt;%\n      mutate(\n           arrived_date = if_else(arrived_date &lt; recall_start, \n                                 as.Date(NA),\n                                  arrived_date),\n           birthday_date = if_else(birthday_date &lt; recall_start,\n                                  as.Date(NA),\n                                  birthday_date),\n           left_date = if_else(left_date &gt; recall_end,\n                              as.Date(NA),\n                               left_date),\n           death_date = if_else(death_date &gt; recall_end,\n                               as.Date(NA),\n                               death_date)\n           )\n\nWe can then use our date variables to define start and end dates for each individual. We can use the find_start_date() function from sitrep to fine the causes for the dates and then use that to calculate the difference between days (person-time).\nstart date: Earliest appropriate arrival event within your recall period Either the beginning of your recall period (which you define in advance), or a date after the start of recall if applicable (e.g. arrivals or births)\nend date: Earliest appropriate departure event within your recall period Either the end of your recall period, or a date before the end of recall if applicable (e.g. departures, deaths)\n\n## create new variables for start and end dates/causes\nsurvey_data &lt;- survey_data %&gt;% \n     ## choose earliest date entered in survey\n     ## from births, household arrivals, and camp arrivals \n     find_start_date(\"birthday_date\",\n                  \"arrived_date\",\n                  period_start = \"recall_start\",\n                  period_end   = \"recall_end\",\n                  datecol      = \"startdate\",\n                  datereason   = \"startcause\" \n                 ) %&gt;%\n     ## choose earliest date entered in survey\n     ## from camp departures, death and end of the study\n     find_end_date(\"left_date\",\n                \"death_date\",\n                period_start = \"recall_start\",\n                period_end   = \"recall_end\",\n                datecol      = \"enddate\",\n                datereason   = \"endcause\" \n               )\n\n\n## label those that were present at the start/end (except births/deaths)\nsurvey_data &lt;- survey_data %&gt;% \n     mutate(\n       ## fill in start date to be the beginning of recall period (for those empty) \n       startdate = if_else(is.na(startdate), recall_start, startdate), \n       ## set the start cause to present at start if equal to recall period \n       ## unless it is equal to the birth date \n       startcause = if_else(startdate == recall_start & startcause != \"birthday_date\",\n                              \"Present at start\", startcause), \n       ## fill in end date to be end of recall period (for those empty) \n       enddate = if_else(is.na(enddate), recall_end, enddate), \n       ## set the end cause to present at end if equall to recall end \n       ## unless it is equal to the death date\n       endcause = if_else(enddate == recall_end & endcause != \"death_date\", \n                            \"Present at end\", endcause))\n\n\n## Define observation time in days\nsurvey_data &lt;- survey_data %&gt;% \n  mutate(obstime = as.numeric(enddate - startdate))"
  },
  {
    "objectID": "readings/survey-analysis.html#weighting",
    "href": "readings/survey-analysis.html#weighting",
    "title": "Survey analysis",
    "section": "",
    "text": "It is important that you drop erroneous observations before adding survey weights. For example if you have observations with negative observation time, you will need to check those (you can do this with the assert_positive_timespan() function from sitrep. Another thing is if you want to drop empty rows (e.g. with drop_na(uid)) or remove duplicates (see handbook section on [De-duplication] for details). Those without consent need to be dropped too.\nIn this example we filter for the cases we want to drop and store them in a separate data frame - this way we can describe those that were excluded from the survey. We then use the anti_join() function from dplyr to remove these dropped cases from our survey data.\n\n\n\n\n\n\nWarning\n\n\n\nYou cant have missing values in your weight variable, or any of the variables relevant to your survey design (e.g. age, sex, strata or cluster variables).\n\n\n\n## store the cases that you drop so you can describe them (e.g. non-consenting \n## or wrong village/cluster)\ndropped &lt;- survey_data %&gt;% \n  filter(!consent | is.na(startdate) | is.na(enddate) | village_name == \"other\")\n\n## use the dropped cases to remove the unused rows from the survey data set  \nsurvey_data &lt;- anti_join(survey_data, dropped, by = names(dropped))\n\nAs mentioned above we demonstrate how to add weights for three different study designs (stratified, cluster and stratified cluster). These require information on the source population and/or the clusters surveyed. We will use the stratified cluster code for this example, but use whichever is most appropriate for your study design.\n\n# stratified ------------------------------------------------------------------\n# create a variable called \"surv_weight_strata\"\n# contains weights for each individual - by age group, sex and health district\nsurvey_data &lt;- add_weights_strata(x = survey_data,\n                                         p = population,\n                                         surv_weight = \"surv_weight_strata\",\n                                         surv_weight_ID = \"surv_weight_ID_strata\",\n                                         age_group, sex, health_district)\n\n## cluster ---------------------------------------------------------------------\n\n# get the number of people of individuals interviewed per household\n# adds a variable with counts of the household (parent) index variable\nsurvey_data &lt;- survey_data %&gt;%\n  add_count(index, name = \"interviewed\")\n\n\n## create cluster weights\nsurvey_data &lt;- add_weights_cluster(x = survey_data,\n                                          cl = cluster_counts,\n                                          eligible = member_number,\n                                          interviewed = interviewed,\n                                          cluster_x = village_name,\n                                          cluster_cl = cluster,\n                                          household_x = index,\n                                          household_cl = households,\n                                          surv_weight = \"surv_weight_cluster\",\n                                          surv_weight_ID = \"surv_weight_ID_cluster\",\n                                          ignore_cluster = FALSE,\n                                          ignore_household = FALSE)\n\n\n# stratified and cluster ------------------------------------------------------\n# create a survey weight for cluster and strata\nsurvey_data &lt;- survey_data %&gt;%\n  mutate(surv_weight_cluster_strata = surv_weight_strata * surv_weight_cluster)"
  },
  {
    "objectID": "readings/survey-analysis.html#survey-design-objects",
    "href": "readings/survey-analysis.html#survey-design-objects",
    "title": "Survey analysis",
    "section": "",
    "text": "Create survey object according to your study design. Used the same way as data frames to calculate weight proportions etc. Make sure that all necessary variables are created before this.\nThere are four options, comment out those you do not use: - Simple random - Stratified - Cluster - Stratified cluster\nFor this template - we will pretend that we cluster surveys in two separate strata (health districts A and B). So to get overall estimates we need have combined cluster and strata weights.\nAs mentioned previously, there are two packages available for doing this. The classic one is survey and then there is a wrapper package called srvyr that makes tidyverse-friendly objects and functions. We will demonstrate both, but note that most of the code in this chapter will use srvyr based objects. The one exception is that the gtsummary package only accepts survey objects.\n\n\nThe survey package effectively uses base R coding, and so it is not possible to use pipes (%&gt;%) or other dplyr syntax. With the survey package we use the svydesign() function to define a survey object with appropriate clusters, weights and strata.\nNOTE: we need to use the tilde (~) in front of variables, this is because the package uses the base R syntax of assigning variables based on formulae. \n\n# simple random ---------------------------------------------------------------\nbase_survey_design_simple &lt;- svydesign(ids = ~1, # 1 for no cluster ids\n                   weights = NULL,               # No weight added\n                   strata = NULL,                # sampling was simple (no strata)\n                   data = survey_data            # have to specify the dataset\n                  )\n\n## stratified ------------------------------------------------------------------\nbase_survey_design_strata &lt;- svydesign(ids = ~1,  # 1 for no cluster ids\n                   weights = ~surv_weight_strata, # weight variable created above\n                   strata = ~health_district,     # sampling was stratified by district\n                   data = survey_data             # have to specify the dataset\n                  )\n\n# cluster ---------------------------------------------------------------------\nbase_survey_design_cluster &lt;- svydesign(ids = ~village_name, # cluster ids\n                   weights = ~surv_weight_cluster, # weight variable created above\n                   strata = NULL,                 # sampling was simple (no strata)\n                   data = survey_data              # have to specify the dataset\n                  )\n\n# stratified cluster ----------------------------------------------------------\nbase_survey_design &lt;- svydesign(ids = ~village_name,      # cluster ids\n                   weights = ~surv_weight_cluster_strata, # weight variable created above\n                   strata = ~health_district,             # sampling was stratified by district\n                   data = survey_data                     # have to specify the dataset\n                  )"
  },
  {
    "objectID": "readings/survey-analysis.html#descriptive-analysis",
    "href": "readings/survey-analysis.html#descriptive-analysis",
    "title": "Survey analysis",
    "section": "",
    "text": "In this section we will focus on how to investigate and visualize bias in a sample. We will also look at visualising population flow in a survey setting using alluvial/sankey diagrams.\nIn general, you should consider including the following descriptive analyses:\n\nFinal number of clusters, households and individuals included\n\nNumber of excluded individuals and the reasons for exclusion\nMedian (range) number of households per cluster and individuals per household\n\n\n\nCompare the proportions in each age group between your sample and the source population. This is important to be able to highlight potential sampling bias. You could similarly repeat this looking at distributions by sex.\nNote that these p-values are just indicative, and a descriptive discussion (or visualisation with age-pyramids below) of the distributions in your study sample compared to the source population is more important than the binomial test itself. This is because increasing sample size will more often than not lead to differences that may be irrelevant after weighting your data.\n\n## counts and props of the study population\nag &lt;- survey_data %&gt;% \n  group_by(age_group) %&gt;% \n  drop_na(age_group) %&gt;% \n  tally() %&gt;% \n  mutate(proportion = n / sum(n), \n         n_total = sum(n))\n\n## counts and props of the source population\npropcount &lt;- population %&gt;% \n  group_by(age_group) %&gt;%\n    tally(population) %&gt;%\n    mutate(proportion = n / sum(n))\n\n## bind together the columns of two tables, group by age, and perform a \n## binomial test to see if n/total is significantly different from population\n## proportion.\n  ## suffix here adds to text to the end of columns in each of the two datasets\nleft_join(ag, propcount, by = \"age_group\", suffix = c(\"\", \"_pop\")) %&gt;%\n  group_by(age_group) %&gt;%\n  ## broom::tidy(binom.test()) makes a data frame out of the binomial test and\n  ## will add the variables p.value, parameter, conf.low, conf.high, method, and\n  ## alternative. We will only use p.value here. You can include other\n  ## columns if you want to report confidence intervals\n  mutate(binom = list(broom::tidy(binom.test(n, n_total, proportion_pop)))) %&gt;%\n  unnest(cols = c(binom)) %&gt;% # important for expanding the binom.test data frame\n  mutate(proportion_pop = proportion_pop * 100) %&gt;%\n  ## Adjusting the p-values to correct for false positives \n  ## (because testing multiple age groups). This will only make \n  ## a difference if you have many age categories\n  mutate(p.value = p.adjust(p.value, method = \"holm\")) %&gt;%\n                      \n  ## Only show p-values over 0.001 (those under report as &lt;0.001)\n  mutate(p.value = ifelse(p.value &lt; 0.001, \n                          \"&lt;0.001\", \n                          as.character(round(p.value, 3)))) %&gt;% \n  \n  ## rename the columns appropriately\n  select(\n    \"Age group\" = age_group,\n    \"Study population (n)\" = n,\n    \"Study population (%)\" = proportion,\n    \"Source population (n)\" = n_pop,\n    \"Source population (%)\" = proportion_pop,\n    \"P-value\" = p.value\n  )\n\n# A tibble: 5 × 6\n# Groups:   Age group [5]\n  `Age group` `Study population (n)` `Study population (%)`\n  &lt;chr&gt;                        &lt;int&gt;                  &lt;dbl&gt;\n1 0-2                             12                 0.0256\n2 3-14                            42                 0.0896\n3 15-29                           64                 0.136 \n4 30-44                           52                 0.111 \n5 45+                            299                 0.638 \n# ℹ 3 more variables: `Source population (n)` &lt;dbl&gt;,\n#   `Source population (%)` &lt;dbl&gt;, `P-value` &lt;chr&gt;"
  },
  {
    "objectID": "readings/survey-analysis.html#weighted-proportions",
    "href": "readings/survey-analysis.html#weighted-proportions",
    "title": "Survey analysis",
    "section": "",
    "text": "This section will detail how to produce tables for weighted counts and proportions, with associated confidence intervals and design effect.\n\n\nWe can use the svyciprop() function from survey to get weighted proportions and accompanying 95% confidence intervals. An appropriate design effect can be extracted using the svymean() rather than svyprop() function. It is worth noting that svyprop() only appears to accept variables between 0 and 1 (or TRUE/FALSE), so categorical variables will not work.\nNOTE: Functions from survey also accept srvyr design objects, but here we have used the survey design object just for consistency \n\n## produce weighted counts \nsvytable(~died, base_survey_design)\n\ndied\n     FALSE       TRUE \n1406244.43   76213.01 \n\n## produce weighted proportions\nsvyciprop(~died, base_survey_design, na.rm = T)\n\n              2.5% 97.5%\ndied 0.0514 0.0208  0.12\n\n## get the design effect \nsvymean(~died, base_survey_design, na.rm = T, deff = T) %&gt;% \n  deff()\n\ndiedFALSE  diedTRUE \n 3.755508  3.755508 \n\n\nWe can combine the functions from survey shown above in to a function which we define ourselves below, called svy_prop; and we can then use that function together with map() from the purrr package to iterate over several variables and create a table. See the handbook iteration chapter for details on purrr.\n\n# Define function to calculate weighted counts, proportions, CI and design effect\n# x is the variable in quotation marks \n# design is your survey design object\n\nsvy_prop &lt;- function(design, x) {\n  \n  ## put the variable of interest in a formula \n  form &lt;- as.formula(paste0( \"~\" , x))\n  ## only keep the TRUE column of counts from svytable\n  weighted_counts &lt;- svytable(form, design)[[2]]\n  ## calculate proportions (multiply by 100 to get percentages)\n  weighted_props &lt;- svyciprop(form, design, na.rm = TRUE) * 100\n  ## extract the confidence intervals and multiply to get percentages\n  weighted_confint &lt;- confint(weighted_props) * 100\n  ## use svymean to calculate design effect and only keep the TRUE column\n  design_eff &lt;- deff(svymean(form, design, na.rm = TRUE, deff = TRUE))[[TRUE]]\n  \n  ## combine in to one data frame\n  full_table &lt;- cbind(\n    \"Variable\"        = x,\n    \"Count\"           = weighted_counts,\n    \"Proportion\"      = weighted_props,\n    weighted_confint, \n    \"Design effect\"   = design_eff\n    )\n  \n  ## return table as a dataframe\n  full_table &lt;- data.frame(full_table, \n             ## remove the variable names from rows (is a separate column now)\n             row.names = NULL)\n  \n  ## change numerics back to numeric\n  full_table[ , 2:6] &lt;- as.numeric(full_table[, 2:6])\n  \n  ## return dataframe\n  full_table\n}\n\n## iterate over several variables to create a table \npurrr::map(\n  ## define variables of interest\n  c(\"left\", \"died\", \"arrived\"), \n  ## state function using and arguments for that function (design)\n  svy_prop, design = base_survey_design) %&gt;% \n  ## collapse list in to a single data frame\n  bind_rows() %&gt;% \n  ## round \n  mutate(across(where(is.numeric), round, digits = 1))\n\n  Variable    Count Proportion X2.5. X97.5. Design.effect\n1     left 701199.1       47.3  39.2   55.5           2.4\n2     died  76213.0        5.1   2.1   12.1           3.8\n3  arrived 761799.0       51.4  40.9   61.7           3.9"
  },
  {
    "objectID": "readings/survey-analysis.html#weighted-ratios",
    "href": "readings/survey-analysis.html#weighted-ratios",
    "title": "Survey analysis",
    "section": "",
    "text": "Similarly for weighted ratios (such as for mortality ratios) you can use survey:::svyratio:\n\n\n\nratio &lt;- svyratio(~died, \n         denominator = ~obstime, \n         design = base_survey_design)\n\nci &lt;- confint(ratio)\n\ncbind(\n  ratio$ratio * 10000, \n  ci * 10000\n)\n\n      obstime    2.5 %   97.5 %\ndied 5.981922 1.194294 10.76955"
  },
  {
    "objectID": "resources/getting-help.html",
    "href": "resources/getting-help.html",
    "title": "Where to get help",
    "section": "",
    "text": "Where to get help\n\nUse the built-in RStudio help interface to search for more information on R functions\n\n\n\n\n\nRStudio help interface.\n\n\n\n\nOne of the fastest ways to get help, is to use the RStudio help interface. This panel by default can be found at the lower right hand panel of RStudio. As seen in the screenshot, by typing the word “Mean”, RStudio tries to also give a number of suggestions that you might be interested in. The description is then shown in the display window.\n\n\nI know the name of the function I want to use, but I’m not sure how to use it\nIf you need help with a specific function, let’s say barplot(), you can type:\n\n?barplot\n\nIf you just need to remind yourself of the names of the arguments, you can use:\n\nargs(lm)\n\n\n\nI want to use a function that does X, there must be a function for it but I don’t know which one…\nIf you are looking for a function to do a particular task, you can use the help.search() function, which is called by the double question mark ??. However, this only looks through the installed packages for help pages with a match to your search request\n\n??kruskal\n\nIf you can’t find what you are looking for, you can use the rdocumentation.org website that searches through the help files across all packages available.\nFinally, a generic Google or internet search “R &lt;task&gt;” will often either send you to the appropriate package documentation or a helpful forum where someone else has already asked your question.\n\n\nI am stuck… I get an error message that I don’t understand\nStart by googling the error message. However, this doesn’t always work very well because often, package developers rely on the error catching provided by R. You end up with general error messages that might not be very helpful to diagnose a problem (e.g. “subscript out of bounds”). If the message is very generic, you might also include the name of the function or package you’re using in your query.\nHowever, you should check Stack Overflow. Search using the [r] tag. Most questions have already been answered, but the challenge is to use the right words in the search to find the answers:\nhttp://stackoverflow.com/questions/tagged/r\nThe Introduction to R can also be dense for people with little programming experience but it is a good place to understand the underpinnings of the R language.\nThe R FAQ is dense and technical but it is full of useful information.\n\n\nAsking for help\nThe key to receiving help from someone is for them to rapidly grasp your problem. You should make it as easy as possible to pinpoint where the issue might be.\nTry to use the correct words to describe your problem. For instance, a package is not the same thing as a library. Most people will understand what you meant, but others have really strong feelings about the difference in meaning. The key point is that it can make things confusing for people trying to help you. Be as precise as possible when describing your problem.\nIf possible, try to reduce what doesn’t work to a simple reproducible example. If you can reproduce the problem using a very small data frame instead of your 50000 rows and 10000 columns one, provide the small one with the description of your problem. When appropriate, try to generalise what you are doing so even people who are not in your field can understand the question. For instance instead of using a subset of your real dataset, create a small (3 columns, 5 rows) generic one. For more information on how to write a reproducible example see this article by Hadley Wickham.\nTo share an object with someone else, if it’s relatively small, you can use the function dput(). It will output R code that can be used to recreate the exact same object as the one in memory:\n\n## iris is an example data frame that comes with R and head() is a\n## function that returns the first part of the data frame\ndput(head(iris))\n\nstructure(list(Sepal.Length = c(5.1, 4.9, 4.7, 4.6, 5, 5.4), \n    Sepal.Width = c(3.5, 3, 3.2, 3.1, 3.6, 3.9), Petal.Length = c(1.4, \n    1.4, 1.3, 1.5, 1.4, 1.7), Petal.Width = c(0.2, 0.2, 0.2, \n    0.2, 0.2, 0.4), Species = structure(c(1L, 1L, 1L, 1L, 1L, \n    1L), levels = c(\"setosa\", \"versicolor\", \"virginica\"), class = \"factor\")), row.names = c(NA, \n6L), class = \"data.frame\")\n\n\nIf the object is larger, provide either the raw file (i.e., your CSV file) with your script up to the point of the error (and after removing everything that is not relevant to your issue). Alternatively, in particular if your question is not related to a data frame, you can save any R object to a file[^export]:\n\nsaveRDS(iris, file=\"/tmp/iris.rds\")\n\nThe content of this file is however not human readable and cannot be posted directly on Stack Overflow. Instead, it can be sent to someone by email who can read it with the readRDS() command (here it is assumed that the downloaded file is in a Downloads folder in the user’s home directory):\n\nsome_data &lt;- readRDS(file=\"~/Downloads/iris.rds\")\n\nLast, but certainly not least, always include the output of sessionInfo() as it provides critical information about your platform, the versions of R and the packages that you are using, and other information that can be very helpful to understand your problem.\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19045)\n\nMatrix products: default\n\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\ntime zone: America/New_York\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.1       htmltools_0.5.6   rstudioapi_0.15.0 rmarkdown_2.24   \n [9] knitr_1.43        jsonlite_1.8.7    xfun_0.40         digest_0.6.33    \n[13] rlang_1.1.1       evaluate_0.21    \n\n\n\n\nWhere to ask for help?\n\nThe person sitting next to you. Don’t hesitate to talk to your neighbour during the workshop, compare your answers, and ask for help.\nThe instructors. We’re here to help you.\nYour friendly colleagues: if you know someone with more experience than you, they might be able and willing to help you.\nStack Overflow: if your question hasn’t been answered before and is well crafted, chances are you will get an answer in less than 5 min. Remember to follow their guidelines on how to ask a good question.\nThe R-help mailing list: it is read by a lot of people (including most of the R core team), a lot of people post to it, but the tone can be pretty dry, and it is not always very welcoming to new users. If your question is valid, you are likely to get an answer very fast but don’t expect that it will come with smiley faces. Also, here more than anywhere else, be sure to use correct vocabulary (otherwise you might get an answer pointing to the misuse of your words rather than answering your question). You will also have more success if your question is about a base function rather than a specific package.\nIf your question is about a specific package, see if there is a mailing list for it. Usually it’s included in the DESCRIPTION file of the package that can be accessed using packageDescription(\"name-of-package\"). You may also want to try to email the author of the package directly, or open an issue on the code repository (e.g., GitHub).\nThere are also some topic-specific mailing lists (GIS, phylogenetics, etc…), the complete list is here.\n\n\n\nMore resources\n\nThe Posting Guide for the R mailing lists.\nHow to ask for R help useful guidelines.\nThis blog post by Jon Skeet has quite comprehensive advice on how to ask programming questions.\nThe reprex package is very helpful to create reproducible examples when asking for help. The rOpenSci community call “How to ask questions so they get answered” (Github link and video recording) includes a presentation of the reprex package and of its philosophy.\n\n\nThe materials in this lesson have been adapted from the Introduction to data analysis with R and Bioconductor workshop, which is a part of the Carpentries Incubator. These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "Additional Resources",
    "section": "",
    "text": "Help documentation\nSearch the RStudio “Help” tab for documentation on R packages and specific functions. This is within the pane that also contains Files, Plots, and Packages (typically in the lower-right pane). As a shortcut, you can also type the name of a package or function into the R console after a question-mark to open the relevant Help page. Do not include parentheses.\nFor example: ?filter or ?diagrammeR.\nInteractive tutorials\nThere are several ways to learn R interactively within RStudio.\nRStudio itself offers a Tutorial pane that is powered by the learnr R package. Simply install this package and open a tutorial via the new “Tutorial” tab in the upper-right RStudio pane (which also contains Environment and History tabs).\nThe R package swirl offers interactive courses in the R Console. Install and load this package, then run the command swirl() (empty parentheses) in the R console. You will see prompts appear in the Console. Respond by typing in the Console. It will guide you through a course of your choice.\n\n\n\nThere are many PDF “cheatsheets” available on the RStudio website, for example:\n\nFactors with forcats package\n\nDates and times with lubridate package\n\nStrings with stringr package\n\niterative opertaions with purrr package\n\nData import\n\nData transformation cheatsheet with dplyr package\n\nR Markdown (to create documents like PDF, Word, Powerpoint…)\n\nShiny (to build interactive web apps)\n\nData visualization with ggplot2 package\n\nCartography (GIS)\n\nleaflet package (interactive maps)\n\nPython with R (reticulate package)\n\nThis is an online R resource specifically for Excel users\n\n\n\n\n\nR Markdown is a widely-used tool for creating automated, reproducible, and share-worthy outputs, such as reports. It can generate static or interactive outputs, in Word, pdf, html, powerpoint, and other formats.\nAn R Markdown script intersperces R code and text such that the script actually becomes your output document. You can create an entire formatted document, including narrative text (can be dynamic to change based on your data), tables, figures, bullets/numbers, bibliographies, etc.\n\nThe creators of RStudio have an official beginner’s guide on RMarkdown.\nR for Data Science also has a great chapter on Quarto.\nThe definitive R Markdown guide\nQuarto is the new R Markdown put out by Posit, the makers of RStudio (and formerly named RStudio Inc.). You can find a quick tutorial on Quarto here\n\n\n\n\n\nSome great lecture slides on data visualization\nanother plotting cheat sheet\ntidyverse ggplot basics page\n\nplotting continuous variables\n\nR for Data Science pages on data visualization\ngraphics for communicaton\n\n\n\n\n\n\nThe full flextable book is here: https://ardata-fr.github.io/flextable-book/\nThe Github site is here\n\nA manual of all the flextable functions can be found here\nA gallery of beautiful example flextable tables with code can be accessed here\n\n\n\n\nThe Data Carpentry page on dplyr\n\nThe tidyverse reference pages on group_by() and grouping\n\nThis page on Data manipulation\n\nSummarize with conditions in dplyr\n\nA helpful tutorial on pivoting tutorial\nThe tidyverse page on joins\n\nThe R for Data Science page on relational data\n\n\n\n\n\nThese lecture notes are a good resource on regression.\n\n\n\n\n\nLecture slides on surveys\nUCLA stats page\n\nArticle by Pew research for a general audeince on why we use survey weights\n\n\n\n\n\n\n\nR has a vibrant twitter community where you can learn tips, shortcuts, and news - follow these accounts:\n\nEpiHandbook (makers of the textbook these materials are based on) @epiRhandbook\n\nR Function A Day @rfuntionaday is an incredible resource\n\nR for Data Science @rstats4ds\n\nRStudio @RStudio\n\nRStudio Tips @rstudiotips\n\nR-Bloggers @Rbloggers\n\nR-ladies @RLadiesGlobal\n\nHadley Wickham @hadleywickham\n\nAlso:\n#epitwitter and #rstats\n\n\n\nA definitive text is the R for Data Science book by Garrett Grolemund and Hadley Wickham\nThe R4Epis project website aims to “develop standardised data cleaning, analysis and reporting tools to cover common types of outbreaks and population-based surveys that would be conducted in an MSF emergency response setting.” You can find R basics training materials, templates for RMarkdown reports on outbreaks and surveys, and tutorials to help you set them up.\n\n\n\n\nMateriales de RStudio en Español\nIntroduction à R et au tidyverse (Francais)"
  },
  {
    "objectID": "resources/resources.html#other-r-resources",
    "href": "resources/resources.html#other-r-resources",
    "title": "Additional Resources",
    "section": "",
    "text": "Help documentation\nSearch the RStudio “Help” tab for documentation on R packages and specific functions. This is within the pane that also contains Files, Plots, and Packages (typically in the lower-right pane). As a shortcut, you can also type the name of a package or function into the R console after a question-mark to open the relevant Help page. Do not include parentheses.\nFor example: ?filter or ?diagrammeR.\nInteractive tutorials\nThere are several ways to learn R interactively within RStudio.\nRStudio itself offers a Tutorial pane that is powered by the learnr R package. Simply install this package and open a tutorial via the new “Tutorial” tab in the upper-right RStudio pane (which also contains Environment and History tabs).\nThe R package swirl offers interactive courses in the R Console. Install and load this package, then run the command swirl() (empty parentheses) in the R console. You will see prompts appear in the Console. Respond by typing in the Console. It will guide you through a course of your choice.\n\n\n\nThere are many PDF “cheatsheets” available on the RStudio website, for example:\n\nFactors with forcats package\n\nDates and times with lubridate package\n\nStrings with stringr package\n\niterative opertaions with purrr package\n\nData import\n\nData transformation cheatsheet with dplyr package\n\nR Markdown (to create documents like PDF, Word, Powerpoint…)\n\nShiny (to build interactive web apps)\n\nData visualization with ggplot2 package\n\nCartography (GIS)\n\nleaflet package (interactive maps)\n\nPython with R (reticulate package)\n\nThis is an online R resource specifically for Excel users\n\n\n\n\n\nR Markdown is a widely-used tool for creating automated, reproducible, and share-worthy outputs, such as reports. It can generate static or interactive outputs, in Word, pdf, html, powerpoint, and other formats.\nAn R Markdown script intersperces R code and text such that the script actually becomes your output document. You can create an entire formatted document, including narrative text (can be dynamic to change based on your data), tables, figures, bullets/numbers, bibliographies, etc.\n\nThe creators of RStudio have an official beginner’s guide on RMarkdown.\nR for Data Science also has a great chapter on Quarto.\nThe definitive R Markdown guide\nQuarto is the new R Markdown put out by Posit, the makers of RStudio (and formerly named RStudio Inc.). You can find a quick tutorial on Quarto here\n\n\n\n\n\nSome great lecture slides on data visualization\nanother plotting cheat sheet\ntidyverse ggplot basics page\n\nplotting continuous variables\n\nR for Data Science pages on data visualization\ngraphics for communicaton\n\n\n\n\n\n\nThe full flextable book is here: https://ardata-fr.github.io/flextable-book/\nThe Github site is here\n\nA manual of all the flextable functions can be found here\nA gallery of beautiful example flextable tables with code can be accessed here\n\n\n\n\nThe Data Carpentry page on dplyr\n\nThe tidyverse reference pages on group_by() and grouping\n\nThis page on Data manipulation\n\nSummarize with conditions in dplyr\n\nA helpful tutorial on pivoting tutorial\nThe tidyverse page on joins\n\nThe R for Data Science page on relational data\n\n\n\n\n\nThese lecture notes are a good resource on regression.\n\n\n\n\n\nLecture slides on surveys\nUCLA stats page\n\nArticle by Pew research for a general audeince on why we use survey weights\n\n\n\n\n\n\n\nR has a vibrant twitter community where you can learn tips, shortcuts, and news - follow these accounts:\n\nEpiHandbook (makers of the textbook these materials are based on) @epiRhandbook\n\nR Function A Day @rfuntionaday is an incredible resource\n\nR for Data Science @rstats4ds\n\nRStudio @RStudio\n\nRStudio Tips @rstudiotips\n\nR-Bloggers @Rbloggers\n\nR-ladies @RLadiesGlobal\n\nHadley Wickham @hadleywickham\n\nAlso:\n#epitwitter and #rstats\n\n\n\nA definitive text is the R for Data Science book by Garrett Grolemund and Hadley Wickham\nThe R4Epis project website aims to “develop standardised data cleaning, analysis and reporting tools to cover common types of outbreaks and population-based surveys that would be conducted in an MSF emergency response setting.” You can find R basics training materials, templates for RMarkdown reports on outbreaks and surveys, and tutorials to help you set them up.\n\n\n\n\nMateriales de RStudio en Español\nIntroduction à R et au tidyverse (Francais)"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html",
    "href": "session-materials/session0/bootcamp-2.html",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "",
    "text": "We will begin with a mostly processed dataset from NHANES designed to examine the relationship between diabetes and dental caries in adolescents. Specifically, this dataset contains the 3346 adolescents recorded in NHANES from 2005 to 2010 with non-missing dental decay data.\nWe can load the data into R as a dataframe using the read.csv function.\n\nnhanes &lt;- read.csv(\"https://raw.githubusercontent.com/ccb-hms/hsdm-r-course/main/session-materials/session0/session0Data.csv\")\n\nThis statement doesn’t produce any output because, as you might recall, assignments don’t display anything. If we want to check that our data has been loaded, we can see the contents of the data frame by typing its name:\n\nnhanes\n\nWow… that was a lot of output. At least it means the data loaded properly. Let’s check the top (the first 6 lines) of this data frame using the function head():\n\nhead(nhanes)\n\n  sequence.id age.years    sex                      ethnicity\n1       31521        13 Female             Non-Hispanic White\n2       31981        15 Female             Non-Hispanic Black\n3       32326        16   Male             Non-Hispanic Black\n4       32369        15   Male Other Race - Including Multi-R\n5       32537        13 Female               Mexican American\n6       32604        15 Female             Non-Hispanic White\n                     birthplace family.PIR dental.decay.present\n1 Born in 50 US States or Washi       5.00                FALSE\n2 Born in 50 US States or Washi       0.67                FALSE\n3 Born in 50 US States or Washi       1.24                FALSE\n4 Born in 50 US States or Washi       0.93                FALSE\n5 Born in 50 US States or Washi       4.13                FALSE\n6 Born in 50 US States or Washi       5.00                FALSE\n  dental.restoration.present plasma.glucose hba1c   bmi\n1                      FALSE             88   5.1 17.62\n2                      FALSE             94   4.7 20.65\n3                      FALSE             NA   6.2 35.62\n4                       TRUE             NA   5.3 27.72\n5                      FALSE             NA   5.1 19.43\n6                      FALSE             NA   5.0 21.22"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#dataframes",
    "href": "session-materials/session0/bootcamp-2.html#dataframes",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "",
    "text": "We will begin with a mostly processed dataset from NHANES designed to examine the relationship between diabetes and dental caries in adolescents. Specifically, this dataset contains the 3346 adolescents recorded in NHANES from 2005 to 2010 with non-missing dental decay data.\nWe can load the data into R as a dataframe using the read.csv function.\n\nnhanes &lt;- read.csv(\"https://raw.githubusercontent.com/ccb-hms/hsdm-r-course/main/session-materials/session0/session0Data.csv\")\n\nThis statement doesn’t produce any output because, as you might recall, assignments don’t display anything. If we want to check that our data has been loaded, we can see the contents of the data frame by typing its name:\n\nnhanes\n\nWow… that was a lot of output. At least it means the data loaded properly. Let’s check the top (the first 6 lines) of this data frame using the function head():\n\nhead(nhanes)\n\n  sequence.id age.years    sex                      ethnicity\n1       31521        13 Female             Non-Hispanic White\n2       31981        15 Female             Non-Hispanic Black\n3       32326        16   Male             Non-Hispanic Black\n4       32369        15   Male Other Race - Including Multi-R\n5       32537        13 Female               Mexican American\n6       32604        15 Female             Non-Hispanic White\n                     birthplace family.PIR dental.decay.present\n1 Born in 50 US States or Washi       5.00                FALSE\n2 Born in 50 US States or Washi       0.67                FALSE\n3 Born in 50 US States or Washi       1.24                FALSE\n4 Born in 50 US States or Washi       0.93                FALSE\n5 Born in 50 US States or Washi       4.13                FALSE\n6 Born in 50 US States or Washi       5.00                FALSE\n  dental.restoration.present plasma.glucose hba1c   bmi\n1                      FALSE             88   5.1 17.62\n2                      FALSE             94   4.7 20.65\n3                      FALSE             NA   6.2 35.62\n4                       TRUE             NA   5.3 27.72\n5                      FALSE             NA   5.1 19.43\n6                      FALSE             NA   5.0 21.22"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#what-are-data-frames",
    "href": "session-materials/session0/bootcamp-2.html#what-are-data-frames",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "What are data frames?",
    "text": "What are data frames?\nData frames are the de facto data structure for most tabular data, and what we use for statistics and plotting.\nA data frame can be created by hand, but most commonly they are generated by the functions read.csv() or read.table(); in other words, when importing spreadsheets from your hard drive (or the web).\nA data frame is the representation of data in the format of a table where the columns are vectors that all have the same length. Because columns are vectors, each column must contain a single type of data (e.g., characters, integers, factors).\nWe can see this when inspecting the structure of a data frame with the function str():\n\nstr(nhanes)\n\n'data.frame':   3346 obs. of  11 variables:\n $ sequence.id               : int  31521 31981 32326 32369 32537 32604 31129 31148 31347 31458 ...\n $ age.years                 : int  13 15 16 15 13 15 15 16 16 13 ...\n $ sex                       : chr  \"Female\" \"Female\" \"Male\" \"Male\" ...\n $ ethnicity                 : chr  \"Non-Hispanic White\" \"Non-Hispanic Black\" \"Non-Hispanic Black\" \"Other Race - Including Multi-R\" ...\n $ birthplace                : chr  \"Born in 50 US States or Washi\" \"Born in 50 US States or Washi\" \"Born in 50 US States or Washi\" \"Born in 50 US States or Washi\" ...\n $ family.PIR                : num  5 0.67 1.24 0.93 4.13 5 2.71 1.06 3.1 3.76 ...\n $ dental.decay.present      : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ dental.restoration.present: logi  FALSE FALSE FALSE TRUE FALSE FALSE ...\n $ plasma.glucose            : int  88 94 NA NA NA NA NA 91 NA NA ...\n $ hba1c                     : num  5.1 4.7 6.2 5.3 5.1 5 5.2 5 5.1 5.1 ...\n $ bmi                       : num  17.6 20.6 35.6 27.7 19.4 ..."
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#inspecting-data.frame-objects",
    "href": "session-materials/session0/bootcamp-2.html#inspecting-data.frame-objects",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "Inspecting data.frame Objects",
    "text": "Inspecting data.frame Objects\nWe already saw how the functions head() and str() can be useful to check the content and the structure of a data frame. Here is a non-exhaustive list of functions to get a sense of the content/structure of the data. Let’s try them out!\nSize:\n\ndim(nhanes) - returns a vector with the number of rows as the first element, and the number of columns as the second element (the dimensions of the object).\nnrow(nhanes) - returns the number of rows.\nncol(nhanes) - returns the number of columns.\n\nContent:\n\nhead(nhanes) - shows the first 6 rows.\ntail(nhanes) - shows the last 6 rows.\n\nNames:\n\nnames(nhanes) - returns the column names (synonym of colnames() for data.frame objects).\nrownames(nhanes) - returns the row names.\n\nSummary:\n\nstr(nhanes) - structure of the object and information about the class, length and content of each column.\nsummary(nhanes) - summary statistics for each column.\n\nNote: most of these functions are “generic”, they can be used on other types of objects besides data.frame."
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#packages",
    "href": "session-materials/session0/bootcamp-2.html#packages",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "Packages",
    "text": "Packages\nBracket subsetting is handy, but it can be cumbersome and difficult to read, especially for complicated operations.\nSome packages can greatly facilitate our task when we manipulate data. Packages in R are basically sets of additional functions that let you do more stuff. The functions we’ve been using so far, like str() or data.frame(), come built into R; Loading packages can give you access to other specific functions. Before you use a package for the first time you need to install it on your machine, and then you should import it in every subsequent R session when you need it.\n\nThe package dplyr provides powerful tools for data manipulation tasks. It is built to work directly with data frames, with many manipulation tasks optimised.\nAs we will see latter on, sometimes we want a data frame to be reshaped to be able to do some specific analyses or for visualisation. The package tidyr addresses this common problem of reshaping data and provides tools for manipulating data in a tidy way.\n\nTo learn more about dplyr and tidyr after the workshop, you may want to check out this handy data transformation with dplyr cheatsheet and this one about tidyr.\n\nThe tidyverse package is an “umbrella-package” that installs several useful packages for data analysis which work well together, such as tidyr, dplyr, ggplot2, tibble, etc. These packages help us to work and interact with the data. They allow us to do many things with your data, such as subsetting, transforming, visualising, etc.\n\nPackages can be installed using the install.packages command. This downloads and installs the package into your entire R installation. This means that you would not need to re-install the package for a new project.\n\ninstall.packages(\"tidyverse\")\n\nOnce a package is installed, it can be loaded using the library function. This tells R to use all of the functions inside the loaded package. library loads packages into your R session, and thus this line needs to be run each time you open R.\n\n## load the tidyverse packages, incl. dplyr\nlibrary(\"tidyverse\")\nlibrary(\"DT\")"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#loading-data-with-tidyverse",
    "href": "session-materials/session0/bootcamp-2.html#loading-data-with-tidyverse",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "Loading data with tidyverse",
    "text": "Loading data with tidyverse\nInstead of read.csv(), we will read in our data using the read_csv() function (notice the _ instead of the .), from the tidyverse package readr.\n\nnhanes &lt;- read_csv(\"https://raw.githubusercontent.com/ccb-hms/hsdm-r-course/main/session-materials/session0/session0Data.csv\")\n\n## view the data\ndatatable(nhanes)\n\n\n\n\n\n\nNotice that the class of the data is now referred to as a “tibble”.\nTibbles tweak some of the behaviors of the data frame objects we introduced in the previously. The data structure is very similar to a data frame. For our purposes the only differences are that:\n\nIt displays the data type of each column under its name. Note that &lt;dbl&gt; is a data type defined to hold numeric values with decimal points.\nIt only prints the first few rows of data and only as many columns as fit on one screen.\n\nWe are now going to learn some of the most common dplyr functions:\n\nselect(): subset columns\nfilter(): subset rows on conditions\nmutate(): create new columns by using information from other columns\ngroup_by() and summarise(): create summary statistics on grouped data\narrange(): sort results\ncount(): count discrete values"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#selecting-columns-and-filtering-rows",
    "href": "session-materials/session0/bootcamp-2.html#selecting-columns-and-filtering-rows",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "Selecting columns and filtering rows",
    "text": "Selecting columns and filtering rows\nTo select columns of a data frame, use select(). The first argument to this function is the data frame (nhanes), and the subsequent arguments are the columns to keep.\n\nselect(nhanes, sex, age.years, dental.decay.present, hba1c)\n\n# A tibble: 3,346 × 4\n   sex    age.years dental.decay.present hba1c\n   &lt;chr&gt;      &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n 1 Female        13 FALSE                  5.1\n 2 Female        15 FALSE                  4.7\n 3 Male          16 FALSE                  6.2\n 4 Male          15 FALSE                  5.3\n 5 Female        13 FALSE                  5.1\n 6 Female        15 FALSE                  5  \n 7 Male          15 FALSE                  5.2\n 8 Female        16 FALSE                  5  \n 9 Female        16 FALSE                  5.1\n10 Male          13 FALSE                  5.1\n# ℹ 3,336 more rows\n\n\nTo select all columns except certain ones, put a “-” in front of the variable to exclude it.\n\nselect(nhanes, -sequence.id, -family.PIR)\n\n# A tibble: 3,346 × 9\n   age.years sex    ethnicity                    birthplace dental.decay.present\n       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                        &lt;chr&gt;      &lt;lgl&gt;               \n 1        13 Female Non-Hispanic White           Born in 5… FALSE               \n 2        15 Female Non-Hispanic Black           Born in 5… FALSE               \n 3        16 Male   Non-Hispanic Black           Born in 5… FALSE               \n 4        15 Male   Other Race - Including Mult… Born in 5… FALSE               \n 5        13 Female Mexican American             Born in 5… FALSE               \n 6        15 Female Non-Hispanic White           Born in 5… FALSE               \n 7        15 Male   Non-Hispanic Black           Born in 5… FALSE               \n 8        16 Female Non-Hispanic White           Born in 5… FALSE               \n 9        16 Female Non-Hispanic White           Born in 5… FALSE               \n10        13 Male   Mexican American             Born in 5… FALSE               \n# ℹ 3,336 more rows\n# ℹ 4 more variables: dental.restoration.present &lt;lgl&gt;, plasma.glucose &lt;dbl&gt;,\n#   hba1c &lt;dbl&gt;, bmi &lt;dbl&gt;\n\n\nThis will select all the variables in nhanes except sequence.id and family.PIR.\nTo choose rows based on a specific criteria, use filter():\n\nfilter(nhanes, sex == \"Male\")\n\n# A tibble: 1,703 × 11\n   sequence.id age.years sex   ethnicity                   birthplace family.PIR\n         &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                       &lt;chr&gt;           &lt;dbl&gt;\n 1       32326        16 Male  Non-Hispanic Black          Born in 5…       1.24\n 2       32369        15 Male  Other Race - Including Mul… Born in 5…       0.93\n 3       31129        15 Male  Non-Hispanic Black          Born in 5…       2.71\n 4       31458        13 Male  Mexican American            Born in 5…       3.76\n 5       31763        18 Male  Mexican American            Born in 5…       3.1 \n 6       31848        16 Male  Non-Hispanic White          Born in 5…       5   \n 7       31882        16 Male  Non-Hispanic White          Born in 5…       1.82\n 8       32758        13 Male  Non-Hispanic Black          Born in 5…       1.72\n 9       32831        15 Male  Mexican American            Born in 5…       1.03\n10       32840        13 Male  Non-Hispanic White          Born in 5…       5   \n# ℹ 1,693 more rows\n# ℹ 5 more variables: dental.decay.present &lt;lgl&gt;,\n#   dental.restoration.present &lt;lgl&gt;, plasma.glucose &lt;dbl&gt;, hba1c &lt;dbl&gt;,\n#   bmi &lt;dbl&gt;\n\nfilter(nhanes, sex == \"Male\" & plasma.glucose &gt; 80)\n\n# A tibble: 753 × 11\n   sequence.id age.years sex   ethnicity          birthplace          family.PIR\n         &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;              &lt;chr&gt;                    &lt;dbl&gt;\n 1       31882        16 Male  Non-Hispanic White Born in 50 US Stat…       1.82\n 2       32758        13 Male  Non-Hispanic Black Born in 50 US Stat…       1.72\n 3       32831        15 Male  Mexican American   Born in 50 US Stat…       1.03\n 4       33165        18 Male  Non-Hispanic Black Born in 50 US Stat…       0.73\n 5       33215        17 Male  Mexican American   Born in 50 US Stat…      NA   \n 6       31698        14 Male  Non-Hispanic Black Born in 50 US Stat…       4.55\n 7       32275        13 Male  Mexican American   Born in 50 US Stat…       3.76\n 8       33870        17 Male  Non-Hispanic Black Born in 50 US Stat…      NA   \n 9       34097        14 Male  Non-Hispanic Black Born in 50 US Stat…       1.12\n10       32261        13 Male  Non-Hispanic Black Born in 50 US Stat…       1.2 \n# ℹ 743 more rows\n# ℹ 5 more variables: dental.decay.present &lt;lgl&gt;,\n#   dental.restoration.present &lt;lgl&gt;, plasma.glucose &lt;dbl&gt;, hba1c &lt;dbl&gt;,\n#   bmi &lt;dbl&gt;\n\nfilter(nhanes, sex == \"Male\" & !is.na(hba1c))\n\n# A tibble: 1,561 × 11\n   sequence.id age.years sex   ethnicity                   birthplace family.PIR\n         &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                       &lt;chr&gt;           &lt;dbl&gt;\n 1       32326        16 Male  Non-Hispanic Black          Born in 5…       1.24\n 2       32369        15 Male  Other Race - Including Mul… Born in 5…       0.93\n 3       31129        15 Male  Non-Hispanic Black          Born in 5…       2.71\n 4       31458        13 Male  Mexican American            Born in 5…       3.76\n 5       31763        18 Male  Mexican American            Born in 5…       3.1 \n 6       31848        16 Male  Non-Hispanic White          Born in 5…       5   \n 7       31882        16 Male  Non-Hispanic White          Born in 5…       1.82\n 8       32758        13 Male  Non-Hispanic Black          Born in 5…       1.72\n 9       32831        15 Male  Mexican American            Born in 5…       1.03\n10       32840        13 Male  Non-Hispanic White          Born in 5…       5   \n# ℹ 1,551 more rows\n# ℹ 5 more variables: dental.decay.present &lt;lgl&gt;,\n#   dental.restoration.present &lt;lgl&gt;, plasma.glucose &lt;dbl&gt;, hba1c &lt;dbl&gt;,\n#   bmi &lt;dbl&gt;"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#pipes",
    "href": "session-materials/session0/bootcamp-2.html#pipes",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "Pipes",
    "text": "Pipes\nWhat if you want to select and filter at the same time? There are three ways to do this: use intermediate steps, nested functions, or pipes.\nWith intermediate steps, you create a temporary data frame and use that as input to the next function, like this:\n\nnhanes2 &lt;- filter(nhanes, sex == \"Male\")\nnhanes3 &lt;- select(nhanes2, sex, age.years, dental.decay.present, hba1c)\nnhanes3\n\n# A tibble: 1,703 × 4\n   sex   age.years dental.decay.present hba1c\n   &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n 1 Male         16 FALSE                  6.2\n 2 Male         15 FALSE                  5.3\n 3 Male         15 FALSE                  5.2\n 4 Male         13 FALSE                  5.1\n 5 Male         18 FALSE                  5.2\n 6 Male         16 FALSE                  5  \n 7 Male         16 FALSE                  4.5\n 8 Male         13 FALSE                  5.4\n 9 Male         15 FALSE                  4.5\n10 Male         13 FALSE                  5.1\n# ℹ 1,693 more rows\n\n\nThis is readable, but can clutter up your workspace with lots of intermediate objects that you have to name individually. With multiple steps, that can be hard to keep track of.\nYou can also nest functions (i.e. one function inside of another), like this:\n\nnhanes3 &lt;- select(filter(nhanes, sex == \"Male\"), sex, age.years, dental.decay.present, hba1c)\nnhanes3\n\n# A tibble: 1,703 × 4\n   sex   age.years dental.decay.present hba1c\n   &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n 1 Male         16 FALSE                  6.2\n 2 Male         15 FALSE                  5.3\n 3 Male         15 FALSE                  5.2\n 4 Male         13 FALSE                  5.1\n 5 Male         18 FALSE                  5.2\n 6 Male         16 FALSE                  5  \n 7 Male         16 FALSE                  4.5\n 8 Male         13 FALSE                  5.4\n 9 Male         15 FALSE                  4.5\n10 Male         13 FALSE                  5.1\n# ℹ 1,693 more rows\n\n\nThis is handy, but can be difficult to read if too many functions are nested, as R evaluates the expression from the inside out (in this case, filtering, then selecting).\nThe last option, pipes, are a recent addition to R. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset.\nPipes in R look like %&gt;% (made available via the magrittr package) or |&gt; (through base R). If you use RStudio, you can type the pipe with Ctrl + Shift + M if you have a PC or Cmd + Shift + M if you have a Mac.\nIn the above code, we use the pipe to send the nhanes dataset first through filter() to keep rows where sex is Male, then through select() to keep only the selected columns.\nThe pipe %&gt;% takes the object on its left and passes it directly as the first argument to the function on its right, we don’t need to explicitly include the data frame as an argument to the filter() and select() functions any more.\n\nnhanes %&gt;%\n  filter(sex == \"Male\") %&gt;%\n  select(sex, age.years, dental.decay.present, hba1c)\n\n# A tibble: 1,703 × 4\n   sex   age.years dental.decay.present hba1c\n   &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n 1 Male         16 FALSE                  6.2\n 2 Male         15 FALSE                  5.3\n 3 Male         15 FALSE                  5.2\n 4 Male         13 FALSE                  5.1\n 5 Male         18 FALSE                  5.2\n 6 Male         16 FALSE                  5  \n 7 Male         16 FALSE                  4.5\n 8 Male         13 FALSE                  5.4\n 9 Male         15 FALSE                  4.5\n10 Male         13 FALSE                  5.1\n# ℹ 1,693 more rows\n\n\nSome may find it helpful to read the pipe like the word “then”. For instance, in the above example, we took the data frame rna, then we filtered for rows with sex == \"Male\", then we selected columns sex, age.years, dental.decay.present, and hba1c.\nThe dplyr functions by themselves are somewhat simple, but by combining them into linear workflows with the pipe, we can accomplish more complex manipulations of data frames.\nIf we want to create a new object with this smaller version of the data, we can assign it a new name:\n\nnhanes3 %&gt;%\n  filter(sex == \"Male\") %&gt;%\n  select(sex, age.years, dental.decay.present, hba1c)\n\n# A tibble: 1,703 × 4\n   sex   age.years dental.decay.present hba1c\n   &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n 1 Male         16 FALSE                  6.2\n 2 Male         15 FALSE                  5.3\n 3 Male         15 FALSE                  5.2\n 4 Male         13 FALSE                  5.1\n 5 Male         18 FALSE                  5.2\n 6 Male         16 FALSE                  5  \n 7 Male         16 FALSE                  4.5\n 8 Male         13 FALSE                  5.4\n 9 Male         15 FALSE                  4.5\n10 Male         13 FALSE                  5.1\n# ℹ 1,693 more rows\n\nnhanes3\n\n# A tibble: 1,703 × 4\n   sex   age.years dental.decay.present hba1c\n   &lt;chr&gt;     &lt;dbl&gt; &lt;lgl&gt;                &lt;dbl&gt;\n 1 Male         16 FALSE                  6.2\n 2 Male         15 FALSE                  5.3\n 3 Male         15 FALSE                  5.2\n 4 Male         13 FALSE                  5.1\n 5 Male         18 FALSE                  5.2\n 6 Male         16 FALSE                  5  \n 7 Male         16 FALSE                  4.5\n 8 Male         13 FALSE                  5.4\n 9 Male         15 FALSE                  4.5\n10 Male         13 FALSE                  5.1\n# ℹ 1,693 more rows\n\n\n\n\n\n\n\n\nChallenge:\n\n\n\nUsing pipes, subset the nhanes data to keep female participants 15 years or older, where the hba1c is greater than 5.2 (and is not NA), and retain only the columns sex, age.years, and plasma.glucose.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnhanes %&gt;%\n  filter(hba1c &gt; 5.2,\n         sex == \"Female\",\n         age.years &gt;= 15 ) %&gt;%\n  select(sex, age.years, plasma.glucose)\n\n# A tibble: 319 × 3\n   sex    age.years plasma.glucose\n   &lt;chr&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n 1 Female        16            132\n 2 Female        16             NA\n 3 Female        17             90\n 4 Female        15             90\n 5 Female        15             83\n 6 Female        18             92\n 7 Female        17             NA\n 8 Female        17             NA\n 9 Female        17             79\n10 Female        18             93\n# ℹ 309 more rows"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#mutate",
    "href": "session-materials/session0/bootcamp-2.html#mutate",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "Mutate",
    "text": "Mutate\nFrequently you’ll want to create new columns based on the values of existing columns, for example to do unit conversions, or to find the ratio of values in two columns. For this we’ll use mutate().\nTo create a new column of age in months:\n\nnhanes %&gt;%\n  mutate(age.months = age.years * 12) %&gt;%\n  select(age.months, age.years)\n\n# A tibble: 3,346 × 2\n   age.months age.years\n        &lt;dbl&gt;     &lt;dbl&gt;\n 1        156        13\n 2        180        15\n 3        192        16\n 4        180        15\n 5        156        13\n 6        180        15\n 7        180        15\n 8        192        16\n 9        192        16\n10        156        13\n# ℹ 3,336 more rows\n\n\nYou can also create a second new column based on the first new column within the same call of mutate():\n\nnhanes %&gt;%\n  mutate(age.months = age.years * 12,\n         lived_200_months = age.months &gt;= 200) %&gt;%\n  select(age.months, age.years, lived_200_months)\n\n# A tibble: 3,346 × 3\n   age.months age.years lived_200_months\n        &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;           \n 1        156        13 FALSE           \n 2        180        15 FALSE           \n 3        192        16 FALSE           \n 4        180        15 FALSE           \n 5        156        13 FALSE           \n 6        180        15 FALSE           \n 7        180        15 FALSE           \n 8        192        16 FALSE           \n 9        192        16 FALSE           \n10        156        13 FALSE           \n# ℹ 3,336 more rows\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a new data frame from the nhanes data that meets the following criteria: contains the columns sex, under_14, ethnicity, and family.PIR columns. The under_14 column should be a logical variable indicating whether or not the age of the participant is less than 14. This data frame must only contain participants born in the USA or Mexico, have a non-missing plasma glucose value, and with a BMI less than 30.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnhanes %&gt;%\n  mutate(under_14 = age.years &lt; 14) %&gt;%\n  filter(birthplace == 'Born in 50 US States or Washi' | birthplace == \"Born in Mexico\") %&gt;%\n  filter(!is.na(plasma.glucose)) %&gt;%\n  filter(bmi &lt; 30) %&gt;%\n  select(sex, under_14, ethnicity, family.PIR)\n\n# A tibble: 1,147 × 4\n   sex    under_14 ethnicity          family.PIR\n   &lt;chr&gt;  &lt;lgl&gt;    &lt;chr&gt;                   &lt;dbl&gt;\n 1 Female TRUE     Non-Hispanic White       5   \n 2 Female FALSE    Non-Hispanic Black       0.67\n 3 Female FALSE    Non-Hispanic White       1.06\n 4 Male   FALSE    Non-Hispanic White       1.82\n 5 Male   TRUE     Non-Hispanic Black       1.72\n 6 Male   FALSE    Mexican American         1.03\n 7 Male   FALSE    Non-Hispanic Black       0.73\n 8 Male   FALSE    Mexican American        NA   \n 9 Female FALSE    Non-Hispanic White       1.2 \n10 Male   FALSE    Non-Hispanic Black       2.23\n# ℹ 1,137 more rows"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#split-apply-combine-data-analysis",
    "href": "session-materials/session0/bootcamp-2.html#split-apply-combine-data-analysis",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "Split-apply-combine data analysis",
    "text": "Split-apply-combine data analysis\nMany data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results. dplyr makes this very easy through the use of the group_by() function.\n\nnhanes %&gt;%\n  group_by(birthplace)\n\n# A tibble: 3,346 × 11\n# Groups:   birthplace [7]\n   sequence.id age.years sex    ethnicity                  birthplace family.PIR\n         &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;chr&gt;           &lt;dbl&gt;\n 1       31521        13 Female Non-Hispanic White         Born in 5…       5   \n 2       31981        15 Female Non-Hispanic Black         Born in 5…       0.67\n 3       32326        16 Male   Non-Hispanic Black         Born in 5…       1.24\n 4       32369        15 Male   Other Race - Including Mu… Born in 5…       0.93\n 5       32537        13 Female Mexican American           Born in 5…       4.13\n 6       32604        15 Female Non-Hispanic White         Born in 5…       5   \n 7       31129        15 Male   Non-Hispanic Black         Born in 5…       2.71\n 8       31148        16 Female Non-Hispanic White         Born in 5…       1.06\n 9       31347        16 Female Non-Hispanic White         Born in 5…       3.1 \n10       31458        13 Male   Mexican American           Born in 5…       3.76\n# ℹ 3,336 more rows\n# ℹ 5 more variables: dental.decay.present &lt;lgl&gt;,\n#   dental.restoration.present &lt;lgl&gt;, plasma.glucose &lt;dbl&gt;, hba1c &lt;dbl&gt;,\n#   bmi &lt;dbl&gt;\n\n\nThe group_by() function doesn’t perform any data processing, it groups the data into subsets: in the example above, our initial tibble of 3346 observations is split into 7 groups based on the birthplace variable.\nWe could similarly decide to group the tibble by sex:\n\nnhanes %&gt;%\n  group_by(sex)\n\n# A tibble: 3,346 × 11\n# Groups:   sex [2]\n   sequence.id age.years sex    ethnicity                  birthplace family.PIR\n         &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                      &lt;chr&gt;           &lt;dbl&gt;\n 1       31521        13 Female Non-Hispanic White         Born in 5…       5   \n 2       31981        15 Female Non-Hispanic Black         Born in 5…       0.67\n 3       32326        16 Male   Non-Hispanic Black         Born in 5…       1.24\n 4       32369        15 Male   Other Race - Including Mu… Born in 5…       0.93\n 5       32537        13 Female Mexican American           Born in 5…       4.13\n 6       32604        15 Female Non-Hispanic White         Born in 5…       5   \n 7       31129        15 Male   Non-Hispanic Black         Born in 5…       2.71\n 8       31148        16 Female Non-Hispanic White         Born in 5…       1.06\n 9       31347        16 Female Non-Hispanic White         Born in 5…       3.1 \n10       31458        13 Male   Mexican American           Born in 5…       3.76\n# ℹ 3,336 more rows\n# ℹ 5 more variables: dental.decay.present &lt;lgl&gt;,\n#   dental.restoration.present &lt;lgl&gt;, plasma.glucose &lt;dbl&gt;, hba1c &lt;dbl&gt;,\n#   bmi &lt;dbl&gt;\n\n\nHere our initial tibble of 3346 observations is split into 2 groups based on the sex variable.\nOnce the data has been grouped, subsequent operations will be applied on each group independently.\n\nThe summarise() function\ngroup_by() is often used together with summarise(), which collapses each group into a single-row summary of that group.\ngroup_by() takes as arguments the column names that contain the categorical variables for which you want to calculate the summary statistics. So to compute the mean bmi by birthplace:\n\nnhanes %&gt;%\n  group_by(birthplace) %&gt;%\n  summarise(mean_bmi = mean(bmi, na.rm = TRUE))\n\n# A tibble: 7 × 2\n  birthplace                     mean_bmi\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 Born Elsewhere                     22.2\n2 Born in 50 US States or Washi      24.3\n3 Born in Mexico                     24.2\n4 Born in Other Non-Spanish Spea     22.5\n5 Born in Other Spanish Speaking     24.3\n6 Don't Know                         22.9\n7 Refused                            23.9\n\n\nBut we can can also group by multiple columns:\n\nnhanes %&gt;%\n  group_by(sex, ethnicity) %&gt;%\n  summarise(mean_bmi = mean(bmi, na.rm = TRUE))\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 10 × 3\n# Groups:   sex [2]\n   sex    ethnicity                      mean_bmi\n   &lt;chr&gt;  &lt;chr&gt;                             &lt;dbl&gt;\n 1 Female Mexican American                   24.5\n 2 Female Non-Hispanic Black                 26.0\n 3 Female Non-Hispanic White                 23.3\n 4 Female Other Hispanic                     24.5\n 5 Female Other Race - Including Multi-R     22.9\n 6 Male   Mexican American                   24.9\n 7 Male   Non-Hispanic Black                 23.9\n 8 Male   Non-Hispanic White                 23.4\n 9 Male   Other Hispanic                     24.0\n10 Male   Other Race - Including Multi-R     23.8\n\n\nOnce the data is grouped, you can also summarise multiple variables at the same time (and not necessarily on the same variable). For instance, we could add a column indicating the median plasma.glucose by sex and ethnicity:\n\nnhanes %&gt;%\n  group_by(sex, ethnicity) %&gt;%\n  summarise(mean_plasma_glucose = mean(plasma.glucose),\n            median_plasma_glucose = median(plasma.glucose))\n\n`summarise()` has grouped output by 'sex'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 10 × 4\n# Groups:   sex [2]\n   sex    ethnicity                    mean_plasma_glucose median_plasma_glucose\n   &lt;chr&gt;  &lt;chr&gt;                                      &lt;dbl&gt;                 &lt;dbl&gt;\n 1 Female Mexican American                              NA                    NA\n 2 Female Non-Hispanic Black                            NA                    NA\n 3 Female Non-Hispanic White                            NA                    NA\n 4 Female Other Hispanic                                NA                    NA\n 5 Female Other Race - Including Mult…                  NA                    NA\n 6 Male   Mexican American                              NA                    NA\n 7 Male   Non-Hispanic Black                            NA                    NA\n 8 Male   Non-Hispanic White                            NA                    NA\n 9 Male   Other Hispanic                                NA                    NA\n10 Male   Other Race - Including Mult…                  NA                    NA\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nCalculate the mean hba1c of participants born in the USA by age.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nnhanes %&gt;%\n  filter(birthplace == 'Born in 50 US States or Washi') %&gt;%\n  group_by(age.years) %&gt;%\n  summarise(mean_hba1c = mean(hba1c))\n\n# A tibble: 6 × 2\n  age.years mean_hba1c\n      &lt;dbl&gt;      &lt;dbl&gt;\n1        13         NA\n2        14         NA\n3        15         NA\n4        16         NA\n5        17         NA\n6        18         NA\n\n\n\n\n\n\n\nCounting\nWhen working with data, we often want to know the number of observations found for each factor or combination of factors. For this task, dplyr provides count(). For example, if we wanted to count the number of rows of data for each age, we would do:\n\nnhanes %&gt;%\n    count(age.years)\n\n# A tibble: 6 × 2\n  age.years     n\n      &lt;dbl&gt; &lt;int&gt;\n1        13   537\n2        14   576\n3        15   558\n4        16   590\n5        17   558\n6        18   527\n\n\nThe count() function is shorthand for something we’ve already seen: grouping by a variable, and summarising it by counting the number of observations in that group. In other words, nhanes %&gt;% count(age.years) is equivalent to:\n\nnhanes %&gt;%\n    group_by(age.years) %&gt;%\n    summarise(n = n())\n\n# A tibble: 6 × 2\n  age.years     n\n      &lt;dbl&gt; &lt;int&gt;\n1        13   537\n2        14   576\n3        15   558\n4        16   590\n5        17   558\n6        18   527\n\n\nThe previous example shows the use of count() to count the number of rows/observations for one factor (i.e., infection). If we wanted to count a combination of factors, such as age and sex, we would specify the first and the second factor as the arguments of count():\n\nnhanes %&gt;%\n    count(age.years, sex)\n\n# A tibble: 12 × 3\n   age.years sex        n\n       &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n 1        13 Female   268\n 2        13 Male     269\n 3        14 Female   290\n 4        14 Male     286\n 5        15 Female   271\n 6        15 Male     287\n 7        16 Female   288\n 8        16 Male     302\n 9        17 Female   263\n10        17 Male     295\n11        18 Female   263\n12        18 Male     264\n\n\nIt is sometimes useful to sort the result to facilitate the comparisons. We can use arrange() to sort the table. For instance, we might want to arrange the table above by age:\n\nnhanes %&gt;%\n    count(age.years, sex) %&gt;%\n    arrange(age.years)\n\n# A tibble: 12 × 3\n   age.years sex        n\n       &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n 1        13 Female   268\n 2        13 Male     269\n 3        14 Female   290\n 4        14 Male     286\n 5        15 Female   271\n 6        15 Male     287\n 7        16 Female   288\n 8        16 Male     302\n 9        17 Female   263\n10        17 Male     295\n11        18 Female   263\n12        18 Male     264\n\n\nor by counts:\n\nnhanes %&gt;%\n    count(age.years, sex) %&gt;%\n    arrange(n)\n\n# A tibble: 12 × 3\n   age.years sex        n\n       &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n 1        17 Female   263\n 2        18 Female   263\n 3        18 Male     264\n 4        13 Female   268\n 5        13 Male     269\n 6        15 Female   271\n 7        14 Male     286\n 8        15 Male     287\n 9        16 Female   288\n10        14 Female   290\n11        17 Male     295\n12        16 Male     302\n\n\nTo sort in descending order, we need to add the desc() function:\n\nnhanes %&gt;%\n    count(age.years, sex) %&gt;%\n    arrange(desc(n))\n\n# A tibble: 12 × 3\n   age.years sex        n\n       &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n 1        16 Male     302\n 2        17 Male     295\n 3        14 Female   290\n 4        16 Female   288\n 5        15 Male     287\n 6        14 Male     286\n 7        15 Female   271\n 8        13 Male     269\n 9        13 Female   268\n10        18 Male     264\n11        17 Female   263\n12        18 Female   263\n\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nHow many participants have a non-empty plasma glucose value for each age?\nUse group_by() and summarise() to evaluate the poverty income ratio (family.PIR) by ethnicity. Which ethnicity has the highest poverty income ratio?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#1\nnhanes %&gt;% filter(!is.na(plasma.glucose)) %&gt;%\n  group_by(age.years) %&gt;%\n  count()\n\n# A tibble: 6 × 2\n# Groups:   age.years [6]\n  age.years     n\n      &lt;dbl&gt; &lt;int&gt;\n1        13   227\n2        14   232\n3        15   259\n4        16   258\n5        17   240\n6        18   241\n\n#2\nnhanes %&gt;% group_by(ethnicity) %&gt;%\n  summarise(mean_PIR = mean(family.PIR, na.rm = TRUE)) %&gt;%\n  arrange(desc(mean_PIR))\n\n# A tibble: 5 × 2\n  ethnicity                      mean_PIR\n  &lt;chr&gt;                             &lt;dbl&gt;\n1 Non-Hispanic White                 2.86\n2 Other Race - Including Multi-R     2.35\n3 Other Hispanic                     1.96\n4 Non-Hispanic Black                 1.90\n5 Mexican American                   1.74"
  },
  {
    "objectID": "session-materials/session0/bootcamp-2.html#exporting-data",
    "href": "session-materials/session0/bootcamp-2.html#exporting-data",
    "title": "Bootcamp Part 2: Starting with Data",
    "section": "Exporting data",
    "text": "Exporting data\nNow that you have learned how to use dplyr to extract information from or summarise your raw data, you may want to export these new data sets to share them with your collaborators or for archival.\nSimilar to the read_csv() function used for reading CSV files into R, there is a write_csv() function that generates CSV files from data frames.\nBefore using write_csv(), we are going to create a new folder, data_output, in our working directory that will store this generated dataset. We don’t want to write generated datasets in the same directory as our raw data. It’s good practice to keep them separate. The data folder should only contain the raw, unaltered data, and should be left alone to make sure we don’t delete or modify it. In contrast, our script will generate the contents of the data_output directory, so even if the files it contains are deleted, we can always re-generate them.\nLet’s use write_csv() to save the nhanes data.\n\nwrite_csv(nhanes, file = \"nhanes_processed.csv\")\n\n\nThe materials in this lesson have been adapted from work created by the (HBC)](http://bioinformatics.sph.harvard.edu/) and Data Carpentry (http://datacarpentry.org/), as well as materials created by Laurent Gatto, Charlotte Soneson, Jenny Drnevich, Robert Castelo, and Kevin Rue-Albert. These are open access materials distributed under the terms of the Creative Commons Attribution license (CC BY 4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
  },
  {
    "objectID": "session-materials/session3/session3.html",
    "href": "session-materials/session3/session3.html",
    "title": "Week 3: Plotting",
    "section": "",
    "text": "Data\nWe will be picking up where we left off with the data last week.\n\nlibrary(tidyverse) \n\nnhanes_processed &lt;- read_csv(\"week3Data.csv\") \n\n\n\n\n\n\n\n\n\n\nFirst steps with ggplot2\nAs you saw in this week’s reading, ggplot2 utilizes a specific syntax for creating plots. We can summarize it as:\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) +  &lt;GEOM_FUNCTION&gt;()\nWhere we define a dataset, choose which variables map to which aspects of the plot, and then choose the geom() or type of plot to draw.\nLet’s plug the NHANES dataset into a plot.\n\n1ggplot(nhanes_processed)\n\n\n1\n\ndata and mapping are positional arguments in the ggplot function, so we don’t have to name them. However, it can be good practice to include the argument names so that it’s immediately obvious what each argument is.\n\n\n\n\n\n\n\nSince we haven’t told ggplot what we want to display, we just get a blank plot. If we add some mappings for the x and y axes:\n\nggplot(nhanes_processed, aes(x = hba1c, y = plasma.glucose))\n\n\n\n\nWe now get labeled axes and scales based on the variable range. Finally, we can add a geom(). Let’s make a scatterplot, created with geom_point() in ggplot.\n\nggplot(nhanes_processed, aes(x = hba1c, y = plasma.glucose)) +\n  geom_point()\n\nWarning: Removed 1893 rows containing missing values (`geom_point()`).\n\n\n\n\n\nNow, let’s make a boxplot showing how BMI varies by subject ethnicity. Inside of the geom_boxplot function, we’ll also set the varwidth parameter to true so that the box sizes vary with how many samples are in each category.\n\nggplot(nhanes_processed, aes(x = ethnicity, y = bmi)) + \n  geom_boxplot(varwidth = TRUE)\n\nWarning: Removed 25 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nInstead of a boxplot, try making a bee swarm plot or a violin plot. Note that the beeswarm plot is in a separate package, ggbeeswarm. You might need to vary the cex argument in the beeswarm plot to increase the spacing between the strips.\n\n# install.packages(\"ggbeeswarm\")\nlibrary(ggbeeswarm)\n#TODO your plot here\n\n# Beeswarm plot\nggplot(nhanes_processed, aes(x = ethnicity, y = bmi)) + \n  geom_beeswarm(cex = 0.5)\n\nWarning: Removed 25 rows containing missing values (`geom_point()`).\n\n\n\n\n# Violin plot\nggplot(nhanes_processed, aes(x = ethnicity, y = bmi)) + \n  geom_violin()\n\nWarning: Removed 25 rows containing non-finite values (`stat_ydensity()`).\n\n\n\n\n\n\n\nNote that we can also easily make boxplots using R’s builtin plotting boxplot function.\n\nboxplot(bmi ~ ethnicity, data = nhanes_processed)\n\n\n\n\n\n\nMapping Variables\nBeyond the actual axes we can use mappings to encode variables as various aspects of a plot. Some of the most commonly used other mapping types are shape, fill, color, size, and linetype.\nFor instance, let’s take our scatterplot from before and color the points by diabetic status.\n\nggplot(nhanes_processed, aes(x = hba1c, y = plasma.glucose, color = diabetes)) +\n  geom_point() \n\nWarning: Removed 1893 rows containing missing values (`geom_point()`).\n\n\n\n\n\nIt is difficult to tell how many diabetic participants are in this plot, as it’s possible that the red diabetic points have been covered by the blue and green points. We can alter the transparency of the points by changing alpha. Remember we can also change parts of the plot outside of aes() to have them not depend on any variable.\n\nggplot(nhanes_processed, aes(x = hba1c, y = plasma.glucose, color = diabetes)) +\n  geom_point(alpha = 0.6) \n\nWarning: Removed 1893 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe can also have a single variable encoded into multiple parts of the plot.\n\nggplot(nhanes_processed, aes(x = hba1c, y = plasma.glucose, color = diabetes, shape = diabetes)) +\n  geom_point(alpha = 0.6) \n\nWarning: Removed 1893 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\nTry coloring your boxplot from before by age.years. What happens? What about when you use age.cat? Remember to use fill instead of color for shapes like boxplots.\n\n\n# We can't color by age since it's numeric, ggplot gives an error. \nggplot(nhanes_processed, aes(x = ethnicity, y = bmi, fill = age.years)) + \n  geom_boxplot()\n\nWarning: Removed 25 rows containing non-finite values (`stat_boxplot()`).\n\n\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\nggplot(nhanes_processed, aes(x = ethnicity, y = bmi, fill = age.cat)) + \n  geom_boxplot()\n\nWarning: Removed 25 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\nNow try flipping which variables are encoded in x and fill. Which version do you think works better?\n\n\nggplot(nhanes_processed, aes(x = age.cat, y = bmi, fill = ethnicity)) + \n  geom_boxplot()\n\nWarning: Removed 25 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nCustomizing Plots\nTaking a figure all the way to publication-quality can require careful fine tuning. ggplot has a variety of useful themes and other ways to improve a figure’s appearance and readability.\nHere’s an example of some of what you can do. Note that changing the fig.width setting for the code block will not effect how the image looks when exported.\n\n#Maybe we want a color scheme from a Wes Anderson movie:\nlibrary(wesanderson)\n\n#Note that this font import can take multiple minutes to run\npal &lt;- wes_palette(\"Zissou1\", 2, type = \"continuous\")\n\nggplot(nhanes_processed, aes(x = ethnicity, y = bmi, fill = dental.caries)) + \n geom_boxplot() +\n theme_minimal() +\n ggtitle(\"BMI by ethicity and dental caries\") +\n xlab(\"Ethnicity\") +\n ylab(\"BMI\") +\n scale_fill_manual(values = pal, name = \"Dental Caries\") +\n theme(text = element_text(size=14), axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\nWarning: Removed 25 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse a figure you’ve already created, or choose a new plot to create. Try to get that figure to as close to publication-ready as possible. The R Graph Gallery is a great place to look for inspiration on what you can do to improve your plots.\nIf there’s something you want to change but don’t know how, try checking the ggplot2 cheatsheet, the extended ggplot chapter, and the official documentation.\n\n# TODO your plotting code here\n\n\n\n\n\nExporting plots\n\n\n\n\n\n\nExercise\n\n\n\nThe Journal of the American Dental Association (JADA) manuscript guidlines list the following as their figure formatting requirements:\n\nFormats for Figures If your electronic artwork is created in a Microsoft Office application (Word, PowerPoint, Excel) then please supply “as is” in the native document format. Otherwise, regardless of the application used to create figures, the final artwork should be saved as or converted to 1 of the following formats:\n\nTIFF, JPEG, or PPT: Color or grayscale photographs (halftones): always use a minimum of 300 dpi.\nTIFF, JPEG, or PPT: Bitmapped line drawings: use a minimum of 1,000 dpi.\nTIFF, JPEG, or PPT: Combinations bitmapped line/halftone (color or grayscale): a minimum of 500 dpi is required.\n\n\nWhile Nature’s formatting guidelines are\nNature preferred formats are:\n\nLayered Photoshop (PSD) or TIFF format (high resolution, 300–600 dots per inch (dpi) for photographic images. In Photoshop, it is possible to create images with separate components on different layers. This is particularly useful for placing text labels or arrows over an image, as it allows them to be edited later. If you have done this, please send the Photoshop file (.psd) with the layers intact.\nAdobe Illustrator (AI), Postscript, Vector EPS or PDF format for figures containing line drawings and graphs, including figures combining text and line art with photographs or scans.\nIf these formats are not possible, we can also accept the following formats: JPEG (high-resolution, 300–600 dpi), CorelDraw (up to version 8), Microsoft Word, Excel or PowerPoint.\n\nExport your figure using ggsave to comply with one of these sets of guidelines."
  },
  {
    "objectID": "session-materials/session5/session5.html",
    "href": "session-materials/session5/session5.html",
    "title": "Week 5: Exploratory analysis and basic tests",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(DT)"
  },
  {
    "objectID": "session-materials/session5/session5.html#analyzing-the-adolescent-data",
    "href": "session-materials/session5/session5.html#analyzing-the-adolescent-data",
    "title": "Week 5: Exploratory analysis and basic tests",
    "section": "Analyzing the adolescent data",
    "text": "Analyzing the adolescent data\nAs we just saw, we can use group_by and summarize to see summary statistics by various subgroups. For instance, let’s take a look at dental caries and birthplace.\n\nnhanes |&gt;\n  group_by(dental.caries, birthplace) |&gt;\n  summarize(n = n())\n\n`summarise()` has grouped output by 'dental.caries'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   dental.caries [2]\n  dental.caries birthplace         n\n  &lt;lgl&gt;         &lt;chr&gt;          &lt;int&gt;\n1 FALSE         Outside the US   155\n2 FALSE         Within the US   1180\n3 FALSE         &lt;NA&gt;               1\n4 TRUE          Outside the US   251\n5 TRUE          Within the US   1758\n6 TRUE          &lt;NA&gt;               1\n\n\n\n\n\n\n\n\nSummarizing\n\n\n\nCalculate the percent of adolescents with dental caries for each birthplace category.\n\nnhanes |&gt;\n  group_by(birthplace) |&gt; # Group by birthplace\n  summarize(n = n(), caries = sum(dental.caries)) |&gt;\n  mutate(perc_caries = caries/n)\n\n# A tibble: 3 × 4\n  birthplace         n caries perc_caries\n  &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;       &lt;dbl&gt;\n1 Outside the US   406    251       0.618\n2 Within the US   2938   1758       0.598\n3 &lt;NA&gt;               2      1       0.5  \n\n\n\n\n\nInsulin\nLet’s look at the demographics of who’s taking insulin by birthplace and family income.\n\nnhanes |&gt;\n  filter(diabetes == \"diabetic\") |&gt;\n  group_by(birthplace, family.PIR.cat) |&gt; \n  summarize(n = n(), insulin = sum((DIQ050) == \"Yes\")) |&gt;\n  mutate(perc_insulin = insulin/n)\n\n`summarise()` has grouped output by 'birthplace'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 5\n# Groups:   birthplace [2]\n  birthplace     family.PIR.cat     n insulin perc_insulin\n  &lt;chr&gt;          &lt;chr&gt;          &lt;int&gt;   &lt;int&gt;        &lt;dbl&gt;\n1 Outside the US &lt;1                 2       1        0.5  \n2 Outside the US &gt;=1                1       0        0    \n3 Within the US  &lt;1                 6       2        0.333\n4 Within the US  &gt;=1               10       2        0.2  \n5 Within the US  &lt;NA&gt;               1       1        1    \n\n\nWe can also take a look at ethnicity. Let’s split this into two steps. First we make a table containing summarized data.\n\ninsulin_tbl &lt;- nhanes |&gt;\n  filter(diabetes == \"diabetic\") |&gt;\n  group_by(RIDRETH1, birthplace) |&gt; \n  summarize(n = n(), insulin = sum((DIQ050) == \"Yes\"), na.rm=TRUE) |&gt;\n  mutate(perc_insulin = insulin/n)\n\n`summarise()` has grouped output by 'RIDRETH1'. You can override using the\n`.groups` argument.\n\n\nAnd now we can plot it.\n\nggplot(insulin_tbl, aes(x = birthplace, y = perc_insulin)) + \n  geom_bar(stat = \"identity\") + \n  facet_grid(~RIDRETH1)\n\n\n\n\nUnfortunately, it turns out insulin data is quite sparse for adolescents in the dataset. Thus, we can’t really get a good picture of insulin use within our study population. This is probably why this data was not included in the paper’s analysis."
  },
  {
    "objectID": "session-materials/session7/session7.html",
    "href": "session-materials/session7/session7.html",
    "title": "Week 7: Advanced Regression",
    "section": "",
    "text": "This week we’ll take a deeper look at regression models using a different NHANES dataset."
  },
  {
    "objectID": "session-materials/session7/session7.html#load-the-data",
    "href": "session-materials/session7/session7.html#load-the-data",
    "title": "Week 7: Advanced Regression",
    "section": "Load the data",
    "text": "Load the data\nThere are 6063 observations, some are incomplete and have missing values for some covariates. There are 22 covariates, which have cryptic names and you need to use the meta-data to resolve them. The survey is very complex and typically any analysis requires a substantial amount of reading of the documentation. Here we will guide you past some of the hurdles.\nWe load up the data and the metadata. In the metadata we have a textual description of the phenotype, the short name, and the target. The target tells us which of the sampled individuals was eligible to answer the question.\n\nnhanesDataPath = \"\"\nload(\"d4.rda\")\nload(\"metaD.rda\")\nDT::datatable(metaD)\n\n\n\n\n\n\nWe will look at the relationship between the variable LBXTC (which is Total Cholesterol in mg/dL measured by a blood draw) and the age of the participant in years.\n\n\n\n\n\nAnd we can see that in this plot, over-plotting is a substantial issue here. You might also notice what seems like a lot of data at age 80, this is because any age over 80 was truncated to 80 to prevent reidentification of survey participants. In a complete analysis, this should probably be adjusted for in some way, but we will ignore it for now.\nWe can try some other methods, such as hexbin plotting and smoothScatter to get a better idea of the distribution of the data.\n\n\n\n\n\nNow we can see a few outliers - with extremely high serum cholesterol. We get a sense that the trend is not exactly a straight line, but rather a parabola, lower for the young and the old and a bit higher in the middle.\nWe fit a linear model first.\n\nlm1 = lm(d4$LBXTC ~ d4$RIDAGEYR)\nsummary(lm1)\n\n\nCall:\nlm(formula = d4$LBXTC ~ d4$RIDAGEYR)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-114.68  -27.95   -2.91   23.34  357.19 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 170.0140     1.4340  118.56   &lt;2e-16 ***\nd4$RIDAGEYR   0.3708     0.0285   13.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.24 on 5689 degrees of freedom\n  (372 observations deleted due to missingness)\nMultiple R-squared:  0.02891,   Adjusted R-squared:  0.02874 \nF-statistic: 169.4 on 1 and 5689 DF,  p-value: &lt; 2.2e-16\n\n\n\nplot(lm1$fitted.values, lm1$residuals)\n##fit a loess curve\nl2 = loess(lm1$residuals ~ lm1$fitted.values)\npl = predict(l2, newdata=sort(lm1$fitted.values))\nlines(x=sort(lm1$fitted.values), y=pl, col=\"blue\", lwd=2)\nabline(h=0, col=\"red\")\n\n\n\n\nNotice that both terms in the model are very significant, but that the multiple \\(R^2\\) is only around 2%.\nSo age, in years, is not explaining very much of the variation. But because we have such a large data set, the parameter estimates are found to be significantly different from zero."
  },
  {
    "objectID": "session-materials/session7/session7.html#spline-models",
    "href": "session-materials/session7/session7.html#spline-models",
    "title": "Week 7: Advanced Regression",
    "section": "Spline Models",
    "text": "Spline Models\nWhen a linear model does not appear sufficient we can try other models. One choice is to use natural splines, which are very flexible. They create a “knotted” line, where at each knot the slope of the line can change. They are based on B-splines with the prevision that the model is linear outside the range of the data.\nHowever, we have to decide the number of knots we want to use in the model. Based on the initial analysis, we chose to use df=7, which gives five internal knots when fitting the splines. You have almost 6,000 degrees of freedom here, so using up a few to get a more appropriate fit seems good.\n\nlibrary(\"splines\")\nlm2 = lm(d4$LBXTC ~ ns(d4$RIDAGEYR, df=7))\nsummary(lm2)\n\n\nCall:\nlm(formula = d4$LBXTC ~ ns(d4$RIDAGEYR, df = 7))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-113.43  -26.32   -2.88   22.47  343.31 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               154.799      2.178  71.071  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)1   39.956      3.379  11.826  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)2   32.705      4.074   8.028 1.20e-15 ***\nns(d4$RIDAGEYR, df = 7)3   55.583      3.637  15.283  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)4   42.275      3.725  11.347  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)5   30.111      3.352   8.984  &lt; 2e-16 ***\nns(d4$RIDAGEYR, df = 7)6   41.098      5.758   7.137 1.07e-12 ***\nns(d4$RIDAGEYR, df = 7)7   15.992      2.478   6.453 1.19e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.62 on 5683 degrees of freedom\n  (372 observations deleted due to missingness)\nMultiple R-squared:  0.1044,    Adjusted R-squared:  0.1033 \nF-statistic: 94.65 on 7 and 5683 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use an anova to compare the models.\n\nanova(lm1, lm2)\n\nAnalysis of Variance Table\n\nModel 1: d4$LBXTC ~ d4$RIDAGEYR\nModel 2: d4$LBXTC ~ ns(d4$RIDAGEYR, df = 7)\n  Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   5689 9674185                                  \n2   5683 8922039  6    752146 79.848 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice also that the multiple \\(R^2\\) went up to about 10%, a pretty substantial increase, suggesting that the curvilinear nature of the relationship is substantial.\nThe residual standard error also decreased by about 5%.\nWe have lost the simple explanation that comes from fitting a linear model. We cannot say that your serum cholesterol increases by \\(a\\) units per year, but that model was wrong, so it really shouldn’t be used.\nWe can use the regression model we fit to make predictions for any one, and these are substantially more accurate."
  },
  {
    "objectID": "session-materials/session7/session7.html#spline-models-1",
    "href": "session-materials/session7/session7.html#spline-models-1",
    "title": "Week 7: Advanced Regression",
    "section": "Spline Models",
    "text": "Spline Models\nEven though the regression summary prints out a different row for each spline term, they are not independent variables, and you need to either retain them all, or retain none of them."
  },
  {
    "objectID": "session-materials/session7/session7.html#sex",
    "href": "session-materials/session7/session7.html#sex",
    "title": "Week 7: Advanced Regression",
    "section": "Sex",
    "text": "Sex\nNext we might want to start to add other variables and explore the different relationships. Let’s consider sex, for now we will leave out age, and just try to understand what happens with sex First let’s fit the model without an intercept\n\nlm3 = lm(LBXTC ~ RIAGENDR-1, data=d4)\nsummary(lm3)\n\n\nCall:\nlm(formula = LBXTC ~ RIAGENDR - 1, data = d4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-107.55  -29.55   -3.55   24.21  360.45 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \nRIAGENDRMale   184.5534     0.7974   231.4   &lt;2e-16 ***\nRIAGENDRFemale 189.7914     0.7692   246.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.76 on 5689 degrees of freedom\n  (372 observations deleted due to missingness)\nMultiple R-squared:  0.9526,    Adjusted R-squared:  0.9526 \nF-statistic: 5.722e+04 on 2 and 5689 DF,  p-value: &lt; 2.2e-16\n\n\nHere we can see the mean for males is a bit higher than for females Both are significant, and notice how large the multiple \\(R^2\\) value is. This is not a very interesting test - we are asking if the mean is zero, which isn’t even physically possible: \\[\n  y_i = \\beta_M \\cdot 1_{M,i} + \\beta_F \\cdot 1_{F,i}\n\\] Where \\(1_{M,i}\\) is 1 if the \\(i^{th}\\) case is male and zero otherwise, similarly for \\(1_{F,i}\\) Instead let’s use a model with an intercept to ask if the mean for males is different than that for females. \\[\n  y_i = \\beta_0 + \\beta_1 \\cdot 1_{F,i}\n\\]\nThis way, \\(E[Y|M] = \\beta_0\\) and \\(E[Y|F] = \\beta_0 + \\beta_1\\), and \\(\\beta_1\\) estimates the difference in mean between male and female\n\nlm3 = lm(LBXTC ~ RIAGENDR, data=d4)\nsummary(lm3)\n\n\nCall:\nlm(formula = LBXTC ~ RIAGENDR, data = d4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-107.55  -29.55   -3.55   24.21  360.45 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    184.5534     0.7974 231.434  &lt; 2e-16 ***\nRIAGENDRFemale   5.2380     1.1080   4.728 2.33e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.76 on 5689 degrees of freedom\n  (372 observations deleted due to missingness)\nMultiple R-squared:  0.003913,  Adjusted R-squared:  0.003738 \nF-statistic: 22.35 on 1 and 5689 DF,  p-value: 2.327e-06\n\n\nNow we see an Intercept term, which is the overall mean. The estimate for females represents how they differ by sex, if it is zero then there is no difference in total cholesterol between men and women."
  },
  {
    "objectID": "session-materials/session7/session7.html#looking-at-more-variables",
    "href": "session-materials/session7/session7.html#looking-at-more-variables",
    "title": "Week 7: Advanced Regression",
    "section": "Looking at more variables",
    "text": "Looking at more variables\nNow we will put together a set of features (variables) that we are interested in. For simplicity we only keep participants for which we have all the data. Here, we use apply, which apply a function to each row in the dataset. We also could have used the tidyverse functions for handling missing data which we’ve seen previously.\n\nivars = c(\"RIDAGEYR\", \"RIAGENDR\", \"RIDRETH1\", \"DMDEDUC2\", \"INDFMPIR\", \"LBDHDD\", \"LBXGH\", \"BMXBMI\", \"LBXTC\")\n\nd4sub = d4[,ivars]\ncompCases = apply(d4sub, 1, function(x) sum(is.na(x)))\ncC = compCases==0\nd4sub = d4sub[cC,]\ndim(d4sub)\n\n[1] 4592    9"
  },
  {
    "objectID": "session-materials/session7/session7.html#one-quick-transformation",
    "href": "session-materials/session7/session7.html#one-quick-transformation",
    "title": "Week 7: Advanced Regression",
    "section": "One quick transformation",
    "text": "One quick transformation\nThe variable DMDEDUC2 is a bit too granular for our purposes. Let’s modify it to be less than high school, high school and more than high school\n\ntable(d4sub$DMDEDUC2)\n\n\n                               Less than 9th grade \n                                               523 \n9-11th grade (Includes 12th grade with no diploma) \n                                               506 \n            High school graduate/GED or equivalent \n                                              1007 \n                         Some college or AA degree \n                                              1391 \n                         College graduate or above \n                                              1164 \n                                        Don't Know \n                                                 1 \n\ndd = d4sub$DMDEDUC2\n\ndd[dd==\"Don't Know\"] = NA\n\neduS = ifelse(dd == \"Less than 9th grade\" | dd ==\"9-11th grade (Includes 12th grade with no diploma)\", \"&lt;HS\", ifelse(dd == \"High school graduate/GED or equivalent\", \"HS\", \"&gt;HS\" ))\n\n#stick this into our dataframe\n#and drop the NA\nd4sub$eduS = eduS\nd4sub = d4sub[-which(is.na(eduS)), ]\n\ntable(eduS, dd, useNA = \"always\")\n\n      dd\neduS   Less than 9th grade 9-11th grade (Includes 12th grade with no diploma)\n  &lt;HS                  523                                                506\n  &gt;HS                    0                                                  0\n  HS                     0                                                  0\n  &lt;NA&gt;                   0                                                  0\n      dd\neduS   High school graduate/GED or equivalent Some college or AA degree\n  &lt;HS                                       0                         0\n  &gt;HS                                       0                      1391\n  HS                                     1007                         0\n  &lt;NA&gt;                                      0                         0\n      dd\neduS   College graduate or above Don't Know &lt;NA&gt;\n  &lt;HS                          0          0    0\n  &gt;HS                       1164          0    0\n  HS                           0          0    0\n  &lt;NA&gt;                         0          0    1"
  },
  {
    "objectID": "session-materials/session7/session7.html#back-to-regression",
    "href": "session-materials/session7/session7.html#back-to-regression",
    "title": "Week 7: Advanced Regression",
    "section": "Back to Regression",
    "text": "Back to Regression\nLet’s run a regression including all of our variables on cholesterol. We can do this by just indicating . in the formula.\n\nlmF = lm(LBXTC ~ ., data=d4sub)\nsummary(lmF)\n\n\nCall:\nlm(formula = LBXTC ~ ., data = d4sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-123.19  -27.68   -3.40   22.41  362.93 \n\nCoefficients: (2 not defined because of singularities)\n                                                            Estimate Std. Error\n(Intercept)                                                139.06987    5.34623\nRIDAGEYR                                                     0.11231    0.03739\nRIAGENDRFemale                                               0.99071    1.29161\nRIDRETH1Other Hispanic                                      -2.41290    2.23553\nRIDRETH1Non-Hispanic White                                  -1.86248    1.94109\nRIDRETH1Non-Hispanic Black                                  -8.60649    2.07247\nRIDRETH1Other Race - Including Multi-Racial                  1.79765    2.26131\nDMDEDUC29-11th grade (Includes 12th grade with no diploma)   1.95438    2.59288\nDMDEDUC2High school graduate/GED or equivalent               2.09692    2.32957\nDMDEDUC2Some college or AA degree                            2.23659    2.30162\nDMDEDUC2College graduate or above                           -1.09357    2.50664\nINDFMPIR                                                     1.13163    0.43699\nLBDHDD                                                       0.42540    0.03907\nLBXGH                                                        2.52364    0.55377\nBMXBMI                                                       0.21048    0.09372\neduS&gt;HS                                                           NA         NA\neduSHS                                                            NA         NA\n                                                           t value Pr(&gt;|t|)    \n(Intercept)                                                 26.013  &lt; 2e-16 ***\nRIDAGEYR                                                     3.003  0.00268 ** \nRIAGENDRFemale                                               0.767  0.44310    \nRIDRETH1Other Hispanic                                      -1.079  0.28049    \nRIDRETH1Non-Hispanic White                                  -0.960  0.33736    \nRIDRETH1Non-Hispanic Black                                  -4.153 3.34e-05 ***\nRIDRETH1Other Race - Including Multi-Racial                  0.795  0.42668    \nDMDEDUC29-11th grade (Includes 12th grade with no diploma)   0.754  0.45104    \nDMDEDUC2High school graduate/GED or equivalent               0.900  0.36810    \nDMDEDUC2Some college or AA degree                            0.972  0.33123    \nDMDEDUC2College graduate or above                           -0.436  0.66266    \nINDFMPIR                                                     2.590  0.00964 ** \nLBDHDD                                                      10.890  &lt; 2e-16 ***\nLBXGH                                                        4.557 5.32e-06 ***\nBMXBMI                                                       2.246  0.02477 *  \neduS&gt;HS                                                         NA       NA    \neduSHS                                                          NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.72 on 4576 degrees of freedom\nMultiple R-squared:  0.04276,   Adjusted R-squared:  0.03983 \nF-statistic:  14.6 on 14 and 4576 DF,  p-value: &lt; 2.2e-16\n\n\nWe see that being Non-hispanic black seems to have a pretty big effect, so we might want to just include that, and group all other ethnicities together. Education level seems to have little to add, let’s drop it to simplify the analysis. It might be good to get a sense of the relationship with the poverty level variable INDFMPIR.\n\n Black = ifelse(d4sub$RIDRETH1 == \"Non-Hispanic Black\", \"B\", \"nonB\")\n ivars = c(\"RIDAGEYR\", \"INDFMPIR\", \"LBDHDD\", \"LBXGH\", \"BMXBMI\", \"LBXTC\")\n\n d5sub = cbind(d4sub[,ivars], Black)\n lmFx = lm(LBXTC ~ . , data=d5sub)\n summary(lmFx)\n\n\nCall:\nlm(formula = LBXTC ~ ., data = d5sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-123.65  -27.94   -3.45   22.47  360.06 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 132.78645    5.19610  25.555  &lt; 2e-16 ***\nRIDAGEYR      0.09909    0.03618   2.739  0.00619 ** \nINDFMPIR      0.86401    0.38147   2.265  0.02356 *  \nLBDHDD        0.43038    0.03662  11.753  &lt; 2e-16 ***\nLBXGH         2.60260    0.54925   4.738 2.22e-06 ***\nBMXBMI        0.21216    0.09002   2.357  0.01848 *  \nBlacknonB     7.49101    1.52918   4.899 9.98e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.73 on 4584 degrees of freedom\nMultiple R-squared:  0.0408,    Adjusted R-squared:  0.03955 \nF-statistic:  32.5 on 6 and 4584 DF,  p-value: &lt; 2.2e-16\n\n\nWe can compare the two models using the anova function.\n\nanova(lmFx, lmF) \n\nExercise: Plot RIDAGEYR versus INDFMPIR. What do you see in the plot? Does anything cause you concern about fitting a linear model?"
  },
  {
    "objectID": "session-materials/session7/session7.html#missing-indicator-approach",
    "href": "session-materials/session7/session7.html#missing-indicator-approach",
    "title": "Week 7: Advanced Regression",
    "section": "Missing Indicator Approach",
    "text": "Missing Indicator Approach\nThe missing indicator approach may be useful for data where there is a limit of detection, so that values below some lower bound \\(\\alpha_L\\) or above some upper bound \\(\\alpha_U\\) are set to \\(\\alpha_L\\) or to \\(\\alpha_U\\) respectively. A similar approach can be used for the Windsorization used for RIDAGEYR variable, where values over 80 are set to 80. We are not proposing using this for other types of missingness, although there is a righ literature and users may want to explore the broader use of this method. However, it is important to realize that often bias or increased variance may obtain, often dependent on untestable assumptions.\nThe reason that we believe this approach is appropriate for the cases listed is that we actually did measure the variable, and many over variables on those individuals, but we cannot report the exact value for any individual. Hence, one can interpret the indicator as being some form of average over all individuals affected by the reporting limits. It is probably worth noting that these limitations have implications for predication methods. While one in principle could estimate some outcome for an 88 year old person, the data won’t support that prediction. Instead one should ignore the linear predictor for age and use the indicator variable. Chiou et al (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6812630/) provide some rationale for logistic regression and the references therein point to results for linear regression. Groenwold et al (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3414599/) provide advice on more general use of the method to address missing data.\nNext let’s try to fix out model to deal with the repeated values in these two variables. Now an important consideration is to try and assess just how to interpret them. For RIDAGEYR the documentation states that anyone over age 80 in the database has their age represented as 80. This is not censoring. The measurements (eg BMI, cholesterol etc.) were all made on a person of some age larger than 80. We just Windsorized their ages, and so these individuals really are not the same as the others, where we are getting accurate age values.\n(https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3414599/)\nSo basically what we want to do is to create a dummy variable for those whose age is reported as 80.\n\nAge80 = d5sub$RIDAGEYR == 80\nd6sub=cbind(d5sub, Age80)\n\nlmFx2  = lm(LBXTC ~ . + Age80 , data=d6sub)\n summary(lmFx2)\n\n\nCall:\nlm(formula = LBXTC ~ . + Age80, data = d6sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-125.97  -28.09   -3.05   22.34  359.17 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 130.95895    5.16754  25.343  &lt; 2e-16 ***\nRIDAGEYR      0.23581    0.03996   5.901 3.88e-09 ***\nINDFMPIR      0.68993    0.37964   1.817   0.0692 .  \nLBDHDD        0.42924    0.03638  11.799  &lt; 2e-16 ***\nLBXGH         2.29476    0.54709   4.194 2.79e-05 ***\nBMXBMI        0.15983    0.08969   1.782   0.0748 .  \nBlacknonB     7.78276    1.51968   5.121 3.16e-07 ***\nAge80TRUE   -22.72915    2.90328  -7.829 6.08e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.46 on 4583 degrees of freedom\nMultiple R-squared:  0.05346,   Adjusted R-squared:  0.05201 \nF-statistic: 36.98 on 7 and 4583 DF,  p-value: &lt; 2.2e-16\n\n\nExercise: What changes do you notice in the model fit? Use the anova function to compare lmFx3 to lmFx2. How do you interpret the output? Hint: look at INDFMPIR\nExercise: Try to fit missing indicator variables for both of the repeated values in the INDFMPIR variable. Then interpret the output.\n\nPov5 = d6sub$INDFMPIR == 5\nPov0 = d6sub$INDFMPIR == 0\nd7sub = cbind(d6sub, Pov5, Pov0)\n\nlmFx2  = lm(LBXTC ~ . + Age80 + Pov0 + Pov5, data=d7sub)\n summary(lmFx2)\n\n\nCall:\nlm(formula = LBXTC ~ . + Age80 + Pov0 + Pov5, data = d7sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-126.04  -28.11   -3.09   22.39  358.65 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 131.44125    5.21065  25.226  &lt; 2e-16 ***\nRIDAGEYR      0.23419    0.04003   5.850 5.25e-09 ***\nINDFMPIR      0.44100    0.54075   0.816   0.4148    \nLBDHDD        0.42898    0.03639  11.788  &lt; 2e-16 ***\nLBXGH         2.30525    0.54743   4.211 2.59e-05 ***\nBMXBMI        0.15911    0.08977   1.773   0.0764 .  \nBlacknonB     7.76271    1.52024   5.106 3.42e-07 ***\nAge80TRUE   -22.67132    2.90734  -7.798 7.74e-15 ***\nPov5TRUE      1.35245    2.30269   0.587   0.5570    \nPov0TRUE     -3.09279    6.30631  -0.490   0.6239    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.47 on 4581 degrees of freedom\nMultiple R-squared:  0.05357,   Adjusted R-squared:  0.05171 \nF-statistic: 28.81 on 9 and 4581 DF,  p-value: &lt; 2.2e-16\n\n\nIt seems that some of the apparent effect of INDFMPIR seems to be related to the fact that we are not fitting RIDAGEYR properly.\n\n lmFx3 = lm(LBXTC ~ ns(RIDAGEYR, df=7)+ INDFMPIR+ LBDHDD+LBXGH + BMXBMI + Black + Age80 + Pov0 + Pov5, data=d7sub)\n summary(lmFx3)\n\n\nCall:\nlm(formula = LBXTC ~ ns(RIDAGEYR, df = 7) + INDFMPIR + LBDHDD + \n    LBXGH + BMXBMI + Black + Age80 + Pov0 + Pov5, data = d7sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-109.14  -27.01   -3.55   22.15  350.63 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           123.26835    5.74709  21.449  &lt; 2e-16 ***\nns(RIDAGEYR, df = 7)1  29.34696    3.89739   7.530 6.08e-14 ***\nns(RIDAGEYR, df = 7)2  27.30104    5.01414   5.445 5.46e-08 ***\nns(RIDAGEYR, df = 7)3  44.63075    4.49373   9.932  &lt; 2e-16 ***\nns(RIDAGEYR, df = 7)4  27.23653    4.71406   5.778 8.07e-09 ***\nns(RIDAGEYR, df = 7)5  14.25183    4.46425   3.192 0.001420 ** \nns(RIDAGEYR, df = 7)6  30.51098    8.12103   3.757 0.000174 ***\nns(RIDAGEYR, df = 7)7   7.84477    5.21942   1.503 0.132909    \nINDFMPIR               -0.07063    0.53106  -0.133 0.894207    \nLBDHDD                  0.44469    0.03569  12.460  &lt; 2e-16 ***\nLBXGH                   1.87930    0.53859   3.489 0.000489 ***\nBMXBMI                  0.06358    0.08824   0.721 0.471190    \nBlacknonB               8.23587    1.49119   5.523 3.52e-08 ***\nAge80TRUE              -7.16363    5.57718  -1.284 0.199048    \nPov0TRUE               -6.82759    6.18622  -1.104 0.269792    \nPov5TRUE                0.45875    2.25604   0.203 0.838876    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.62 on 4575 degrees of freedom\nMultiple R-squared:  0.09415,   Adjusted R-squared:  0.09118 \nF-statistic:  31.7 on 15 and 4575 DF,  p-value: &lt; 2.2e-16\n\n\nExercise\nIn the code below, we drop the terms that were not statistically significant in the model and then compare this smaller model to the larger one, above. Try interpreting the results.\n\nlmFx4 = lm(LBXTC ~ ns(RIDAGEYR, df=7) + LBDHDD+LBXGH+Black, data=d7sub)\nsummary(lmFx4)\n\n\nCall:\nlm(formula = LBXTC ~ ns(RIDAGEYR, df = 7) + LBDHDD + LBXGH + \n    Black, data = d7sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-107.41  -27.02   -3.52   22.08  350.52 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           125.09513    5.00892  24.974  &lt; 2e-16 ***\nns(RIDAGEYR, df = 7)1  29.75058    3.88181   7.664 2.18e-14 ***\nns(RIDAGEYR, df = 7)2  27.26624    4.98275   5.472 4.68e-08 ***\nns(RIDAGEYR, df = 7)3  45.51138    4.42996  10.274  &lt; 2e-16 ***\nns(RIDAGEYR, df = 7)4  26.21025    4.58323   5.719 1.14e-08 ***\nns(RIDAGEYR, df = 7)5  17.38565    3.87109   4.491 7.26e-06 ***\nns(RIDAGEYR, df = 7)6  28.13377    7.76676   3.622 0.000295 ***\nns(RIDAGEYR, df = 7)7   2.17338    2.73486   0.795 0.426833    \nLBDHDD                  0.43702    0.03422  12.772  &lt; 2e-16 ***\nLBXGH                   1.90936    0.53091   3.596 0.000326 ***\nBlacknonB               8.09991    1.47684   5.485 4.37e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.61 on 4580 degrees of freedom\nMultiple R-squared:  0.09345,   Adjusted R-squared:  0.09147 \nF-statistic: 47.21 on 10 and 4580 DF,  p-value: &lt; 2.2e-16\n\nanova(lmFx4, lmFx3)\n\nAnalysis of Variance Table\n\nModel 1: LBXTC ~ ns(RIDAGEYR, df = 7) + LBDHDD + LBXGH + Black\nModel 2: LBXTC ~ ns(RIDAGEYR, df = 7) + INDFMPIR + LBDHDD + LBXGH + BMXBMI + \n    Black + Age80 + Pov0 + Pov5\n  Res.Df     RSS Df Sum of Sq      F Pr(&gt;F)\n1   4580 7186815                           \n2   4575 7181301  5    5513.8 0.7025 0.6215"
  }
]