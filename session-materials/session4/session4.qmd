---
title: "Week 4: Preparing Data"
author: "Center for Computational Biomedicine"
format: 
  html: default
code-annotations: select
---

```{r}
library(tidyverse)
```

# Data

This week we are going to start learning how to get to the cleaned data we have been using for the past two weeks. 

Note that this week the data is in the `.RData` format.
The `.RData` format is specific to R and allows us to easily dump multiple objects to a file exactly as the are inside of our R session. 
This format is not recommended for research as it limits analyses to only using R and could potentially not be compatible with a future R version. 
However, for today it will allow us to jump right into the data cleaning process. 

The `nhanes_beheshti_data.RData` file contains 3 separate dataframes, `base_df_d`, `base_df_e`, and `base_df_f`.
They each contain the equivalent set of columns from the `D`, `E`, and `F` NHANES data files for 2005-2006, 2007-2008, and 2009-2010, respectively.  

Let's load and take a look at the data.

```{r}
load("nhanes_beheshti_data.RData")
```
```{r}
summary(base_df_d)
```
```{r}
summary(base_df_e)
```
```{r}
summary(base_df_f)
```



::: {.callout-info collapse="true"}
## See more about how we downloaded the data

The NHANES data was downloaded using a packaged called `phonto`.
`phonto` connects to a database of the NHANES data developed by the Center for Computational Biomedicine (CCB).
The database is a relational database running on a virtual machine using the containerization software [Docker](https://www.docker.com/). 

We will be learning a bit more about this in the last week of class.
The code used to download the data is shown here:

```r
library(phonto)
library(DBI)

cols_d = list(DEMO_D=c("RIDAGEYR","RIAGENDR","RIDRETH1","DMDEDUC2", 
                       "DMDBORN", "INDFMPIR", "SDMVPSU", "SDMVSTRA",
                       "WTINT2YR", "WTMEC2YR"), 
              OHX_D =c("OHXDECAY", "OHXREST"),
              GLU_D =c("LBXGLU", "WTSAF2YR"), GHB_D = "LBXGH",
              BMX_D="BMXBMI"
)
base_df_d <- jointQuery(cols_d)

cols_e = list(DEMO_E=c("RIDAGEYR","RIAGENDR","RIDRETH1","DMDEDUC2", 
                       "DMDBORN2", "INDFMPIR", "SDMVPSU", "SDMVSTRA",
                       "WTINT2YR", "WTMEC2YR"), 
              OHX_E =c("OHXDECAY", "OHXREST"),
              GLU_E =c("LBXGLU", "WTSAF2YR"), GHB_E = "LBXGH",
              BMX_E ="BMXBMI"
)
base_df_e <- jointQuery(cols_e)

cols_f = list(DEMO_F=c("RIDAGEYR","RIAGENDR","RIDRETH1","DMDEDUC2", 
                       "DMDBORN2", "INDFMPIR", "SDMVPSU", "SDMVSTRA",
                       "WTINT2YR", "WTMEC2YR"), 
              OHXDEN_F =c("OHXDECAY", "OHXREST"),
              GLU_F =c("LBXGLU", "WTSAF2YR"), GHB_F = "LBXGH",
              BMX_F ="BMXBMI"
)
base_df_f <- jointQuery(cols_f)
save(base_df_d, base_df_e, base_df_f, file = "nhanes_beheshti_data.RData")
```
:::


# Preparing the data

In the reading, we saw how to combine multiple sets of observations by appending datasets. However, before we need to make sure corresponding columns between years can be combined. 

NHANES will often change how it encodes or records different variables from year to year.
Our first red flag are any columns whose name has changed.
Let's take a look at the column names.

```{r}
colnames(base_df_d)
colnames(base_df_e)
colnames(base_df_f)
```

We can also check for equality directly:

```{r}
colnames(base_df_d)[colnames(base_df_d) != colnames(base_df_e)]
colnames(base_df_e)[colnames(base_df_e) != colnames(base_df_f)]
colnames(base_df_f)[colnames(base_df_f) != colnames(base_df_d)]
```

It looks like `DMDBORN` changed after 2006 to `DMDBORN2`. 
If we check the demographic variable definitions for the three collection cycles [DEMO_D](https://wwwn.cdc.gov/nchs/nhanes/2005-2006/DEMO_D.htm#DMDBORN), [DEMO_E](https://wwwn.cdc.gov/Nchs/Nhanes/2007-2008/DEMO_E.htm#DMDBORN2), and [DEMO_F](https://wwwn.cdc.gov/Nchs/Nhanes/2009-2010/DEMO_F.htm#DMDBORN2) we can see that the encoding has changed over time.

To continue the analysis, let's also convert these dataframes into tibbles so we can take advantage of Tidyverse functionality. 

```{r}
tib_d <- as_tibble(base_df_d)
tib_e <- as_tibble(base_df_e)
tib_f <- as_tibble(base_df_f)
```

::: {.callout-tip appearance="minimal"}
## Exercise
a. Resolve the change the variable coding between the datasets.
Use your own judgement on the best way to resolve this issue. 

```{r}

# Step 1: Change things to have the same sets of values
# TODO your code here

# Step 2: Rename the columns to fit between the datasets
tib_e <- rename(tib_e, DMDBORN = DMDBORN2)
tib_f <- rename(tib_f, DMDBORN = DMDBORN2)
```

b. Check table 1b to see how the authors chose to handle this in [Beheshti et. al. 2021](https://pubmed.ncbi.nlm.nih.gov/33892837/). Do you think there were any other factors which went into their decision?

:::

## Combining the data

Now we can combine the data together.

```{r}
# Combine all years together
tib_all <- tib_d %>%
  bind_rows(tib_e) %>%
  bind_rows(tib_f)
```


## Filter relevant population

::: callout-tip
## Which order should I process my data in?

Choosing the order to perform data cleaning steps depends on a competing factors.

1. **Where information is needed**: Even if an analysis is being performed on only a subset of dataset, the full dataset can help inform data processing decisions such as how to handle missing data. Next week, we'll see that when using survey sample weights we need to use the full dataset to infer population statistics, and thus will need to clean and transform variables before getting the adolecent population. 
2. **Processing speed**: Smaller datasets are both faster to process and take up less memory than larger datasets. Thus, performing steps which eliminate data first can result in a faster-running computational pipeline. Modern computers can handle most basic data-cleaning operations on hundreds of thousands of observations in seconds, so most of the time we don't need to worry about speed. However, for large datasets or complex algorithms it may become necessary to first reduce the size of your data before performing further cleaning steps. 

:::

Next we need to filter the summary population. 
Luckily, the paper provides total numbers of participants for most of the filtering steps performed. 
This makes it much easier for us to check our work. 

>The total number of individuals who participated in the
NHANES from 2005 to 2010 were 31,034 study-participants.
Among them, 3,660 were nonedentulous adolescents (aged 13
to 18 years old at the time of screening) who had exam survey
weights assigned. From those 3,660 adolescents, data to measure
the main outcome variable (dental caries experience) were
available for a final sample of 3,346 adolescents, representing
the population of 24,386,135 U.S. adolescents after applying
the NHANES sample weights, which was described and analyzed
in this study.

We can start by filtering on age. 
However, `RIDAGEYR` is currently not a numeric column. 
If we try converting `RIDAGEYR` to be numeric using `as.numeric`, we get a warning:

```{r}
all_ado <- tib_all %>%
  mutate(RIDAGEYR = as.numeric(RIDAGEYR)) # Convert age to be numeric
```

::: {.callout-tip appearance="minimal"}
## Exercise

a. What is causing this warning? Do we need to resolve it?
**Hint** If we are getting `NA` when converting to numeric, then there must be some column entries which R cannot convert to be numbers.

b. Once you've decided how to deal with the warning, filter `tib_all` to only include the desired population as described in the paper. Check how many rows you have at each step to double-check your work. 

```{r}
all_ado <- all_ado %>%
  filter(RIDAGEYR >= 13 & RIDAGEYR <= 18) %>% # Gets the 3660 nonedentulous adolescents
  filter(!is.na(OHXDECAY)) # Gets the 3346 with non-NA dental carie variable
```

:::

# Data cleaning pipeline

We can choose to leave the variables with their NHANES column names or change them to be a bit more human-readable. 
Either way, we also want to convert each variable to it's appropriate type or category. 

## Numeric variables
Let's start by converting numeric columns:

```{r}
# Set numeric columns to be numeric
cleaned_ado <- mutate(all_ado,
                   LBXGLU = as.numeric(LBXGLU),
                   LBXGH = as.numeric(LBXGH),
                   BMXBMI = as.numeric(BMXBMI))
```

## Categorical variables

Many of the NHANES values are are categorical data, bur right now are stored as text. 
We can check what values exist by converting them to factors before making the decision of how to handle them in the analysis: 

```{r}
tib_all %>%
  mutate(across(c(RIAGENDR, RIDRETH1, DMDEDUC2, DMDBORN), as.factor), .keep = "none") %>%
  summary()
```

It looks like one of the options for `DMDBORN` has a lingering quote character.
This can happen due to irregularities in how NHANES data is presented or small mistakes in data processing. 
We use the `gsub` function to replace all instances of double quotes with the empty string in the column.

```{r}
cleaned_ado <- mutate(cleaned_ado,
                   RIAGENDR = as.factor(RIAGENDR),
                   RIDRETH1 = as.factor(RIDRETH1),
                   DMDBORN = gsub("\"", "", DMDBORN), # Remove quotes
                   DMDBORN = as.factor(DMDBORN),
                   OHXDECAY = (OHXDECAY == "Yes"),
                   OHXREST = (OHXREST == "Yes"))

```

## New variables

We need to create the variables mentioned in the paper are not directly present in NHANES.
For some variables such as country of birth, we could store the categories in `DMDBorn` but this way we can also keep the original distribution.  

```{r}
# Set columns to categories as in the paper
cleaned_ado <- mutate(cleaned_ado,

                    
                    age.cat = cut(
                      RIDAGEYR,
                      breaks = c(13, 15, 18),
                      include.lowest = TRUE,
                      labels = c("13-15", "16-18")),
                   
                    plasma.glucose.cat = case_when(
                     LBXGLU < 100 ~ "<100 mg/dl",
                     LBXGLU < 126 ~ ">=100 mg/dl and <126 mg/dl", 
                     LBXGLU >= 126 ~ ">=126 mg/dl",
                     .default = NA),
                   
                   hba1c.cat = case_when(
                     LBXGH < 5.7 ~ "<5.7%",
                     LBXGH >= 5.7 ~ ">=5.7% and <6.5%",
                     LBXGH >= 6.5 ~ ">= 6.5%",
                     .default = NA),
                   
                   bmi.cat = case_when( #Technically we should also have an underweight category
                     BMXBMI < 25 ~ "Normal", 
                     BMXBMI < 30 ~ "Overweight",
                     BMXBMI >= 30 ~ "Obese",
                     .default = NA), 
                   
                   family.PIR.cat = case_when(
                     INDFMPIR == "PIR value greater than or equa" ~ ">= 1",
                     INDFMPIR == "Value greater than or equal to" ~ ">= 1",
                     as.numeric(INDFMPIR) >= 1 ~ ">=1",
                     as.numeric(INDFMPIR) < 1 ~ "<1",
                     .default = NA),
                   family.PIR.car = as.numeric(family.PIR.cat) < 1.0,
                   
                   birthplace = case_when(
                     DMDBORN == "Born in 50 US States or Washi" ~ "Within the US",
                     is.na(DMDBORN) ~ NA,
                     DMDBORN == "Don't Know" ~ NA,
                     DMDBORN == "Refused" ~ NA,
                    .default = "Outside the US"),
                   dental.caries = OHXDECAY | OHXREST)

```

::: {.callout-tip appearance="minimal"}
## Exercise

Add a `diabetes` column to the dataset based on the guidelines from Beheshti et. al:

>Individuals with an HbA1C of at least 6.5 percent
or a plasma-fasting glucose value of at least 126 mg/dl were
considered diabetics; prediabetics were those whose HbA1C
ranged from 5.7 percent to 6.4 percent and whose fasting
plasma glucose remained within 100 to 125 mg/dl; the remaining
study participants, who had less than 5.7 percent HbA1C
and less than 100 mg/dl fasting plasma glucose, were classified
as nondiabetics.

It is recommended to use `case_when`, but feel free to use any method you want. 

*Hint: remember to account for missing data! When should `diabetes` be `NA`?

```{r}
# Add diabetes column
cleaned_ado <- cleaned_ado %>% 
          mutate(diabetes = case_when(
            is.na(LBXGH) & is.na(LBXGLU) ~ NA,
           LBXGH >= 6.5 | LBXGLU >= 126 ~ "diabetic",
           LBXGH >= 5.7 | LBXGLU >= 100 ~ "prediabetic",
            .default = "nondiabetic"))
```
:::

# Summary Statistics

We can easily use R functions so see the summary statistics of our data. 

::: {.callout-tip appearance="minimal"}
## Exercise

Calculate the percent of adolescents with dental caries experience. 

```{r}
# Recall that you can sum boolean variables to get a count
sum(cleaned_ado$dental.caries)/nrow(cleaned_ado)
```
:::

### Summarize groups

We can use `group_by` and `summarize` to see summary statistics by various subgroups. 
For instance, let's take a look at gender and birthplace. 

```{r}
cleaned_ado %>%
  group_by(RIAGENDR, birthplace) %>%
  summarize(n = n())
```
::: {.callout-tip appearance="minimal"}
## Summarizing

Calculate the percent of adolescents with dental caries for each birthplace category. 

```{r}
cleaned_ado %>%
  group_by(birthplace) %>% # Group by education level
  summarize(n = n(), caries = sum(dental.caries)) %>%
  mutate(perc_caries = caries/n)

```
:::


# Summary statistics or visualization?

Summary statistics can compress datasets of any size into just a few numbers. 
This can be vital for understanding large amounts of data, but can also lead to misconceptions.
Let's take a look at a classic example, Anscombe's quartet. 

```{r}
library(datasets)
datasets::anscombe
```

First, we need to restructure this data so we can easily group it by each of the different 4 datasets included. 
We'll learn more about what the `pivot_longer` command does in the last week of class.
```{r}
# Let's restructure this data to have 3 variables, x, y, and the dataset number it belongs to
long_ans <- anscombe %>%
  pivot_longer(
    everything(),
    cols_vary = "slowest",
    names_to = c(".value", "set"),
    names_pattern = "(.)(.)"
  )

```

Now we can visualize the 4 datasets easily using a `facet_wrap` in ggplot:

```{r}
ggplot(long_ans,aes(x=x,y=y,group=set))+geom_point()+facet_wrap(~set)
```
::: {.callout-tip appearance="minimal"}
# Exercise 

Calculate the mean and variance for x and y, and the correlation between x and y of each of the 4 sets of data. 

```{r}
long_ans %>%
  group_by(set) %>%
  summarise(mean_x = mean(x), mean_y = mean(y), variance_x = var(x), variance_y = var(y), correlation = cor(x, y))
```

:::

### Datasaurus

A similar example is in the `datasauRus` package in R. 

```{r}
library(datasauRus)

datasaurus_dozen %>% 
  group_by(dataset) %>% 
  summarize(
    mean_x    = mean(x),
    mean_y    = mean(y),
    std_dev_x = sd(x),
    std_dev_y = sd(y),
    corr_x_y  = cor(x, y)
  )
```

```{r}
library(ggplot2)
ggplot(datasaurus_dozen, aes(x = x, y = y, colour = dataset))+
  geom_point()+
  theme_void()+
  theme(legend.position = "none")+
  facet_wrap(~dataset, ncol = 3)
```