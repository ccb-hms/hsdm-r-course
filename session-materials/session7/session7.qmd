---
title: "Week 7: Advanced Regression"
date: "2023-06-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("splines")
library("DT")
```

This week we'll take a deeper look at regression models using a different NHANES dataset. 

## Load the data

There are 6063 observations, some are incomplete and have missing values for some covariates. There are 22 covariates, which have cryptic names and you need to use the meta-data to resolve them.  The survey is very complex and typically any analysis requires a substantial amount of reading of the documentation.  Here we will guide you past some of the hurdles.

We load up the data and the metadata. In the metadata we have a textual description of the phenotype, the short name, and the target.  The target tells us which of the sampled individuals was eligible to answer the question. 

```{r loadData}
nhanesDataPath = ""
load("d4.rda")
load("metaD.rda")
DT::datatable(metaD)
```

We will look at the relationship between the variable LBXTC (which is Total Cholesterol in mg/dL measured by a blood draw) and the age of the participant in years. 

```{r, echo=FALSE}
plot(d4$RIDAGEYR, d4$LBXTC, xlab="Age in Years", ylab="Total Cholesterol, mg/dL")
```

And we can see that in this plot, over-plotting is a substantial issue here.
You might also notice what seems like a lot of data at age 80, this is because any age over 80 was truncated to 80 to prevent reidentification of survey participants. In a complete analysis, this should probably be adjusted for in some way, but we will ignore it for now.

We can try some other methods, such as `hexbin` plotting and `smoothScatter` to get a better idea of the distribution of the data.  

```{r, echo=FALSE}
smoothScatter(d4$RIDAGEYR, d4$LBXTC, xlab="Age in Years", ylab="Total Cholesterol, mg/dL")
```

Now we can see a few outliers - with extremely high serum cholesterol.
We get a sense that the trend is not exactly a straight line, but rather a parabola, lower for the young and the old and a bit higher in the middle.

We fit a linear model first.

```{r}    
lm1 = lm(d4$LBXTC ~ d4$RIDAGEYR)
summary(lm1)
```

```{r, echo=TRUE}
plot(lm1$fitted.values, lm1$residuals)
##fit a loess curve
l2 = loess(lm1$residuals ~ lm1$fitted.values)
pl = predict(l2, newdata=sort(lm1$fitted.values))
lines(x=sort(lm1$fitted.values), y=pl, col="blue", lwd=2)
abline(h=0, col="red")

```

Notice that both terms in the model are very significant, but that the multiple $R^2$ is only around 2%.  
So age, in years, is not explaining very much of the variation. 
But because we have such a large data set, the parameter estimates are found to be significantly different from zero.

## Spline Models

When a linear model does not appear sufficient we can try other models.
One choice is to use natural splines, which are very flexible.
They create a "knotted" line, where at each knot the slope of the line can change. 
They are based on B-splines with the prevision that the model is linear outside the range of the data.

However, we have to decide the number of knots we want to use in the model. 
Based on the initial analysis, we chose to use df=7, which gives five internal knots when fitting the splines.
You have almost 6,000 degrees of freedom here, so using up a few to get a more appropriate fit seems good.

```{r}
library("splines")
lm2 = lm(d4$LBXTC ~ ns(d4$RIDAGEYR, df=7))
summary(lm2)
```


We can use an anova to compare the models.
```{r}
anova(lm1, lm2)
```

Notice also that the multiple $R^2$ went up to about 10%, a pretty substantial increase, suggesting that the curvilinear nature of the relationship is substantial.

The residual standard error also decreased by about 5%.

We have lost the simple explanation that comes from fitting a linear model. We cannot say that your serum cholesterol increases by $a$ units per year, but that model was wrong, so it really shouldn't be used.

We can use the regression model we fit to make predictions for any one, and these are substantially more accurate.


## Spline Models

Even though the regression summary prints out a different row for each spline term, they are not independent variables, and you need to either retain them all, or retain none of them.

## Sex

Next we might want to start to add other variables and explore the different relationships.
Let's consider sex, for now we will leave out age, and just try to understand what happens with sex
First let's fit the model without an intercept

```{r}
lm3 = lm(LBXTC ~ RIAGENDR-1, data=d4)
summary(lm3)

```

Here we can see the mean for males is a bit higher than for females
Both are significant, and notice how large the multiple $R^2$ value is.
This is not a very interesting test - we are asking if the mean is zero, which isn't even physically possible:
$$
  y_i = \beta_M \cdot 1_{M,i} + \beta_F \cdot 1_{F,i}
$$
Where $1_{M,i}$ is 1 if the $i^{th}$ case is male and zero otherwise, similarly for $1_{F,i}$
Instead let's use a model with an intercept to ask if the mean for males is different than that for females.
$$ 
  y_i = \beta_0 + \beta_1 \cdot 1_{F,i}
$$

This way, $E[Y|M] = \beta_0$ and $E[Y|F] = \beta_0 + \beta_1$, and $\beta_1$ estimates the difference in mean between male and female


```{r}
lm3 = lm(LBXTC ~ RIAGENDR, data=d4)
summary(lm3)

```

Now we see an Intercept term, which is the overall mean.
The estimate for females represents how they differ by sex, if it is zero then there is no difference in total cholesterol between men and women.

## Looking at more variables

Now we will put together a set of features (variables) that we are interested in.
For simplicity we only keep participants for which we have all the data.
Here, we use `apply`, which apply a function to each row in the dataset. 
We also could have used the tidyverse functions for handling missing data which we've seen previously. 

```{r}
ivars = c("RIDAGEYR", "RIAGENDR", "RIDRETH1", "DMDEDUC2", "INDFMPIR", "LBDHDD", "LBXGH", "BMXBMI", "LBXTC")

d4sub = d4[,ivars]
compCases = apply(d4sub, 1, function(x) sum(is.na(x)))
cC = compCases==0
d4sub = d4sub[cC,]
dim(d4sub)

```

## One quick transformation

The variable `DMDEDUC2` is a bit too granular for our purposes.
Let's modify it to be less than high school, high school and more than high school

```{r}
table(d4sub$DMDEDUC2)
dd = d4sub$DMDEDUC2

dd[dd=="Don't Know"] = NA

eduS = ifelse(dd == "Less than 9th grade" | dd =="9-11th grade (Includes 12th grade with no diploma)", "<HS", ifelse(dd == "High school graduate/GED or equivalent", "HS", ">HS" ))

#stick this into our dataframe
#and drop the NA
d4sub$eduS = eduS
d4sub = d4sub[-which(is.na(eduS)), ]

table(eduS, dd, useNA = "always")
```


## Back to Regression

Let's run a regression including all of our variables on cholesterol. We can do this by just indicating `.` in the formula. 

```{r}
lmF = lm(LBXTC ~ ., data=d4sub)
summary(lmF)
```

We see that being Non-hispanic black seems to have a pretty big effect, so we might want to just include that, and group all other ethnicities together.
Education level seems to have little to add, let's drop it to simplify the analysis. 
 It might be good to get a sense of the relationship with the poverty level variable `INDFMPIR`.

```{r}
 Black = ifelse(d4sub$RIDRETH1 == "Non-Hispanic Black", "B", "nonB")
 ivars = c("RIDAGEYR", "INDFMPIR", "LBDHDD", "LBXGH", "BMXBMI", "LBXTC")

 d5sub = cbind(d4sub[,ivars], Black)
 lmFx = lm(LBXTC ~ . , data=d5sub)
 summary(lmFx)
 
```
 
We can compare the two models using the `anova` function.

```{r, anova, eval=FALSE, echo=TRUE}
anova(lmFx, lmF) 
``` 

**Exercise:**
Plot RIDAGEYR versus INDFMPIR.
What do you see in the plot?  Does anything cause you concern about fitting a linear model?

## Missing Indicator Approach

The missing indicator approach may be useful for data where there is a limit of detection, so that values below some lower bound $\alpha_L$ or above some upper bound $\alpha_U$ are set to $\alpha_L$ or to $\alpha_U$ respectively.  A similar approach can be used for the Windsorization used for RIDAGEYR variable, where values over 80 are set to 80.  We are not proposing using this for other types of missingness, although there is a righ literature and users may want to explore the broader use of this method.  However, it is important to realize that often bias or increased variance may obtain, often dependent on untestable assumptions.

The reason that we believe this approach is appropriate for the cases listed is that we actually did measure the variable, and many over variables on those individuals, but we cannot report the exact value for any individual.  Hence, one can interpret the indicator as being some form of average over all individuals affected by the reporting limits.  It is probably worth noting that these limitations have implications for predication methods. While one in principle could estimate some outcome for an 88 year old person, the data won't support that prediction.  Instead one should ignore the linear predictor for age and use the indicator variable.
Chiou *et al* (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6812630/) provide some rationale for logistic regression and the references therein point to results for linear regression. Groenwold *et al* (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3414599/) provide advice on more general use of the method to address missing data.


Next let's try to fix out model to deal with the repeated values in these two variables.  Now an important consideration is to try and assess just how to interpret them.  For RIDAGEYR the documentation states that anyone over age 80 in the database has their age represented as 80.  This is not censoring.  The measurements (eg BMI, cholesterol etc.) were all made on a person of some age larger than 80.  We just Windsorized their ages, and so these individuals really are not the same as the others, where we are getting accurate age values. 


(https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3414599/)

So basically what we want to do is to create a *dummy variable* for those whose age is reported as 80.

```{r missingIndicator, echo=TRUE, eval=TRUE}

Age80 = d5sub$RIDAGEYR == 80
d6sub=cbind(d5sub, Age80)

lmFx2  = lm(LBXTC ~ . + Age80 , data=d6sub)
 summary(lmFx2)

```
**Exercise:**
What changes do you notice in the model fit?
Use the anova function to compare `lmFx3` to `lmFx2`.
How do you interpret the output?
*Hint: look at `INDFMPIR`*

**Exercise:**
Try to fit missing indicator variables for both of the repeated values in the INDFMPIR variable.
Then interpret the output.

```{r Poverty, echo=TRUE, eval=TRUE}
Pov5 = d6sub$INDFMPIR == 5
Pov0 = d6sub$INDFMPIR == 0
d7sub = cbind(d6sub, Pov5, Pov0)

lmFx2  = lm(LBXTC ~ . + Age80 + Pov0 + Pov5, data=d7sub)
 summary(lmFx2)
```

It seems that some of the apparent effect of INDFMPIR seems to be related to the fact that we are not fitting RIDAGEYR properly.

```{r}
 
 lmFx3 = lm(LBXTC ~ ns(RIDAGEYR, df=7)+ INDFMPIR+ LBDHDD+LBXGH + BMXBMI + Black + Age80 + Pov0 + Pov5, data=d7sub)
 summary(lmFx3)
```

*Exercise*

In the code below, we drop the terms that were not statistically significant in the model and then compare this smaller model to the larger one, above.
Try interpreting the results.
```{r}
lmFx4 = lm(LBXTC ~ ns(RIDAGEYR, df=7) + LBDHDD+LBXGH+Black, data=d7sub)
summary(lmFx4)

anova(lmFx4, lmFx3)
```

