---
title: "Week 7: Regression Models"
author: "Center for Computational Biomedicine"
format: 
  html: default
code-annotations: select
---

```{r}
#| output: false
#| warning: false
library(tidyverse)
library(survey)
library(DT)
library(flextable)
library(officer)
```

# Another NHANES example

Let's take a look at a different NHANES dataset and explore the relationship between cholesterol and age. 

There are 6063 observations, some are incomplete and have missing values for some covariates. There are 22 covariates, which as we've seen have cryptic names and you need to use the meta-data to resolve them.  

We load up the data and the metadata. In the metadata we have a textual description of the phenotype, the short name, and the target.  The target tells us which of the sampled individuals was eligible to answer the question. 

```{r loadData}
nhanesDataPath = ""
load("d4.rda")
load("metaD.rda")
DT::datatable(metaD)
```

We will look at the relationship between the variable LBXTC (which is Total Cholesterol in mg/dL measured by a blood draw) and the age of the participant in years. 

```{r, echo=FALSE}
plot(d4$RIDAGEYR, d4$LBXTC, xlab="Age in Years", ylab="Total Cholesterol, mg/dL")
```

And we can see that plotting, over-plotting is a substantial issue here.
You might also notice what seems like a lot of data at age 80, this is because any age over 80 was truncated to 80 to prevent reidentification of survey participants. In a complete analysis, this should probably be adjusted for in some way, but we will ignore it for now.

We can try some other methods, such as `hexbin` plotting and `smoothScatter`

```{r, echo=FALSE}
smoothScatter(d4$RIDAGEYR, d4$LBXTC, xlab="Age in Years", ylab="Total Cholesterol, mg/dL")
```

Now we can see a few outliers - with extremely high serum cholesterol.
We get a sense that the trend is not exactly a straight line, but rather a parabola, lower for the young and the old and a bit higher in the middle.

We fit a linear model first.

```{r}    
lm1 = lm(d4$LBXTC ~ d4$RIDAGEYR)
summary(lm1)
```

```{r, echo=TRUE}
plot(lm1$fitted.values, lm1$residuals)
##fit a loess curve
l2 = loess(lm1$residuals ~ lm1$fitted.values)
pl = predict(l2, newdata=sort(lm1$fitted.values))
lines(x=sort(lm1$fitted.values), y=pl, col="blue", lwd=2)
abline(h=0, col="red")

```

Notice that both terms in the model are very significant, but that the multiple $R^2$ is only around 2%.  So age, in years, is not explaining very much of the variation. But because we have such a large data set, the parameter estimates are found to be significantly different from zero.

## Spline Models

- when a linear model does not appear sufficient we can try other models.
- one choice is to use natural splines, which are very flexible
- they are based on B-splines with the previso that the model is linear outside the range of the data
- based on the initial analysis, we chose to use df=7, which gives five internal knots when fitting the splines
- you have almost 6,000 degrees of freedom here, so using up a few to get a more appropriate fit seems good.

```{r}
library("splines")
lm2 = lm(d4$LBXTC ~ ns(d4$RIDAGEYR, df=7))
summary(lm2)
```


- we can use standard tools for comparing models
```{r}
anova(lm1, lm2)
```

Notice also that the multiple $R^2$ went up to about 10%, a pretty substantial increase, suggesting that the curvilinear nature of the relationship is substantial.

The residual standard error also decreased by about 5%.

We have lost the simple explanation that comes from fitting a linear model. We cannot say that your serum cholesterol increases by $a$ units per year, but that model was wrong, so it really shouldn't be used.

We can use the regression model we fit to make predictions for any one, and these are substantially more accurate.

- even though the regression summary prints out a different row for each spline term, they are not independent variables, and you need to either retain them all, or retain none of them

## Sex

- next we might want to start to add other variables and explore the different relationships.
- let's consider sex, for now we will leave out age, and just try to understand what happens with sex
- first I will fit the model without an intercept

```{r}
lm3 = lm(LBXTC ~ RIAGENDR-1, data=d4)
summary(lm3)

```

- here we can see the mean for males is a bit higher than for females
- both are significant and notice how large the multiple $R^2$ value is
- this is not a very interesting test - we are asking if the mean is zero, which isn't even physically possible
- our model is
$$
  y_i = \beta_M \cdot 1_{M,i} + \beta_F \cdot 1_{F,i}
$$
- where $1_{M,i}$ is 1 if the $i^{th}$ case is male and zero otherwise, similarly for $1_{F,i}$

- instead we ask if the mean for males is different than that for females
$$ 
  y_i = \beta_0 + \beta_1 \cdot 1_{F,i}
$$

- so that $E[Y|M] = \beta_0$ and $E[Y|F] = \beta_0 + \beta_1$
- $\beta_1$ estimates the difference in mean between male and female


```{r}
lm3 = lm(LBXTC ~ RIAGENDR, data=d4)
summary(lm3)

```

- now we see an Intercept term (that will be the overall mean)
- and the estimate for females is represents how they differ, if it is zero then there is no difference in total cholesterol between men and women

# Recreating the analysis from Behesht et al

## Data

```{r}
load("processed_beheshti.RData")

wt_nhanes <- all_nhanes %>%
  drop_na(WTMEC2YR, SDMVPSU, SDMVSTRA) %>%
  mutate(WTMEC6YR = WTMEC2YR * 1/3)

# Create survey design object
nhanes_design <- svydesign(id     = ~SDMVPSU,
                          strata  = ~SDMVSTRA,
                          weights = ~WTMEC6YR,
                          nest    = TRUE,
                          data    = wt_nhanes)

#Subset the study population
ado_design <- subset(nhanes_design, RIDAGEYR >= 13 & RIDAGEYR <= 18 & !is.na(OHXDECAY))

#Also make a tibble of this data to analyze
ado_data <- wt_nhanes %>%
  filter(RIDAGEYR >= 13 & RIDAGEYR <= 18) %>% 
  filter(!is.na(OHXDECAY)) %>% 
  filter(!is.na(diabetes))
```

## Models

Let's recreate the analysis shown in table 2 of the Beheshti analysis. 
The table's caption explains the 5 different models:

> Model 1=crude (unadjusted); model 2=controlled for age, race/ethnicity, gender; model 3=controlled for age, race/ethnicity, gender, BMI; model 4=controlled for age, race/ethnicity, gender, BMI, and family income-to-poverty ratio; model 5=controlled for age, race/ethnicity, gender, BMI, family income-to-poverty ratio, and country of birth.

::: {.callout-tip appearance="minimal"}
## Discussion

Does the paper give enough detail to reproduce its logistic regression analysis? 

Why or why not?
:::


::: {.callout-tip appearance="minimal"}
## Exercise

Below model 1 has been reproduced.
See if you can recreate models 2-5.

```{r}

# Model 1: Unadjusted
dia_logit1 <- (svyglm(dental.caries~I(diabetes == "diabetic"), family=quasibinomial, design=ado_design, na.action = na.omit))
exp(coef(dia_logit1))

pre_logit1 <- (svyglm(dental.caries~I(diabetes == "prediabetic"), family=quasibinomial, design=ado_design, na.action = na.omit))
exp(coef(pre_logit1))

# Model 2: Controlled for age, race/ethnicity, gender
logit2 <- (svyglm(dental.caries~I(diabetes == "diabetic") + RIDRETH1 + RIAGENDR + as.factor(RIDAGEYR), family=quasibinomial, design=ado_design, na.action = na.omit))
exp(coef(logit2))

# Model 3: Controlled for age, race/ethnicity, gender, BMI

# Model 4: Controlled for age, race/ethnicity, gender, BMI, and family income-to-poverty ratio

# Model 5: Controlled for age, race/ethnicity, gender, BMI, family income-to-poverty ratio, and country of birth

```
:::